{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Faltungsnetz Versuch 5: Daten mit Wiederholung\n",
    "\n",
    "\n",
    "## Einleitung\n",
    "\n",
    "Bei diesem Versuch wurde wiederholt die gleiche Route mit einem Fahrrad befahren. Ziel ist es durch die Wiederholung die  befahrenen unterschiedlichen Oberflächen Typen zu reduzieren und somit eine bessere Klassifizerung zu ermöglichen. \n",
    "\n",
    "## Hypothese\n",
    "\n",
    "Durch Wiederholung bei der Datenerfassung sollte sich eine höhere Accuracy erziehlen lassen als im Faltungsnetz Versuch 4. Auf der befahrenen Route gibt es mehr Unebenheiten im Vergleich zu Datensatz 37 bis 42. Dies ermöglicht mehr Daten jeder Klasse zum Trainieren, weil mehr Daten mit hoher Erschütterung zur Verfügung stehen.  \n",
    "\n",
    "## Versuchsaufbau\n",
    "\n",
    "Eine Route im Naturschutzgebiet Höltigbaum wurde 14 Mal befahren siehe Abb. 1. Dabei setzen sich die Trainingsdaten aus den Datensätzen Nummer 43 und 45 bis 51 zusammen. Die Gesamtzahl der Bilder beträgt Stück.  \n",
    "\n",
    "<img src=\"../daten/abbildungen/karteDatensatz43_45_bis_51.png\" />\n",
    "Abbildung 1: Höltigbaum Route 14 Mal wiederholt befahren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Versuch 5.1: 3 Fahrqualitäts Klassen\n",
    "\n",
    "karteFuzzyVersuch6_datensatz43_45_bis_51.png\n",
    "#### Versuchsaufbau\n",
    "\n",
    "Die Daten in diesem Versuch wurden in Fuzzy Logik Versuch 6 klassifiziert. Hierbei wurde unterschieden zwischen den 3 Klassen \"gut\", \"mittel\" und \"schlecht\" nach der Fahrqualität. Dieser Versuch ist eine Wiederholung des Faltungsnetz Versuchs 4.7 mit dem Unterschied, dass wie beschrieben andere Datensätze verwendet wurden. Es wurden 20 Epochen durchgeführt.  \n",
    "\n",
    "<img src=\"../daten/abbildungen/karteFuzzyVersuch6_datensatz43_45_bis_51.png\"  alt=\"Karte mit labeln aus Fuzzy Versuch 6\" />\n",
    "Abbildung 2: Längengrad und Breitengrad mit labeln aus Fuzzy Logik Versuch 6\n",
    "\n",
    "#### Ergebnis\n",
    "\n",
    "Wie auf Abb. 4 zu sehen sieht die Konfusionsmatrix der Testdaten ähnlich wie in Versuch 4.7 aus. Die Test Accuracy siehe Tab. 5.2 ist, um $2,6\\%$ höher. Eigentlich wurde eine deutlich höhere Accuracy erwartet. Auf Tab. 2 ist die Trainingshistorie abgebildet. Durch erhöhen der Anzahl der Epochen könnte sich auch das Ergebnis verbessern.   \n",
    "  \n",
    "| | \n",
    " --- | --- |\n",
    "<img src=\"../daten/abbildungen/trainingshistorieAccuracyVersuch5_1.png\" alt=\"Trainingshistorie Accuracy von Versuch 5.1\" /> | <img src=\"../daten/abbildungen/trainingshistorieLossVersuch5_1.png\" alt=\"Trainingshistory Loss von Versuch 5.1\" /> \n",
    "Abbildung 3: Accuracy l. S. und Loss r. S. des Versuchs 5.1\n",
    "\n",
    "<img src=\"../daten/abbildungen/konfmatrixVersuch5_1.png\" alt=\"Konfusionmatrix von Versuch 5.1\" /> \n",
    "Abbildung 4: Konfusionmatrix von Versuch 5.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Versuch 5.2: Binäre Anzahl an Klassen\n",
    "\n",
    "### Versuchsaufbau\n",
    "\n",
    "In diesem Versuch wurde der Versuch 4.8 wiederholt nur mit den Datensätzen 43 und 45 bis 51. Auch hier wurde erwartet das die Test Accuracy höher ist als im Versuch 4.8. Bei den Versuch gab es zwei Klassen \"gute\" oder \"schlechte\" Fahrqualität siehe Abb. 5.\n",
    "\n",
    "<img src=\"../daten/abbildungen/karteFuzzyVersuch6_2Klassen_datensatz43_45_bis_51.png\" alt=\"Karte mit 2 Klassen\" />\n",
    "Abbildung 5: Längengrad und Breitengrad Karte mit 2 Klassen\n",
    "\n",
    "### Ergebnis\n",
    "\n",
    "Auf Tab. 2 ist zu sehen, dass die Test Accuracy mit $87,5\\%$ deutlich höher ist als im Versuch 4.8 mit $78,7\\%$. Dies sollte daran liegen, dass die Anzahl der Trainingsdaten höher ist und eine Route wiederholt befahren  wurde. \n",
    "\n",
    "<img src=\"../daten/abbildungen/konfmatrixVersuch5_2.png\" alt=\"Konfusionmatrix von Versuch 5.2\" /> \n",
    "Abbildung 6: Konfusionsmatrix von Versuch 5.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Versuch 5.3: Hypertuning mit 3 Klassen\n",
    "\n",
    "#### Hypothese\n",
    "\n",
    "Die Hypothese war, dass durch anpassen der Parameter eine höhere Test Accuracy erreicht werden kann als in Versuch 5.2 (siehe Tab. 2). \n",
    "\n",
    "#### Versuchsaufbau\n",
    "\n",
    "In diesem Versuch wurde nochmal nach bessere Parametern für das Modell aus Versuch 4.4 gesucht mit Hyperas. Die Daten wurden in Fuzzy Logik Versuch 6 gelabelt. Dabei wurde wie in Versuch 4.3 die Dropout Rate, die Anzahl der Faltungsschichten bei diesem Versuch von 3 bis 5 und die Optimierungsfunktion sowie weitere Parameter mit Hyperopt gesucht. Die Anzahl an Durchläufe mit Hyperopt waren 77.\n",
    "\n",
    "#### Ergebnis\n",
    "\n",
    "Mit den gefundenen Parametern siehe Tab. 1 konnte die Test Accuracy, um $2.92\\%$ optimiert werden. \n",
    "\n",
    "Schicht |             Ausgangsformat          | Anzahl der Parameter  |  \n",
    "       --    |          ---                   |          ---          |\n",
    "Conv2D                       | (368, 70, 32)  |          896          |       \n",
    "Activation(ELU)              | (368, 70, 32)  |          0            |   \n",
    "MaxPooling(2 * 2)            | (184, 35, 32)  |          0            |   \n",
    "Dropout (0.16)               | (184, 35, 32)  |          0            |   \n",
    "Conv2D                       | (184, 35, 32)  |         9248          |\n",
    "Activation(ELU)              | (184, 35, 32)  |          0            |\n",
    "MaxPooling                   | (92, 17, 32)   |          0            |\n",
    "Dropout(0.21)                | (92, 17, 32)   |          0            |\n",
    "Conv2D                       | (92, 17, 64)   |          18496        |    \n",
    "Activation(ELU)              | (92, 17, 64)   |          0            | \n",
    "MaxPooling                   | (46, 8, 64)    |          0            | \n",
    "Dropout(0.44)                | (46, 8, 64)    |          0            | \n",
    "Conv2D                       | (46, 8, 64)    |       36928           |\n",
    "Activation(ELU)              | (46, 8, 64)    |          0            |\n",
    "MaxPooling                   | (23, 4, 64)    |          0            |\n",
    "Dropout(0.13)                | (23, 4, 64)    |          0            | \n",
    "Conv2D                       | (23, 4, 64)    |       36928           |\n",
    "Activation(ELU)              | (23, 4, 64)    |          0            |\n",
    "MaxPooling                   | (11, 2, 64)    |          0            |\n",
    "Dropout(0.40)                | (11, 2, 64)    |          0            | \n",
    "Flatten                      | (1408)         |          0            | \n",
    "Dense                        | (64)           |         90176         |   \n",
    "Activation(ELU)              | (64)           |          0            |   \n",
    "Dropout(0.07)                | (64)           |          0            |         \n",
    "Dense                        | (3)            |          520          |       \n",
    "Activation(ELU)              | (3)            |          0            |\n",
    "Tabelle 1: Parameter des gefundenen Modells aus Versuch 5.3 mit der Optimierungsfunktion Adam und einer batch size von 32 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Versuch 5.4: Parameter aus 5.3 mit 100 Epochen \n",
    "\n",
    "#### Hypothese\n",
    "\n",
    "In Versuch 5.3 wurden jeweils nur 10 Epochen genutzt, um ein Faltungsnetz zu trainieren. Deshalb wurde untersucht ob sich die Test Accuracy erhöht indem die Anzahl der Epochen erhöht wurde.\n",
    "\n",
    "#### Versuchsaufbau\n",
    "\n",
    "Bei diesem Versuch wurde ein Faltungsnetz mit den Parametern aus Versuch 5.3 und den Klassen aus Fuzzy Logik Versuch 6 trainiert. Dabei wurden 100 Epochen ausgeführt.  \n",
    "\n",
    "#### Ergebnis\n",
    "\n",
    "In Tab. 2 ist zu sehen, dass mit Erhöhung der Anzahl der Epochen auf 100 mit dem besten Model die Test Accuracy, um $1.3\\%$ angehoben werden konnte. Auf Abb. 7 l. S. ist zu erkennen, dass ab Epoche das Model überangepasst wurde. Wie auf Abb. 8 zu sehen wurden vor allen die Klassen \"gut\" und \"schlecht\" vorhergesagt.\n",
    "\n",
    "| | \n",
    " --- | --- |\n",
    "<img src=\"../daten/abbildungen/trainingshistorieAccuracyVersuch5_4.png\" alt=\"Trainingshistorie Accuracy von Versuch 5.4\" /> | <img src=\"../daten/abbildungen/trainingshistorieLossVersuch5_4.png\" alt=\"Trainingshistorie Loss von Versuch 5.4\" /> \n",
    "Abbildung 7: Accuracy l. S. und Loss r. S. des Versuchs 5.4\n",
    "\n",
    "<img src=\"../daten/abbildungen/konfmatrixVersuch5_1.png\" alt=\"Konfusionmatrix von Versuch 5.4\" /> \n",
    "Abbildung 8: Konfusionmatrix von Versuch 5.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Versuch 5.5: Test mit Daten aus Datensatz 37-42 \n",
    "\n",
    "#### Hypothese\n",
    "\n",
    "Mit den gefundenen Parametern aus Versuch 5.3 sollte sich auch die Test Accuracy mit den Daten aus Datensatz 37 bis 42 steigern lassen. Die Annahme war, dass die Test Accuracy höher sein wird als in Versuch 4.7 ($56.79\\%$). \n",
    "\n",
    "#### Versuchsaufbau\n",
    "\n",
    "In diesem Versuch wurde der Versuch 5.4 wiederholt mit den Datensätzen 37 bis 42. Hierbei wurden die Daten gelabelt in Fuzzy Logik Versuch 4.   \n",
    "\n",
    "#### Ergebnis\n",
    "\n",
    "Die Test Accuracy betrug $57.96\\%$ siehe Tab.2. Damit war diese nur $1.17\\%$ höher als im Versuch 4.7. \n",
    "Ein Blick auf die Trainingshistorie siehe Abb. 9 zeigt, dass das Faltungsnetz überangepasst war.\n",
    "\n",
    "| | \n",
    " --- | --- |\n",
    "<img src=\"../daten/abbildungen/trainingshistorieAccuracyVersuch5_5.png\" alt=\"Trainingshistorie Accuracy von Versuch 5.5\" /> | <img src=\"../daten/abbildungen/trainingshistorieLossVersuch5_5.png\" alt=\"Trainingshistorie Loss von Versuch 5.5\" /> \n",
    "Abbildung 9: Accuracy l. S. und Loss r. S. des Versuchs 5.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Versuch 5.6: Unterteilung der Geschwindigkeit \n",
    "\n",
    "#### Hypothese\n",
    "\n",
    "Dieser Versuch ist motiviert aus dem Ergebnis von Versuch 4.5. Ziel war es zwei Faltungsnetze zu trainieren für den Geschwindigkeitsbereich \"niedrig\" und \"hoch\". \n",
    "\n",
    "#### Versuchsaufbau\n",
    "\n",
    "Als Parameter für das Faltungsnetz wurden die aus Versuch 5.3 genommen. Die Daten aus Datensatz 43, 45 bis 51 wurden in Fuzzy Versuch 7 gelabelt. Dabei wurden die Fahrqualitäten bei \"niedriger\" und \"hoher\" Geschwindigkeit.\n",
    " \n",
    "#### Ergebnis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ergebnisse\n",
    "\n",
    "Versuch Nr. | Trainings Accuracy | Trainings Loss | Validierungs Accuracy | Validierungs Loss | Test Accuracy | Test Loss \n",
    "--- | --- | --- | --- | --- \n",
    "5.1   | 0.5524 | 0.9188 | 0.6030 | 0.8369 | 0.5939 | 0.8477\n",
    "5.2   | 0.8426 | 0.3926 | 0.8766 | 0.3584 | 0.875  | 0.3552\n",
    "5.3   | 0.6082 | 0.8173 | 0.6269 | 0.7961 | 0.6231 | 0.7944\n",
    "5.4   | 0.6299 | 0.7739 | 0.6536 | 0.7547 | 0.6361 | 0.7725\n",
    "5.5   | 0.5880 | 0.8370 | 0.5761 | 0.9308 | 0.5796 | 0.8937   \n",
    "Tabelle 2: Ergebnisse ver Versuche  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-12T12:43:26.891087Z",
     "start_time": "2018-07-12T12:43:26.851600Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from hyperopt import Trials, STATUS_OK, rand\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import uniform, choice\n",
    "import pandas\n",
    "import seaborn as sns\n",
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as mlines\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-12T12:43:28.236849Z",
     "start_time": "2018-07-12T12:43:28.232852Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras import initializers\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.models import load_model\n",
    "import keras.callbacks as cb\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-12T12:43:31.328242Z",
     "start_time": "2018-07-12T12:43:29.252988Z"
    }
   },
   "outputs": [],
   "source": [
    "featuresDf = pandas.read_csv(filepath_or_buffer=\"../daten/merkmale_datensatz_37_bis_42/merkmaleMitLabelnFuzzyVersuch4.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-12T12:43:34.565372Z",
     "start_time": "2018-07-12T12:43:34.561374Z"
    }
   },
   "outputs": [],
   "source": [
    "# Nummer des aktuellen Versuchs\n",
    "experimentNumber = \"5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-12T12:44:06.110271Z",
     "start_time": "2018-07-12T12:44:06.091793Z"
    }
   },
   "outputs": [],
   "source": [
    "# Hier können die Datensätze ausgewählt werden\n",
    "datasets = ['37','38','39','40','41','42']\n",
    "# Die Pfade zu den Ordnern in welchem sich die Bilder befinden\n",
    "paths = []\n",
    "# Liste mit Pfaden zu den Bildern\n",
    "imagePaths = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-12T12:44:14.369150Z",
     "start_time": "2018-07-12T12:44:09.600521Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/morro/Documents/datenRoh/37/zugeschnitten/\n",
      "Bilder aus folgenden Ordnern werden geladen: ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14']\n",
      "Ordner der geladen wird: 1\n",
      "Ordner der geladen wird: 2\n",
      "Ordner der geladen wird: 3\n",
      "Ordner der geladen wird: 4\n",
      "Ordner der geladen wird: 5\n",
      "Ordner der geladen wird: 6\n",
      "Ordner der geladen wird: 7\n",
      "Ordner der geladen wird: 8\n",
      "Ordner der geladen wird: 9\n",
      "Ordner der geladen wird: 10\n",
      "Ordner der geladen wird: 11\n",
      "Ordner der geladen wird: 12\n",
      "Ordner der geladen wird: 13\n",
      "Ordner der geladen wird: 14\n",
      "C:/Users/morro/Documents/datenRoh/38/zugeschnitten/\n",
      "Bilder aus folgenden Ordnern werden geladen: ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22']\n",
      "Ordner der geladen wird: 1\n",
      "Ordner der geladen wird: 2\n",
      "Ordner der geladen wird: 3\n",
      "Ordner der geladen wird: 4\n",
      "Ordner der geladen wird: 5\n",
      "Ordner der geladen wird: 6\n",
      "Ordner der geladen wird: 7\n",
      "Ordner der geladen wird: 8\n",
      "Ordner der geladen wird: 9\n",
      "Ordner der geladen wird: 10\n",
      "Ordner der geladen wird: 11\n",
      "Ordner der geladen wird: 12\n",
      "Ordner der geladen wird: 13\n",
      "Ordner der geladen wird: 14\n",
      "Ordner der geladen wird: 15\n",
      "Ordner der geladen wird: 16\n",
      "Ordner der geladen wird: 17\n",
      "Ordner der geladen wird: 18\n",
      "Ordner der geladen wird: 19\n",
      "Ordner der geladen wird: 20\n",
      "Ordner der geladen wird: 21\n",
      "Ordner der geladen wird: 22\n",
      "C:/Users/morro/Documents/datenRoh/39/zugeschnitten/\n",
      "Bilder aus folgenden Ordnern werden geladen: ['1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
      "Ordner der geladen wird: 1\n",
      "Ordner der geladen wird: 2\n",
      "Ordner der geladen wird: 3\n",
      "Ordner der geladen wird: 4\n",
      "Ordner der geladen wird: 5\n",
      "Ordner der geladen wird: 6\n",
      "Ordner der geladen wird: 7\n",
      "Ordner der geladen wird: 8\n",
      "Ordner der geladen wird: 9\n",
      "C:/Users/morro/Documents/datenRoh/40/zugeschnitten/\n",
      "Bilder aus folgenden Ordnern werden geladen: ['1', '2', '3', '4', '5', '6', '7']\n",
      "Ordner der geladen wird: 1\n",
      "Ordner der geladen wird: 2\n",
      "Ordner der geladen wird: 3\n",
      "Ordner der geladen wird: 4\n",
      "Ordner der geladen wird: 5\n",
      "Ordner der geladen wird: 6\n",
      "Ordner der geladen wird: 7\n",
      "C:/Users/morro/Documents/datenRoh/41/zugeschnitten/\n",
      "Bilder aus folgenden Ordnern werden geladen: ['1', '2', '3']\n",
      "Ordner der geladen wird: 1\n",
      "Ordner der geladen wird: 2\n",
      "Ordner der geladen wird: 3\n",
      "C:/Users/morro/Documents/datenRoh/42/zugeschnitten/\n",
      "Bilder aus folgenden Ordnern werden geladen: ['1', '2', '3', '4', '5', '6', '7', '8']\n",
      "Ordner der geladen wird: 1\n",
      "Ordner der geladen wird: 2\n",
      "Ordner der geladen wird: 3\n",
      "Ordner der geladen wird: 4\n",
      "Ordner der geladen wird: 5\n",
      "Ordner der geladen wird: 6\n",
      "Ordner der geladen wird: 7\n",
      "Ordner der geladen wird: 8\n"
     ]
    }
   ],
   "source": [
    "for dataset in datasets: # Für jeden Datensatz merke Pfad\n",
    "    paths.append(\"C:/Users/morro/Documents/datenRoh/\" + dataset + \"/zugeschnitten/\")\n",
    "for path in paths: # Für jeden Pfad hole die Namen der Ordner\n",
    "    folders = os.listdir(path)\n",
    "    folders = sorted(folders, key=int) #sortiert die Reihenfolge de Ordner aufsteifend\n",
    "    print(path)\n",
    "    print(\"Bilder aus folgenden Ordnern werden geladen: \" + str(folders))\n",
    "    for folder in folders: # Aus der Liste der Ordner wird ein Ordner ausgewählt\n",
    "        filesPath = path + folder + \"/\"\n",
    "        files = os.listdir(filesPath)\n",
    "        print(\"Ordner der geladen wird: \" + str(folder))\n",
    "        for name in files: # Ein Dateiname aus diesem Ordner\n",
    "            if \"jpg\" not in name:\n",
    "                continue\n",
    "            imagePaths.append(filesPath + name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-12T12:44:16.691696Z",
     "start_time": "2018-07-12T12:44:16.682174Z"
    }
   },
   "outputs": [],
   "source": [
    "classNames = ['gut','mittel','schlecht'] # Namen der Klassen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-12T12:44:17.683154Z",
     "start_time": "2018-07-12T12:44:17.675157Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    63909\n",
       "0    39591\n",
       "2    12290\n",
       "Name: Klasse, dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featuresDf[\"Klasse\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresDf = featuresDf[featuresDf['Klasse'] != 1]\n",
    "featuresDf[featuresDf['Klasse'] == 2] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-12T12:44:27.588470Z",
     "start_time": "2018-07-12T12:44:27.580474Z"
    }
   },
   "outputs": [],
   "source": [
    "yLabels = np_utils.to_categorical(featuresDf[\"Klasse\"], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-12T12:44:28.598132Z",
     "start_time": "2018-07-12T12:44:28.594133Z"
    }
   },
   "outputs": [],
   "source": [
    "# Setzten des RandomState um reproduzierbare Ergebnisse zu erzielen.\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-12T12:44:31.140206Z",
     "start_time": "2018-07-12T12:44:31.104217Z"
    }
   },
   "outputs": [],
   "source": [
    "# Mischen der Trainingsdaten\n",
    "xShuffle, yShuffle = shuffle(imagePaths,yLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-12T12:51:40.849839Z",
     "start_time": "2018-07-12T12:51:40.845841Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12290"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class1Number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-12T12:46:54.767156Z",
     "start_time": "2018-07-12T12:44:33.181099Z"
    }
   },
   "outputs": [],
   "source": [
    "# Die Zelle Normiert die Anzahl der Repräsentanten pro Klasse\n",
    "class1Number = 0\n",
    "class2Number = 0 \n",
    "class3Number = 0\n",
    "maxClasses = featuresDf[\"Klasse\"].value_counts().min()\n",
    "indexToDelete = [] \n",
    "i = -1\n",
    "for label in yShuffle:\n",
    "    i = i + 1\n",
    "    labelNumber = np.argmax(label,axis=0)\n",
    "    if labelNumber == 0 and class1Number < maxClasses:\n",
    "        class1Number = class1Number + 1\n",
    "        continue\n",
    "    elif labelNumber == 0:\n",
    "        indexToDelete.append(i)\n",
    "    if labelNumber == 1 and class2Number < maxClasses:\n",
    "        class2Number = class2Number + 1\n",
    "        continue\n",
    "    elif labelNumber == 1:\n",
    "        indexToDelete.append(i)\n",
    "    if labelNumber == 2 and class3Number < maxClasses:\n",
    "        class3Number = class3Number + 1\n",
    "        continue\n",
    "    elif labelNumber == 2:\n",
    "        indexToDelete.append(i)\n",
    "    if labelNumber == 3 and class4Number < maxClasses:\n",
    "        class4Number = class4Number + 1\n",
    "        continue        \n",
    "xShuffle = [i for j, i in enumerate(xShuffle) if j not in indexToDelete]\n",
    "yShuffle = [i for j, i in enumerate(yShuffle) if j not in indexToDelete]\n",
    "yShuffle = np.asarray(yShuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-12T12:51:47.922660Z",
     "start_time": "2018-07-12T12:51:47.898882Z"
    }
   },
   "outputs": [],
   "source": [
    "# Mischen der Trainingsdaten\n",
    "xShuffle, yShuffle = shuffle(xShuffle,yShuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-12T12:51:51.114221Z",
     "start_time": "2018-07-12T12:51:51.090230Z"
    }
   },
   "outputs": [],
   "source": [
    "# Aufteilung in Trainings, Validation und Testdaten\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(xShuffle, yShuffle, test_size=0.10)\n",
    "xTrain, xVal, yTrain, yVal = train_test_split(xTrain, yTrain, test_size=0.20) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-12T12:51:52.683340Z",
     "start_time": "2018-07-12T12:51:52.660520Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 12290, 0: 12290, 2: 12290})"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import collections\n",
    "collections.Counter(np.argmax(yShuffle, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-12T12:51:58.098749Z",
     "start_time": "2018-07-12T12:51:58.083248Z"
    }
   },
   "outputs": [],
   "source": [
    "# Diese Funktion läd Bilder in den Hauptspeicher\n",
    "# imagesPaths: Liste mit Pfaden zu den Bildern != null\n",
    "def imageLoader(imagePaths):\n",
    "    images = []\n",
    "    for path in imagePaths:\n",
    "        images.append(cv2.cvtColor(cv2.imread(path),cv2.COLOR_BGR2RGB))\n",
    "    imagesNp = np.array(images)\n",
    "    imagesNp = imagesNp.astype('float32')\n",
    "    # Transfomierung der Bildpunkte auf den Wetebereich von 0 bis 1\n",
    "    imagesNp /= 255\n",
    "    return imagesNp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-12T12:51:59.149847Z",
     "start_time": "2018-07-12T12:51:59.141837Z"
    }
   },
   "outputs": [],
   "source": [
    "# Läd Trainingsdaten in batches\n",
    "def dataLoader(imagePaths, features, batchSize):\n",
    "    imagesCount = len(imagePaths)  \n",
    "    while True:\n",
    "        batchStart = 0\n",
    "        batchEnd = batchSize\n",
    "        while batchStart < imagesCount:\n",
    "            limit = min(batchEnd, imagesCount)\n",
    "            x = imageLoader(imagePaths[batchStart:limit])\n",
    "            y = features[batchStart:limit]\n",
    "            yield (x,y) \n",
    "            batchStart += batchSize   \n",
    "            batchEnd += batchSize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-12T12:52:00.289460Z",
     "start_time": "2018-07-12T12:52:00.277017Z"
    }
   },
   "outputs": [],
   "source": [
    "# Parameter für das CNN\n",
    "inputShape     = (368, 70, 3)   # Eingangs Array-Form \n",
    "numNeuronsC1   = 32                # Anzahl der Filter / 1 Faltungsschicht\n",
    "numNeuronsC2   = 32                # Anzahl der Filter / 2 Faltungsschicht\n",
    "numNeuronsC3   = 64                # Anzahl der Filter / 3 Faltungsschicht\n",
    "numNeuronsC4   = 64\n",
    "numNeuronsC5   = 64\n",
    "numNeuronsD1   = 64                # Anzahl der Neuronen des Fully connected layer - vollverbundene Schicht\n",
    "poolSize       = 2                 # Größe der Pooling-Layer\n",
    "convKernelSize = 3                 # Größe des Faltungskern n*n\n",
    "batchSize      = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-12T14:11:51.903652Z",
     "start_time": "2018-07-12T12:52:11.892243Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "829/829 [==============================] - 79s 96ms/step - loss: 1.1078 - acc: 0.3484 - val_loss: 1.0759 - val_acc: 0.4091\n",
      "Epoch 2/100\n",
      "829/829 [==============================] - 48s 57ms/step - loss: 1.0934 - acc: 0.3687 - val_loss: 1.0750 - val_acc: 0.4183\n",
      "Epoch 3/100\n",
      "829/829 [==============================] - 47s 57ms/step - loss: 1.0854 - acc: 0.3918 - val_loss: 1.0613 - val_acc: 0.4254\n",
      "Epoch 4/100\n",
      "829/829 [==============================] - 47s 57ms/step - loss: 1.0745 - acc: 0.4117 - val_loss: 1.1195 - val_acc: 0.3706\n",
      "Epoch 5/100\n",
      "829/829 [==============================] - 47s 56ms/step - loss: 1.0504 - acc: 0.4408 - val_loss: 1.0299 - val_acc: 0.4531\n",
      "Epoch 6/100\n",
      "829/829 [==============================] - 47s 57ms/step - loss: 1.0340 - acc: 0.4558 - val_loss: 1.0772 - val_acc: 0.4354\n",
      "Epoch 7/100\n",
      "829/829 [==============================] - 47s 56ms/step - loss: 0.9793 - acc: 0.5049 - val_loss: 1.0083 - val_acc: 0.4943\n",
      "Epoch 8/100\n",
      "829/829 [==============================] - 47s 56ms/step - loss: 0.9450 - acc: 0.5253 - val_loss: 0.9792 - val_acc: 0.5254\n",
      "Epoch 9/100\n",
      "829/829 [==============================] - 47s 56ms/step - loss: 0.9277 - acc: 0.5388 - val_loss: 0.9368 - val_acc: 0.5355\n",
      "Epoch 10/100\n",
      "829/829 [==============================] - 46s 56ms/step - loss: 0.9200 - acc: 0.5438 - val_loss: 0.9170 - val_acc: 0.5372\n",
      "Epoch 11/100\n",
      "829/829 [==============================] - 48s 57ms/step - loss: 0.9146 - acc: 0.5463 - val_loss: 0.9405 - val_acc: 0.5553\n",
      "Epoch 12/100\n",
      "829/829 [==============================] - 47s 57ms/step - loss: 0.9078 - acc: 0.5491 - val_loss: 0.9190 - val_acc: 0.5552\n",
      "Epoch 13/100\n",
      "829/829 [==============================] - 47s 57ms/step - loss: 0.9026 - acc: 0.5528 - val_loss: 0.9426 - val_acc: 0.5381\n",
      "Epoch 14/100\n",
      "829/829 [==============================] - 47s 56ms/step - loss: 0.8973 - acc: 0.5590 - val_loss: 0.8922 - val_acc: 0.5656\n",
      "Epoch 15/100\n",
      "829/829 [==============================] - 48s 57ms/step - loss: 0.8950 - acc: 0.5564 - val_loss: 0.9098 - val_acc: 0.5653\n",
      "Epoch 16/100\n",
      "829/829 [==============================] - 47s 57ms/step - loss: 0.8897 - acc: 0.5584 - val_loss: 0.9341 - val_acc: 0.5499\n",
      "Epoch 17/100\n",
      "829/829 [==============================] - 47s 57ms/step - loss: 0.8880 - acc: 0.5600 - val_loss: 0.9090 - val_acc: 0.5656\n",
      "Epoch 18/100\n",
      "829/829 [==============================] - 47s 57ms/step - loss: 0.8835 - acc: 0.5674 - val_loss: 0.8923 - val_acc: 0.5720\n",
      "Epoch 19/100\n",
      "829/829 [==============================] - 47s 57ms/step - loss: 0.8798 - acc: 0.5682 - val_loss: 0.9076 - val_acc: 0.5635\n",
      "Epoch 20/100\n",
      "829/829 [==============================] - 47s 57ms/step - loss: 0.8789 - acc: 0.5710 - val_loss: 0.9166 - val_acc: 0.5637\n",
      "Epoch 21/100\n",
      "829/829 [==============================] - 47s 57ms/step - loss: 0.8773 - acc: 0.5688 - val_loss: 0.9599 - val_acc: 0.5602\n",
      "Epoch 22/100\n",
      "829/829 [==============================] - 47s 57ms/step - loss: 0.8695 - acc: 0.5707 - val_loss: 0.9306 - val_acc: 0.5679\n",
      "Epoch 23/100\n",
      "829/829 [==============================] - 47s 56ms/step - loss: 0.8700 - acc: 0.5717 - val_loss: 0.9599 - val_acc: 0.5626\n",
      "Epoch 24/100\n",
      "829/829 [==============================] - 46s 56ms/step - loss: 0.8717 - acc: 0.5689 - val_loss: 0.9547 - val_acc: 0.5575\n",
      "Epoch 25/100\n",
      "829/829 [==============================] - 47s 56ms/step - loss: 0.8689 - acc: 0.5708 - val_loss: 0.8940 - val_acc: 0.5696\n",
      "Epoch 26/100\n",
      "829/829 [==============================] - 47s 57ms/step - loss: 0.8687 - acc: 0.5730 - val_loss: 0.9126 - val_acc: 0.5634\n",
      "Epoch 27/100\n",
      "829/829 [==============================] - 47s 57ms/step - loss: 0.8639 - acc: 0.5775 - val_loss: 0.9245 - val_acc: 0.5649\n",
      "Epoch 28/100\n",
      "829/829 [==============================] - 47s 57ms/step - loss: 0.8627 - acc: 0.5721 - val_loss: 0.9137 - val_acc: 0.5644\n",
      "Epoch 29/100\n",
      "829/829 [==============================] - 47s 56ms/step - loss: 0.8635 - acc: 0.5736 - val_loss: 0.9187 - val_acc: 0.5546\n",
      "Epoch 30/100\n",
      "829/829 [==============================] - 47s 56ms/step - loss: 0.8584 - acc: 0.5762 - val_loss: 0.9292 - val_acc: 0.5505\n",
      "Epoch 31/100\n",
      "829/829 [==============================] - 47s 56ms/step - loss: 0.8610 - acc: 0.5782 - val_loss: 0.9374 - val_acc: 0.5505\n",
      "Epoch 32/100\n",
      "829/829 [==============================] - 47s 57ms/step - loss: 0.8548 - acc: 0.5824 - val_loss: 0.8937 - val_acc: 0.5747\n",
      "Epoch 33/100\n",
      "829/829 [==============================] - 47s 57ms/step - loss: 0.8586 - acc: 0.5796 - val_loss: 0.9564 - val_acc: 0.5431\n",
      "Epoch 34/100\n",
      "829/829 [==============================] - 47s 57ms/step - loss: 0.8567 - acc: 0.5802 - val_loss: 0.9014 - val_acc: 0.5626\n",
      "Epoch 35/100\n",
      "829/829 [==============================] - 47s 57ms/step - loss: 0.8539 - acc: 0.5826 - val_loss: 0.9316 - val_acc: 0.5500\n",
      "Epoch 36/100\n",
      "829/829 [==============================] - 47s 57ms/step - loss: 0.8545 - acc: 0.5829 - val_loss: 0.9478 - val_acc: 0.5479\n",
      "Epoch 37/100\n",
      "829/829 [==============================] - 47s 57ms/step - loss: 0.8554 - acc: 0.5788 - val_loss: 0.9036 - val_acc: 0.5625\n",
      "Epoch 38/100\n",
      "829/829 [==============================] - 47s 57ms/step - loss: 0.8559 - acc: 0.5778 - val_loss: 0.9155 - val_acc: 0.5434\n",
      "Epoch 39/100\n",
      "829/829 [==============================] - 47s 57ms/step - loss: 0.8542 - acc: 0.5802 - val_loss: 0.9110 - val_acc: 0.5470\n",
      "Epoch 40/100\n",
      "829/829 [==============================] - 47s 57ms/step - loss: 0.8523 - acc: 0.5848 - val_loss: 0.9246 - val_acc: 0.5509\n",
      "Epoch 41/100\n",
      "829/829 [==============================] - 47s 56ms/step - loss: 0.8500 - acc: 0.5811 - val_loss: 0.9150 - val_acc: 0.5456\n",
      "Epoch 42/100\n",
      "829/829 [==============================] - 47s 57ms/step - loss: 0.8493 - acc: 0.5823 - val_loss: 0.9113 - val_acc: 0.5531\n",
      "Epoch 43/100\n",
      "829/829 [==============================] - 48s 57ms/step - loss: 0.8468 - acc: 0.5825 - val_loss: 0.9027 - val_acc: 0.5656\n",
      "Epoch 44/100\n",
      "829/829 [==============================] - 48s 58ms/step - loss: 0.8438 - acc: 0.5856 - val_loss: 0.9286 - val_acc: 0.5543\n",
      "Epoch 45/100\n",
      "829/829 [==============================] - 48s 57ms/step - loss: 0.8395 - acc: 0.5923 - val_loss: 0.9097 - val_acc: 0.5671\n",
      "Epoch 46/100\n",
      "829/829 [==============================] - 48s 57ms/step - loss: 0.8447 - acc: 0.5864 - val_loss: 0.9024 - val_acc: 0.5652\n",
      "Epoch 47/100\n",
      "829/829 [==============================] - 47s 57ms/step - loss: 0.8456 - acc: 0.5822 - val_loss: 0.9212 - val_acc: 0.5544\n",
      "Epoch 48/100\n",
      "829/829 [==============================] - 47s 57ms/step - loss: 0.8465 - acc: 0.5834 - val_loss: 0.9244 - val_acc: 0.5599\n",
      "Epoch 49/100\n",
      "829/829 [==============================] - 47s 57ms/step - loss: 0.8429 - acc: 0.5863 - val_loss: 0.9024 - val_acc: 0.5625\n",
      "Epoch 50/100\n",
      "829/829 [==============================] - 47s 57ms/step - loss: 0.8438 - acc: 0.5873 - val_loss: 0.9027 - val_acc: 0.5626\n",
      "Epoch 51/100\n",
      "829/829 [==============================] - 48s 58ms/step - loss: 0.8423 - acc: 0.5865 - val_loss: 0.9233 - val_acc: 0.5599\n",
      "Epoch 52/100\n",
      "829/829 [==============================] - 47s 57ms/step - loss: 0.8427 - acc: 0.5880 - val_loss: 0.9227 - val_acc: 0.5529\n",
      "Epoch 53/100\n",
      "829/829 [==============================] - 47s 57ms/step - loss: 0.8433 - acc: 0.5867 - val_loss: 0.9105 - val_acc: 0.5608\n",
      "Epoch 54/100\n",
      "829/829 [==============================] - 47s 57ms/step - loss: 0.8384 - acc: 0.5917 - val_loss: 0.9255 - val_acc: 0.5608\n",
      "Epoch 55/100\n",
      "829/829 [==============================] - 47s 57ms/step - loss: 0.8393 - acc: 0.5918 - val_loss: 0.9141 - val_acc: 0.5705\n",
      "Epoch 56/100\n",
      "829/829 [==============================] - 47s 57ms/step - loss: 0.8398 - acc: 0.5881 - val_loss: 0.9044 - val_acc: 0.5652\n",
      "Epoch 57/100\n",
      "829/829 [==============================] - 47s 57ms/step - loss: 0.8460 - acc: 0.5869 - val_loss: 0.8973 - val_acc: 0.5638\n",
      "Epoch 58/100\n",
      "829/829 [==============================] - 47s 56ms/step - loss: 0.8370 - acc: 0.5923 - val_loss: 0.9114 - val_acc: 0.5714\n",
      "Epoch 59/100\n",
      "829/829 [==============================] - 48s 58ms/step - loss: 0.8344 - acc: 0.5909 - val_loss: 0.8993 - val_acc: 0.5673\n",
      "Epoch 60/100\n",
      "829/829 [==============================] - 66s 80ms/step - loss: 0.8401 - acc: 0.5924 - val_loss: 0.9161 - val_acc: 0.5656\n",
      "Epoch 61/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "829/829 [==============================] - 51s 61ms/step - loss: 0.8381 - acc: 0.5886 - val_loss: 0.9245 - val_acc: 0.5640\n",
      "Epoch 62/100\n",
      "829/829 [==============================] - 47s 57ms/step - loss: 0.8361 - acc: 0.5898 - val_loss: 0.8965 - val_acc: 0.5734\n",
      "Epoch 63/100\n",
      "829/829 [==============================] - 47s 57ms/step - loss: 0.8358 - acc: 0.5909 - val_loss: 0.9163 - val_acc: 0.5711\n",
      "Epoch 64/100\n",
      "829/829 [==============================] - 47s 56ms/step - loss: 0.8374 - acc: 0.5866 - val_loss: 0.8976 - val_acc: 0.5800\n",
      "Epoch 65/100\n",
      "829/829 [==============================] - 48s 58ms/step - loss: 0.8382 - acc: 0.5878 - val_loss: 0.9117 - val_acc: 0.5628\n",
      "Epoch 66/100\n",
      "829/829 [==============================] - 48s 57ms/step - loss: 0.8290 - acc: 0.5951 - val_loss: 0.9400 - val_acc: 0.5711\n",
      "Epoch 67/100\n",
      "829/829 [==============================] - 47s 57ms/step - loss: 0.8338 - acc: 0.5918 - val_loss: 0.9230 - val_acc: 0.5646\n",
      "Epoch 68/100\n",
      "829/829 [==============================] - 47s 56ms/step - loss: 0.8315 - acc: 0.5899 - val_loss: 0.9303 - val_acc: 0.5708\n",
      "Epoch 69/100\n",
      "829/829 [==============================] - 47s 57ms/step - loss: 0.8312 - acc: 0.5884 - val_loss: 0.9139 - val_acc: 0.5697\n",
      "Epoch 70/100\n",
      "829/829 [==============================] - 47s 56ms/step - loss: 0.8370 - acc: 0.5880 - val_loss: 0.9308 - val_acc: 0.5761\n",
      "Epoch 71/100\n",
      "829/829 [==============================] - 48s 58ms/step - loss: 0.8308 - acc: 0.5918 - val_loss: 0.9080 - val_acc: 0.5782\n",
      "Epoch 72/100\n",
      "829/829 [==============================] - 47s 57ms/step - loss: 0.8274 - acc: 0.5930 - val_loss: 0.9083 - val_acc: 0.5647\n",
      "Epoch 73/100\n",
      "829/829 [==============================] - 47s 57ms/step - loss: 0.8281 - acc: 0.5896 - val_loss: 0.9118 - val_acc: 0.5726\n",
      "Epoch 74/100\n",
      "829/829 [==============================] - 48s 58ms/step - loss: 0.8297 - acc: 0.5930 - val_loss: 0.9067 - val_acc: 0.5674\n",
      "Epoch 75/100\n",
      "829/829 [==============================] - 47s 56ms/step - loss: 0.8297 - acc: 0.5910 - val_loss: 0.9180 - val_acc: 0.5617\n",
      "Epoch 76/100\n",
      "829/829 [==============================] - 47s 56ms/step - loss: 0.8312 - acc: 0.5911 - val_loss: 0.9033 - val_acc: 0.5723\n",
      "Epoch 77/100\n",
      "829/829 [==============================] - 47s 56ms/step - loss: 0.8267 - acc: 0.5941 - val_loss: 0.9436 - val_acc: 0.5671\n",
      "Epoch 78/100\n",
      "829/829 [==============================] - 47s 57ms/step - loss: 0.8377 - acc: 0.5916 - val_loss: 0.9362 - val_acc: 0.5646\n",
      "Epoch 79/100\n",
      "829/829 [==============================] - 47s 56ms/step - loss: 0.8355 - acc: 0.5889 - val_loss: 0.9048 - val_acc: 0.5650\n",
      "Epoch 80/100\n",
      "829/829 [==============================] - 46s 56ms/step - loss: 0.8310 - acc: 0.5891 - val_loss: 0.9471 - val_acc: 0.5562\n",
      "Epoch 81/100\n",
      "829/829 [==============================] - 47s 56ms/step - loss: 0.8283 - acc: 0.5949 - val_loss: 0.9160 - val_acc: 0.5575\n",
      "Epoch 82/100\n",
      "829/829 [==============================] - 47s 57ms/step - loss: 0.8265 - acc: 0.5950 - val_loss: 0.9155 - val_acc: 0.5593\n",
      "Epoch 83/100\n",
      "829/829 [==============================] - 47s 57ms/step - loss: 0.8279 - acc: 0.5906 - val_loss: 0.9201 - val_acc: 0.5509\n",
      "Epoch 84/100\n",
      "829/829 [==============================] - 47s 57ms/step - loss: 0.8313 - acc: 0.5904 - val_loss: 0.9273 - val_acc: 0.5550\n",
      "Epoch 85/100\n",
      "829/829 [==============================] - 48s 58ms/step - loss: 0.8272 - acc: 0.5937 - val_loss: 0.9379 - val_acc: 0.5502\n",
      "Epoch 86/100\n",
      "829/829 [==============================] - 48s 58ms/step - loss: 0.8301 - acc: 0.5942 - val_loss: 0.9548 - val_acc: 0.5600\n",
      "Epoch 87/100\n",
      "829/829 [==============================] - 48s 57ms/step - loss: 0.8275 - acc: 0.5910 - val_loss: 0.9236 - val_acc: 0.5460\n",
      "Epoch 88/100\n",
      "829/829 [==============================] - 47s 57ms/step - loss: 0.8314 - acc: 0.5917 - val_loss: 0.9542 - val_acc: 0.5517\n",
      "Epoch 89/100\n",
      "829/829 [==============================] - 47s 57ms/step - loss: 0.8291 - acc: 0.5917 - val_loss: 0.9071 - val_acc: 0.5541\n",
      "Epoch 90/100\n",
      "829/829 [==============================] - 47s 57ms/step - loss: 0.8281 - acc: 0.5905 - val_loss: 0.9102 - val_acc: 0.5543\n",
      "Epoch 91/100\n",
      "829/829 [==============================] - 47s 57ms/step - loss: 0.8261 - acc: 0.5896 - val_loss: 0.9505 - val_acc: 0.5584\n",
      "Epoch 92/100\n",
      "829/829 [==============================] - 48s 58ms/step - loss: 0.8302 - acc: 0.5905 - val_loss: 0.9113 - val_acc: 0.5602\n",
      "Epoch 93/100\n",
      "829/829 [==============================] - 47s 57ms/step - loss: 0.8324 - acc: 0.5943 - val_loss: 0.8880 - val_acc: 0.5655\n",
      "Epoch 94/100\n",
      "829/829 [==============================] - 47s 57ms/step - loss: 0.8257 - acc: 0.5951 - val_loss: 0.8901 - val_acc: 0.5727\n",
      "Epoch 95/100\n",
      "829/829 [==============================] - 48s 57ms/step - loss: 0.8339 - acc: 0.5888 - val_loss: 0.9104 - val_acc: 0.5540\n",
      "Epoch 96/100\n",
      "829/829 [==============================] - 47s 57ms/step - loss: 0.8267 - acc: 0.5937 - val_loss: 0.9168 - val_acc: 0.5556\n",
      "Epoch 97/100\n",
      "829/829 [==============================] - 47s 57ms/step - loss: 0.8317 - acc: 0.5879 - val_loss: 0.9103 - val_acc: 0.5626\n",
      "Epoch 98/100\n",
      "829/829 [==============================] - 47s 57ms/step - loss: 0.8325 - acc: 0.5947 - val_loss: 0.9545 - val_acc: 0.5490\n",
      "Epoch 99/100\n",
      "829/829 [==============================] - 47s 57ms/step - loss: 0.8258 - acc: 0.5974 - val_loss: 0.9217 - val_acc: 0.5520\n",
      "Epoch 100/100\n",
      "829/829 [==============================] - 47s 57ms/step - loss: 0.8253 - acc: 0.5985 - val_loss: 0.9459 - val_acc: 0.5656\n",
      "Test score: 0.9398466561151587\n",
      "Test accuracy: 0.5646739130434782\n"
     ]
    }
   ],
   "source": [
    "modelCNN = Sequential()\n",
    "modelCNN.add(Conv2D(numNeuronsC1, (convKernelSize, convKernelSize), padding='same', input_shape=inputShape,kernel_initializer=initializers.glorot_uniform(seed=42)))\n",
    "modelCNN.add(Activation(\"elu\"))\n",
    "modelCNN.add(MaxPooling2D(pool_size=(poolSize, poolSize)))\n",
    "modelCNN.add(Dropout(0.16))\n",
    "modelCNN.add(Conv2D(numNeuronsC2, (convKernelSize, convKernelSize), padding='same', kernel_initializer=initializers.glorot_uniform(seed=42)))\n",
    "modelCNN.add(Activation(\"elu\"))\n",
    "modelCNN.add(MaxPooling2D(pool_size=(poolSize, poolSize)))\n",
    "modelCNN.add(Dropout(0.21))\n",
    "modelCNN.add(Conv2D(numNeuronsC3, (convKernelSize, convKernelSize), padding='same', kernel_initializer=initializers.glorot_uniform(seed=42)))\n",
    "modelCNN.add(Activation(\"elu\"))\n",
    "modelCNN.add(MaxPooling2D(pool_size=(poolSize, poolSize)))       \n",
    "modelCNN.add(Dropout(0.44))\n",
    "modelCNN.add(Conv2D(numNeuronsC4, (convKernelSize, convKernelSize), padding='same', kernel_initializer=initializers.glorot_uniform(seed=42)))\n",
    "modelCNN.add(Activation(\"elu\"))\n",
    "modelCNN.add(MaxPooling2D(pool_size=(poolSize, poolSize)))       \n",
    "modelCNN.add(Dropout(0.13))\n",
    "modelCNN.add(Conv2D(numNeuronsC5, (convKernelSize, convKernelSize), padding='same', kernel_initializer=initializers.glorot_uniform(seed=42)))\n",
    "modelCNN.add(Activation(\"elu\"))\n",
    "modelCNN.add(MaxPooling2D(pool_size=(poolSize, poolSize)))       \n",
    "modelCNN.add(Dropout(0.40))\n",
    "modelCNN.add(Flatten())\n",
    "modelCNN.add(Dense(numNeuronsD1, kernel_initializer=initializers.glorot_uniform(seed=42)))\n",
    "modelCNN.add(Activation(\"elu\"))\n",
    "modelCNN.add(Dropout(0.07)) \n",
    "modelCNN.add(Dense(3, kernel_initializer=initializers.glorot_uniform(seed=42)))\n",
    "modelCNN.add(Activation('softmax'))\n",
    "            \n",
    "modelCNN.compile(loss='categorical_crossentropy', optimizer=\"Adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "earlyStopping  = cb.EarlyStopping(monitor='val_acc', patience=100, verbose=1, mode='max')\n",
    "checkpointSafe = cb.ModelCheckpoint('ergebnisse_versuch5/modell_versuch5_' + experimentNumber, monitor='val_acc', save_best_only=True)   \n",
    "hist = modelCNN.fit_generator(dataLoader(xTrain, yTrain, batchSize), epochs=100, steps_per_epoch=(int(len(xTrain)/batchSize)),\n",
    "              validation_data=dataLoader(xVal, yVal, batchSize), validation_steps=(int(len(xVal)/batchSize)), callbacks=[earlyStopping,checkpointSafe])\n",
    "\n",
    "score, acc = modelCNN.evaluate_generator( dataLoader(xTest, yTest, batchSize), steps=(int(len(xTest)/batchSize)))\n",
    "print('Test score: '    +  str(score))\n",
    "print('Test accuracy: ' +  str(acc)  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-12T15:16:12.407192Z",
     "start_time": "2018-07-12T15:16:11.936402Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot Modell Historie Loss und Accuracy\n",
    "plt.figure(1)\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.title('Trainingshistorie: Loss')\n",
    "plt.xlabel('Epoche')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(hist.history['loss'])\n",
    "plt.plot(hist.history['val_loss'])\n",
    "plt.legend(['Training', 'Validation'])\n",
    "plt.xlim(0,99)\n",
    "plt.grid(True)\n",
    "plt.savefig(\"trainingshistorieLossVersuch5_\" + experimentNumber + \".png\")\n",
    "plt.xticks(np.arange(0, 99.1, step=10))\n",
    "plt.figure(2)\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.title('Trainingshistorie: Accuracy')\n",
    "plt.xticks()\n",
    "plt.xlabel('Epoche')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(hist.history['acc'])\n",
    "plt.plot(hist.history['val_acc'])\n",
    "plt.legend(['Training', 'Validation'], loc='right')\n",
    "plt.xlim(0,99)\n",
    "plt.xticks(np.arange(0, 99.1, step=10))\n",
    "plt.grid(True)\n",
    "plt.savefig(\"trainingshistorieAccuracyVersuch5_\" + experimentNumber + \".png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-12T14:13:49.087170Z",
     "start_time": "2018-07-12T14:13:46.923275Z"
    }
   },
   "outputs": [],
   "source": [
    "# Läd Modell\n",
    "modell1 = load_model('ergebnisse_versuch5/modell_versuch5_5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-12T14:13:53.775569Z",
     "start_time": "2018-07-12T14:13:50.049967Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8937412567760633, 0.5796195652173913]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modell1.evaluate_generator( dataLoader(xTest, yTest, 16), steps=int(len(xTest)/16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-12T14:21:06.429982Z",
     "start_time": "2018-07-12T14:21:06.405756Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_6 (Conv2D)            (None, 368, 70, 32)       896       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 368, 70, 32)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 184, 35, 32)       0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 184, 35, 32)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 184, 35, 32)       9248      \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 184, 35, 32)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 92, 17, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 92, 17, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 92, 17, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 92, 17, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 46, 8, 64)         0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 46, 8, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 46, 8, 64)         36928     \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 46, 8, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 23, 4, 64)         0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 23, 4, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 23, 4, 64)         36928     \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 23, 4, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 11, 2, 64)         0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 11, 2, 64)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 1408)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                90176     \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 3)                 195       \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 192,867\n",
      "Trainable params: 192,867\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "modell1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-12T12:35:56.058258Z",
     "start_time": "2018-07-12T12:35:34.229492Z"
    }
   },
   "outputs": [],
   "source": [
    "# Bei der Verwendung von predict_generator werden die daten gemischt \n",
    "validPreds = []\n",
    "imageList = []\n",
    "for path in xTest:\n",
    "    imageList = []   \n",
    "    img = cv2.cvtColor(cv2.imread(path),cv2.COLOR_BGR2RGB)\n",
    "    img = np.array(img)\n",
    "    img = img.astype('float32')\n",
    "    img /= 255\n",
    "    imageList.append(img)\n",
    "    validPreds.append(modell1.predict(np.asarray(imageList)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-12T12:35:57.239782Z",
     "start_time": "2018-07-12T12:35:57.056206Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2014  236   16]\n",
      " [ 857  808  574]\n",
      " [ 189  589 1478]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Konfusionsmatrix\n",
    "validPredArray = np.argmax(np.vstack(validPreds), axis=1)\n",
    "yTestMax = np.argmax(yTest,axis=1)\n",
    "cnfMatrix = confusion_matrix(yTestMax, validPredArray)\n",
    "print(cnfMatrix)\n",
    "fig, ax = plt.subplots(figsize=(6,6)) \n",
    "ax = sns.heatmap(cnfMatrix, fmt=\"d\", cmap=plt.cm.Blues, ax=ax , annot=True)\n",
    "ax.set_xticklabels(classNames)\n",
    "ax.set_yticklabels(classNames)\n",
    "plt.title('Konfusionsmatrix Versuch 5.4')\n",
    "plt.ylabel('Wahre Klasse')\n",
    "plt.xlabel('Vorhergesagte Klasse')\n",
    "plt.savefig('konfmatrixVersuch5_4.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-10T14:38:30.844033Z",
     "start_time": "2018-07-10T14:38:30.808651Z"
    }
   },
   "outputs": [],
   "source": [
    "# Eine Funktion die das zu optimierende Keras-Modell beschreibt\n",
    "# Die vorgehensweise mit einer Funktion ist nach der Dokumentation von Hyperas vorgegeben \n",
    "# siehe https://github.com/maxpumperla/hyperas\n",
    "def model(xTrain, xVal, yTrain, yVal):\n",
    "    # Parameter für das CNN\n",
    "    inputShape     = (368, 70, 3)   # Eingangs Array-Form \n",
    "    numNeuronsC1   = 32                # Anzahl der Filter / 1 Faltungsschicht\n",
    "    poolSize       = 2                 # Größe der Pooling-Layer\n",
    "    convKernelSize = 3                 # Größe des Faltungskern n*n\n",
    "    batchSize      = {{choice([8, 16, 32])}}\n",
    "    print(\"Stapelgroesse (batchSize): \" + str(batchSize))\n",
    "    \n",
    "    model = Sequential()\n",
    "    layerCountTuning = {{choice(['3Layer','4Layer','5Layer'])}}\n",
    "    print(\"Anzahl der Faltungsschichten: \" + layerCountTuning)\n",
    "    af = {{choice(['relu', 'elu'])}}\n",
    "    print(\"Aktivierungsfunktion: \" + af)\n",
    "    optf = {{choice(['RMSprop','Adam'])}}\n",
    "    print(\"Optimierungsfunktion: \" + optf)\n",
    "    model.add(Conv2D(numNeuronsC1, (convKernelSize, convKernelSize), padding='same', input_shape=inputShape,kernel_initializer=initializers.glorot_uniform(seed=42)))\n",
    "    model.add(Activation(af))\n",
    "    model.add(MaxPooling2D(pool_size=(poolSize, poolSize)))\n",
    "    dropoutrate1 = {{uniform(0, 0.70)}}\n",
    "    print(\"Dropout-Rate Faltungsschicht 1: \" + str(dropoutrate1))\n",
    "    model.add(Dropout(dropoutrate1))\n",
    "    \n",
    "    filterCount2 = {{choice([32, 64])}}\n",
    "    print(\"Anzahl der Filter-Maps Faltungsschicht 2: \" + str(filterCount2))\n",
    "    model.add(Conv2D(filterCount2, (convKernelSize, convKernelSize), padding='same', kernel_initializer=initializers.glorot_uniform(seed=42)))\n",
    "    model.add(Activation(af))\n",
    "    model.add(MaxPooling2D(pool_size=(poolSize, poolSize)))\n",
    "    dropoutrate2 = {{uniform(0, 0.70)}}        \n",
    "    print(\"Dropout-Rate Faltungsschicht 2: \" + str(dropoutrate2))\n",
    "    model.add(Dropout(dropoutrate2))\n",
    "    \n",
    "    if layerCountTuning == '3Layer' or layerCountTuning == '4Layer' or layerCountTuning == '5Layer':\n",
    "        filterCount3 = {{choice([64, 128])}}\n",
    "        print(\"Anzahl der Filter-Maps Faltungsschicht 3: \" + str(filterCount3))\n",
    "        model.add(Conv2D(filterCount3, (convKernelSize, convKernelSize), padding='same', kernel_initializer=initializers.glorot_uniform(seed=42)))\n",
    "        model.add(Activation(af))\n",
    "        model.add(MaxPooling2D(pool_size=(poolSize, poolSize)))\n",
    "        dropoutrate3 = {{uniform(0, 0.70)}}        \n",
    "        print(\"Dropout-Rate Faltungsschicht 3: \" + str(dropoutrate3))\n",
    "        model.add(Dropout(dropoutrate3))\n",
    "\n",
    "    if layerCountTuning == '4Layer' or layerCountTuning == '5Layer':\n",
    "        filterCount4 = {{choice([64, 128])}}\n",
    "        print(\"Anzahl der Filter-Maps Faltungsschicht 4: \" + str(filterCount4))\n",
    "        model.add(Conv2D(filterCount4, (convKernelSize, convKernelSize), padding='same', kernel_initializer=initializers.glorot_uniform(seed=42)))\n",
    "        model.add(Activation(af))\n",
    "        model.add(MaxPooling2D(pool_size=(poolSize, poolSize)))\n",
    "        dropoutrate4 = {{uniform(0, 0.70)}}        \n",
    "        print(\"Dropout-Rate Faltungsschicht 4: \" + str(dropoutrate4))\n",
    "        model.add(Dropout(dropoutrate4)) \n",
    "    \n",
    "    if layerCountTuning == '5Layer':\n",
    "        filterCount5 = {{choice([64, 128])}}\n",
    "        print(\"Anzahl der Filter-Maps Faltungsschicht 5: \" + str(filterCount5))\n",
    "        model.add(Conv2D(filterCount5, (convKernelSize, convKernelSize), padding='same', kernel_initializer=initializers.glorot_uniform(seed=42)))\n",
    "        model.add(Activation(af))\n",
    "        model.add(MaxPooling2D(pool_size=(poolSize, poolSize)))\n",
    "        dropoutrate5 = {{uniform(0, 0.70)}}        \n",
    "        print(\"Dropout-Rate Faltungsschicht 5: \" + str(dropoutrate5))\n",
    "        model.add(Dropout(dropoutrate5)) \n",
    "\n",
    "    model.add(Flatten())\n",
    "    dims4 = {{choice([64, 128])}}\n",
    "    print(\"Anzahl der Neuronen des Fully Connected Layer: \" + str(dims4))    \n",
    "    model.add(Dense(dims4, kernel_initializer=initializers.glorot_uniform(seed=42)))\n",
    "    model.add(Activation(af))\n",
    "    dropoutFull = {{uniform(0, 0.70)}}  \n",
    "    print(\"Dropout-Rate Fully Connected Layer: \" + str(dropoutFull))\n",
    "    model.add(Dropout(dropoutFull)) \n",
    "    model.add(Dense(3, kernel_initializer=initializers.glorot_uniform(seed=42)))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    # Diese Funktion läd Bilder in den Hauptspeicher\n",
    "    # imagesPaths: Liste mit Pfaden zu den Bildern != null\n",
    "    def imageLoader(imagePaths):\n",
    "        images = []\n",
    "        for path in imagePaths:\n",
    "            images.append(cv2.cvtColor(cv2.imread(path),cv2.COLOR_BGR2RGB))    \n",
    "        imagesNp = np.array(images)\n",
    "        imagesNp = imagesNp.astype('float32')\n",
    "        imagesNp /= 255\n",
    "        return imagesNp\n",
    "\n",
    "    # Läd Trainingsdaten in batches\n",
    "    def dataLoader(imagePaths, features, batchSize):\n",
    "        imagesCount= len(imagePaths)  \n",
    "        while True:\n",
    "            batchStart = 0\n",
    "            batchEnd = batchSize\n",
    "            while batchStart < imagesCount:\n",
    "                limit = min(batchEnd, imagesCount)\n",
    "                x = imageLoader(imagePaths[batchStart:limit])\n",
    "                y = features[batchStart:limit]\n",
    "                yield (x,y) \n",
    "                batchStart += batchSize   \n",
    "                batchEnd += batchSize\n",
    "                \n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optf, metrics=[\"accuracy\"])\n",
    "    print('Faltungsnetz wird trainiert...')\n",
    "    # Early Stopping unterbricht das Training, wenn nach 10 Epochen die Kostenfunktion nicht weiter minimiert werden konnte \n",
    "    earlyStopping = cb.EarlyStopping(monitor='val_acc', patience=10, verbose=1, mode='max')\n",
    "    checkpointSafe = cb.ModelCheckpoint('ergebnisse_versuch5/modell_versuch5_3', monitor='val_acc', save_best_only=True)   \n",
    "    model.fit_generator(dataLoader(xTrain, yTrain, batchSize), epochs=10, steps_per_epoch=(int(len(xTrain)/batchSize)),\n",
    "              validation_data=dataLoader(xVal, yVal, batchSize), validation_steps=(int(len(xVal)/batchSize)), callbacks=[earlyStopping,checkpointSafe])\n",
    "    score, acc = model.evaluate_generator( dataLoader(xVal, yVal, batchSize), steps=(int(len(xVal)/batchSize)))\n",
    "    print('Test score: ' + str(score))\n",
    "    print('Test accuracy: ' +  str(acc))\n",
    "    # Die Rückgabewerte werden verarbeitet von Hyperas\n",
    "    # loss ist die Kostenfunktion welche minimiert werden soll mit Hyperas\n",
    "    # status (STATUS_OK) gibt an das, dass Modell erfolgreich ausgeführt wurde\n",
    "    return {'loss': -acc, 'status': STATUS_OK, 'model': model}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-10T14:38:31.644985Z",
     "start_time": "2018-07-10T14:38:31.632990Z"
    }
   },
   "outputs": [],
   "source": [
    "def data():\n",
    "    # Hier können die Datensätze ausgewählt werden\n",
    "    datasets = ['43','45','46','47','48','49','50','51']\n",
    "    # Die Pfade zu den Ordnern in welchem sich die Bilder befinden\n",
    "    paths = []\n",
    "    # Liste mit Pfaden zu den Bildern\n",
    "    imagePaths = []\n",
    "    for dataset in datasets: # Für jeden Datensatz merke Pfad\n",
    "        paths.append(\"C:/Users/morro/Documents/datenRoh/\" + dataset + \"/zugeschnitten/\")\n",
    "    for path in paths: # Für jeden Pfad hole die Namen der Ordner\n",
    "        folders = os.listdir(path)\n",
    "        folders = sorted(folders, key=int) #sortiert die Reihenfolge de Ordner aufsteifend\n",
    "        print(path)\n",
    "        print(\"Bilder aus folgenden Ordnern werden geladen: \" + str(folders))\n",
    "        for folder in folders: # Aus der Liste der Ordner wird ein Ordner ausgewählt\n",
    "            filesPath = path + folder + \"/\"\n",
    "            files = os.listdir(filesPath)\n",
    "            print(\"Ordner der geladen wird: \" + str(folder))\n",
    "            for name in files: # Ein Dateiname aus diesem Ordner\n",
    "                if \"jpg\" not in name:\n",
    "                    continue\n",
    "                imagePaths.append(filesPath + name)\n",
    "    # Y Klassen Labels zuweisen\n",
    "    featuresDf = pandas.read_csv(filepath_or_buffer=\"../daten/merkmale_datensatz_43_45_bis_51/merkmaleMitLabelnFuzzyVersuch6.csv\")\n",
    "    yLabels = np_utils.to_categorical(featuresDf['Klasse'], 0)\n",
    "    # Setzten des RandomState um reproduzierbare Ergebnisse zu erzielen.\n",
    "    np.random.seed(42)\n",
    "    # Mischen der Trainingsdaten\n",
    "    xShuffle, yShuffle = shuffle(imagePaths,yLabels)\n",
    "    class1Number = 0\n",
    "    class2Number = 0 \n",
    "    class3Number = 0\n",
    "    class4Number = 0\n",
    "    class5Number = 0\n",
    "    class6Number = 0\n",
    "    class7Number = 0\n",
    "    class8Number = 0\n",
    "    maxClasses = featuresDf[\"Klasse\"].value_counts().min()\n",
    "    indexToDelete = [] \n",
    "    i = -1\n",
    "    for label in yShuffle:\n",
    "        i = i + 1\n",
    "        labelNumber = np.argmax(label,axis=0)\n",
    "        if labelNumber == 0 and class1Number < maxClasses:\n",
    "            class1Number = class1Number + 1\n",
    "            continue\n",
    "        elif labelNumber == 0:\n",
    "            indexToDelete.append(i)\n",
    "        if labelNumber == 1 and class2Number < maxClasses:\n",
    "            class2Number = class2Number + 1\n",
    "            continue\n",
    "        elif labelNumber == 1:\n",
    "            indexToDelete.append(i)\n",
    "        if labelNumber == 2 and class3Number < maxClasses:\n",
    "            class3Number = class3Number + 1\n",
    "            continue\n",
    "        elif labelNumber == 2:\n",
    "            indexToDelete.append(i)\n",
    "        if labelNumber == 3 and class4Number < maxClasses:\n",
    "            class4Number = class4Number + 1\n",
    "            continue        \n",
    "        elif labelNumber == 3:\n",
    "            indexToDelete.append(i)\n",
    "        if labelNumber == 4 and class5Number < maxClasses:\n",
    "            class5Number = class5Number + 1\n",
    "            continue\n",
    "        elif labelNumber == 4:\n",
    "            indexToDelete.append(i)\n",
    "        if labelNumber == 5 and class6Number < maxClasses:\n",
    "            class6Number = class6Number + 1\n",
    "            continue\n",
    "        elif labelNumber == 5:\n",
    "            indexToDelete.append(i)\n",
    "        if labelNumber == 6 and class7Number < maxClasses:\n",
    "            class7Number = class7Number + 1\n",
    "            continue\n",
    "        elif labelNumber == 6:\n",
    "            indexToDelete.append(i)\n",
    "        if labelNumber == 7 and class8Number < maxClasses:\n",
    "            class8Number = class8Number + 1\n",
    "            continue\n",
    "        elif labelNumber == 7:\n",
    "            indexToDelete.append(i)\n",
    "\n",
    "    xShuffle = [i for j, i in enumerate(xShuffle) if j not in indexToDelete]\n",
    "    yShuffle = [i for j, i in enumerate(yShuffle) if j not in indexToDelete]\n",
    "    yShuffle = np.asarray(yShuffle)\n",
    "    xShuffle, yShuffle = shuffle(xShuffle,yShuffle)\n",
    "    # Aufteilung in Trainings und Testdaten\n",
    "    xTrain, xTest, yTrain, yTest = train_test_split(xShuffle, yShuffle, test_size=0.1)\n",
    "    xTrain, xVal, yTrain, yVal = train_test_split(xTrain, yTrain, test_size=0.2)\n",
    "    return xTrain, xVal, yTrain, yVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-07-10T14:38:32.507Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Imports:\n",
      "#coding=utf-8\n",
      "\n",
      "from __future__ import print_function\n",
      "\n",
      "try:\n",
      "    from hyperopt import Trials, STATUS_OK, rand\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas import optim\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas.distributions import uniform, choice\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import seaborn as sns\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import matplotlib.pyplot as plt\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import matplotlib.lines as mlines\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from mpl_toolkits.axes_grid1 import ImageGrid\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import os\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import cv2\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.utils import np_utils\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras import initializers\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.optimizers import RMSprop\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import load_model\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import keras.callbacks as cb\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.utils import shuffle\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.metrics import confusion_matrix\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import collections\n",
      "except:\n",
      "    pass\n",
      "\n",
      ">>> Hyperas search space:\n",
      "\n",
      "def get_space():\n",
      "    return {\n",
      "        'batchSize': hp.choice('batchSize', [8, 16, 32]),\n",
      "        'layerCountTuning': hp.choice('layerCountTuning', ['3Layer','4Layer','5Layer']),\n",
      "        'af': hp.choice('af', ['relu', 'elu']),\n",
      "        'optf': hp.choice('optf', ['RMSprop','Adam']),\n",
      "        'dropoutrate1': hp.uniform('dropoutrate1', 0, 0.70),\n",
      "        'filterCount2': hp.choice('filterCount2', [32, 64]),\n",
      "        'dropoutrate1_1': hp.uniform('dropoutrate1_1', 0, 0.70),\n",
      "        'filterCount3': hp.choice('filterCount3', [64, 128]),\n",
      "        'dropoutrate1_2': hp.uniform('dropoutrate1_2', 0, 0.70),\n",
      "        'filterCount3_1': hp.choice('filterCount3_1', [64, 128]),\n",
      "        'dropoutrate1_3': hp.uniform('dropoutrate1_3', 0, 0.70),\n",
      "        'filterCount3_2': hp.choice('filterCount3_2', [64, 128]),\n",
      "        'dropoutrate1_4': hp.uniform('dropoutrate1_4', 0, 0.70),\n",
      "        'filterCount3_3': hp.choice('filterCount3_3', [64, 128]),\n",
      "        'dropoutrate1_5': hp.uniform('dropoutrate1_5', 0, 0.70),\n",
      "    }\n",
      "\n",
      ">>> Data\n",
      "   1: \n",
      "   2: # Hier können die Datensätze ausgewählt werden\n",
      "   3: datasets = ['43','45','46','47','48','49','50','51']\n",
      "   4: # Die Pfade zu den Ordnern in welchem sich die Bilder befinden\n",
      "   5: paths = []\n",
      "   6: # Liste mit Pfaden zu den Bildern\n",
      "   7: imagePaths = []\n",
      "   8: for dataset in datasets: # Für jeden Datensatz merke Pfad\n",
      "   9:     paths.append(\"C:/Users/morro/Documents/datenRoh/\" + dataset + \"/zugeschnitten/\")\n",
      "  10: for path in paths: # Für jeden Pfad hole die Namen der Ordner\n",
      "  11:     folders = os.listdir(path)\n",
      "  12:     folders = sorted(folders, key=int) #sortiert die Reihenfolge de Ordner aufsteifend\n",
      "  13:     print(path)\n",
      "  14:     print(\"Bilder aus folgenden Ordnern werden geladen: \" + str(folders))\n",
      "  15:     for folder in folders: # Aus der Liste der Ordner wird ein Ordner ausgewählt\n",
      "  16:         filesPath = path + folder + \"/\"\n",
      "  17:         files = os.listdir(filesPath)\n",
      "  18:         print(\"Ordner der geladen wird: \" + str(folder))\n",
      "  19:         for name in files: # Ein Dateiname aus diesem Ordner\n",
      "  20:             if \"jpg\" not in name:\n",
      "  21:                 continue\n",
      "  22:             imagePaths.append(filesPath + name)\n",
      "  23: # Y Klassen Labels zuweisen\n",
      "  24: featuresDf = pandas.read_csv(filepath_or_buffer=\"../daten/merkmale_datensatz_43_45_bis_51/merkmaleMitLabelnFuzzyVersuch6.csv\")\n",
      "  25: yLabels = np_utils.to_categorical(featuresDf['Klasse'], 0)\n",
      "  26: # Setzten des RandomState um reproduzierbare Ergebnisse zu erzielen.\n",
      "  27: np.random.seed(42)\n",
      "  28: # Mischen der Trainingsdaten\n",
      "  29: xShuffle, yShuffle = shuffle(imagePaths,yLabels)\n",
      "  30: class1Number = 0\n",
      "  31: class2Number = 0 \n",
      "  32: class3Number = 0\n",
      "  33: class4Number = 0\n",
      "  34: class5Number = 0\n",
      "  35: class6Number = 0\n",
      "  36: class7Number = 0\n",
      "  37: class8Number = 0\n",
      "  38: maxClasses = featuresDf[\"Klasse\"].value_counts().min()\n",
      "  39: indexToDelete = [] \n",
      "  40: i = -1\n",
      "  41: for label in yShuffle:\n",
      "  42:     i = i + 1\n",
      "  43:     labelNumber = np.argmax(label,axis=0)\n",
      "  44:     if labelNumber == 0 and class1Number < maxClasses:\n",
      "  45:         class1Number = class1Number + 1\n",
      "  46:         continue\n",
      "  47:     elif labelNumber == 0:\n",
      "  48:         indexToDelete.append(i)\n",
      "  49:     if labelNumber == 1 and class2Number < maxClasses:\n",
      "  50:         class2Number = class2Number + 1\n",
      "  51:         continue\n",
      "  52:     elif labelNumber == 1:\n",
      "  53:         indexToDelete.append(i)\n",
      "  54:     if labelNumber == 2 and class3Number < maxClasses:\n",
      "  55:         class3Number = class3Number + 1\n",
      "  56:         continue\n",
      "  57:     elif labelNumber == 2:\n",
      "  58:         indexToDelete.append(i)\n",
      "  59:     if labelNumber == 3 and class4Number < maxClasses:\n",
      "  60:         class4Number = class4Number + 1\n",
      "  61:         continue        \n",
      "  62:     elif labelNumber == 3:\n",
      "  63:         indexToDelete.append(i)\n",
      "  64:     if labelNumber == 4 and class5Number < maxClasses:\n",
      "  65:         class5Number = class5Number + 1\n",
      "  66:         continue\n",
      "  67:     elif labelNumber == 4:\n",
      "  68:         indexToDelete.append(i)\n",
      "  69:     if labelNumber == 5 and class6Number < maxClasses:\n",
      "  70:         class6Number = class6Number + 1\n",
      "  71:         continue\n",
      "  72:     elif labelNumber == 5:\n",
      "  73:         indexToDelete.append(i)\n",
      "  74:     if labelNumber == 6 and class7Number < maxClasses:\n",
      "  75:         class7Number = class7Number + 1\n",
      "  76:         continue\n",
      "  77:     elif labelNumber == 6:\n",
      "  78:         indexToDelete.append(i)\n",
      "  79:     if labelNumber == 7 and class8Number < maxClasses:\n",
      "  80:         class8Number = class8Number + 1\n",
      "  81:         continue\n",
      "  82:     elif labelNumber == 7:\n",
      "  83:         indexToDelete.append(i)\n",
      "  84: \n",
      "  85: xShuffle = [i for j, i in enumerate(xShuffle) if j not in indexToDelete]\n",
      "  86: yShuffle = [i for j, i in enumerate(yShuffle) if j not in indexToDelete]\n",
      "  87: yShuffle = np.asarray(yShuffle)\n",
      "  88: xShuffle, yShuffle = shuffle(xShuffle,yShuffle)\n",
      "  89: # Aufteilung in Trainings und Testdaten\n",
      "  90: xTrain, xTest, yTrain, yTest = train_test_split(xShuffle, yShuffle, test_size=0.1)\n",
      "  91: xTrain, xVal, yTrain, yVal = train_test_split(xTrain, yTrain, test_size=0.2)\n",
      "  92: \n",
      "  93: \n",
      "  94: \n",
      ">>> Resulting replaced keras model:\n",
      "\n",
      "   1: def keras_fmin_fnct(space):\n",
      "   2: \n",
      "   3:     # Parameter für das CNN\n",
      "   4:     inputShape     = (368, 70, 3)   # Eingangs Array-Form \n",
      "   5:     numNeuronsC1   = 32                # Anzahl der Filter / 1 Faltungsschicht\n",
      "   6:     poolSize       = 2                 # Größe der Pooling-Layer\n",
      "   7:     convKernelSize = 3                 # Größe des Faltungskern n*n\n",
      "   8:     batchSize      = space['batchSize']\n",
      "   9:     print(\"Stapelgroesse (batchSize): \" + str(batchSize))\n",
      "  10:     \n",
      "  11:     model = Sequential()\n",
      "  12:     layerCountTuning = space['layerCountTuning']\n",
      "  13:     print(\"Anzahl der Faltungsschichten: \" + layerCountTuning)\n",
      "  14:     af = space['af']\n",
      "  15:     print(\"Aktivierungsfunktion: \" + af)\n",
      "  16:     optf = space['optf']\n",
      "  17:     print(\"Optimierungsfunktion: \" + optf)\n",
      "  18:     model.add(Conv2D(numNeuronsC1, (convKernelSize, convKernelSize), padding='same', input_shape=inputShape,kernel_initializer=initializers.glorot_uniform(seed=42)))\n",
      "  19:     model.add(Activation(af))\n",
      "  20:     model.add(MaxPooling2D(pool_size=(poolSize, poolSize)))\n",
      "  21:     dropoutrate1 = space['dropoutrate1']\n",
      "  22:     print(\"Dropout-Rate Faltungsschicht 1: \" + str(dropoutrate1))\n",
      "  23:     model.add(Dropout(dropoutrate1))\n",
      "  24:     \n",
      "  25:     filterCount2 = space['filterCount2']\n",
      "  26:     print(\"Anzahl der Filter-Maps Faltungsschicht 2: \" + str(filterCount2))\n",
      "  27:     model.add(Conv2D(filterCount2, (convKernelSize, convKernelSize), padding='same', kernel_initializer=initializers.glorot_uniform(seed=42)))\n",
      "  28:     model.add(Activation(af))\n",
      "  29:     model.add(MaxPooling2D(pool_size=(poolSize, poolSize)))\n",
      "  30:     dropoutrate2 = space['dropoutrate1_1']        \n",
      "  31:     print(\"Dropout-Rate Faltungsschicht 2: \" + str(dropoutrate2))\n",
      "  32:     model.add(Dropout(dropoutrate2))\n",
      "  33:     \n",
      "  34:     if layerCountTuning == '3Layer' or layerCountTuning == '4Layer' or layerCountTuning == '5Layer':\n",
      "  35:         print(\"Anzahl der Faltungsschichten: \" + layerCountTuning)\n",
      "  36:         filterCount3 = space['filterCount3']\n",
      "  37:         print(\"Anzahl der Filter-Maps Faltungsschicht 3: \" + str(filterCount3))\n",
      "  38:         model.add(Conv2D(filterCount3, (convKernelSize, convKernelSize), padding='same', kernel_initializer=initializers.glorot_uniform(seed=42)))\n",
      "  39:         model.add(Activation(af))\n",
      "  40:         model.add(MaxPooling2D(pool_size=(poolSize, poolSize)))\n",
      "  41:         dropoutrate3 = space['dropoutrate1_2']        \n",
      "  42:         print(\"Dropout-Rate Faltungsschicht 3: \" + str(dropoutrate3))\n",
      "  43:         model.add(Dropout(dropoutrate3))\n",
      "  44: \n",
      "  45:     if layerCountTuning == '4Layer' or layerCountTuning == '5Layer':\n",
      "  46:         filterCount4 = space['filterCount3_1']\n",
      "  47:         print(\"Anzahl der Filter-Maps Faltungsschicht 4: \" + str(filterCount4))\n",
      "  48:         model.add(Conv2D(filterCount4, (convKernelSize, convKernelSize), padding='same', kernel_initializer=initializers.glorot_uniform(seed=42)))\n",
      "  49:         model.add(Activation(af))\n",
      "  50:         model.add(MaxPooling2D(pool_size=(poolSize, poolSize)))\n",
      "  51:         dropoutrate4 = space['dropoutrate1_3']        \n",
      "  52:         print(\"Dropout-Rate Faltungsschicht 4: \" + str(dropoutrate4))\n",
      "  53:         model.add(Dropout(dropoutrate4)) \n",
      "  54:     \n",
      "  55:     if layerCountTuning == '5Layer':\n",
      "  56:         filterCount5 = space['filterCount3_2']\n",
      "  57:         print(\"Anzahl der Filter-Maps Faltungsschicht 5: \" + str(filterCount5))\n",
      "  58:         model.add(Conv2D(filterCount5, (convKernelSize, convKernelSize), padding='same', kernel_initializer=initializers.glorot_uniform(seed=42)))\n",
      "  59:         model.add(Activation(af))\n",
      "  60:         model.add(MaxPooling2D(pool_size=(poolSize, poolSize)))\n",
      "  61:         dropoutrate5 = space['dropoutrate1_4']        \n",
      "  62:         print(\"Dropout-Rate Faltungsschicht 5: \" + str(dropoutrate5))\n",
      "  63:         model.add(Dropout(dropoutrate5)) \n",
      "  64: \n",
      "  65:     model.add(Flatten())\n",
      "  66:     dims4 = space['filterCount3_3']\n",
      "  67:     print(\"Anzahl der Neuronen des Fully Connected Layer: \" + str(dims4))    \n",
      "  68:     model.add(Dense(dims4, kernel_initializer=initializers.glorot_uniform(seed=42)))\n",
      "  69:     model.add(Activation(af))\n",
      "  70:     dropoutFull = space['dropoutrate1_5']  \n",
      "  71:     print(\"Dropout-Rate Fully Connected Layer: \" + str(dropoutFull))\n",
      "  72:     model.add(Dropout(dropoutFull)) \n",
      "  73:     model.add(Dense(3, kernel_initializer=initializers.glorot_uniform(seed=42)))\n",
      "  74:     model.add(Activation('softmax'))\n",
      "  75:     \n",
      "  76:     # Diese Funktion läd Bilder in den Hauptspeicher\n",
      "  77:     # imagesPaths: Liste mit Pfaden zu den Bildern != null\n",
      "  78:     def imageLoader(imagePaths):\n",
      "  79:         images = []\n",
      "  80:         for path in imagePaths:\n",
      "  81:             images.append(cv2.cvtColor(cv2.imread(path),cv2.COLOR_BGR2RGB))    \n",
      "  82:         imagesNp = np.array(images)\n",
      "  83:         imagesNp = imagesNp.astype('float32')\n",
      "  84:         imagesNp /= 255\n",
      "  85:         return imagesNp\n",
      "  86: \n",
      "  87:     # Läd Trainingsdaten in batches\n",
      "  88:     def dataLoader(imagePaths, features, batchSize):\n",
      "  89:         imagesCount= len(imagePaths)  \n",
      "  90:         while True:\n",
      "  91:             batchStart = 0\n",
      "  92:             batchEnd = batchSize\n",
      "  93:             while batchStart < imagesCount:\n",
      "  94:                 limit = min(batchEnd, imagesCount)\n",
      "  95:                 x = imageLoader(imagePaths[batchStart:limit])\n",
      "  96:                 y = features[batchStart:limit]\n",
      "  97:                 yield (x,y) \n",
      "  98:                 batchStart += batchSize   \n",
      "  99:                 batchEnd += batchSize\n",
      " 100:                 \n",
      " 101:     model.compile(loss='categorical_crossentropy', optimizer=optf, metrics=[\"accuracy\"])\n",
      " 102:     print('Faltungsnetz wird trainiert...')\n",
      " 103:     # Early Stopping unterbricht das Training, wenn nach 10 Epochen die Kostenfunktion nicht weiter minimiert werden konnte \n",
      " 104:     earlyStopping = cb.EarlyStopping(monitor='val_acc', patience=10, verbose=1, mode='max')\n",
      " 105:     checkpointSafe = cb.ModelCheckpoint('ergebnisse_versuch5/modell_versuch5_3', monitor='val_acc', save_best_only=True)   \n",
      " 106:     model.fit_generator(dataLoader(xTrain, yTrain, batchSize), epochs=10, steps_per_epoch=(int(len(xTrain)/batchSize)),\n",
      " 107:               validation_data=dataLoader(xVal, yVal, batchSize), validation_steps=(int(len(xVal)/batchSize)), callbacks=[earlyStopping,checkpointSafe])\n",
      " 108:     score, acc = model.evaluate_generator( dataLoader(xVal, yVal, batchSize), steps=(int(len(xVal)/batchSize)))\n",
      " 109:     print('Test score: ' + str(score))\n",
      " 110:     print('Test accuracy: ' +  str(acc))\n",
      " 111:     # Die Rückgabewerte werden verarbeitet von Hyperas\n",
      " 112:     # loss ist die Kostenfunktion welche minimiert werden soll mit Hyperas\n",
      " 113:     # status (STATUS_OK) gibt an das, dass Modell erfolgreich ausgeführt wurde\n",
      " 114:     return {'loss': -acc, 'status': STATUS_OK, 'model': model}\n",
      " 115: \n",
      "C:/Users/morro/Documents/datenRoh/43/zugeschnitten/\n",
      "Bilder aus folgenden Ordnern werden geladen: ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10']\n",
      "Ordner der geladen wird: 1\n",
      "Ordner der geladen wird: 2\n",
      "Ordner der geladen wird: 3\n",
      "Ordner der geladen wird: 4\n",
      "Ordner der geladen wird: 5\n",
      "Ordner der geladen wird: 6\n",
      "Ordner der geladen wird: 7\n",
      "Ordner der geladen wird: 8\n",
      "Ordner der geladen wird: 9\n",
      "Ordner der geladen wird: 10\n",
      "C:/Users/morro/Documents/datenRoh/45/zugeschnitten/\n",
      "Bilder aus folgenden Ordnern werden geladen: ['1', '2', '3', '4', '5']\n",
      "Ordner der geladen wird: 1\n",
      "Ordner der geladen wird: 2\n",
      "Ordner der geladen wird: 3\n",
      "Ordner der geladen wird: 4\n",
      "Ordner der geladen wird: 5\n",
      "C:/Users/morro/Documents/datenRoh/46/zugeschnitten/\n",
      "Bilder aus folgenden Ordnern werden geladen: ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11']\n",
      "Ordner der geladen wird: 1\n",
      "Ordner der geladen wird: 2\n",
      "Ordner der geladen wird: 3\n",
      "Ordner der geladen wird: 4\n",
      "Ordner der geladen wird: 5\n",
      "Ordner der geladen wird: 6\n",
      "Ordner der geladen wird: 7\n",
      "Ordner der geladen wird: 8\n",
      "Ordner der geladen wird: 9\n",
      "Ordner der geladen wird: 10\n",
      "Ordner der geladen wird: 11\n",
      "C:/Users/morro/Documents/datenRoh/47/zugeschnitten/\n",
      "Bilder aus folgenden Ordnern werden geladen: ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10']\n",
      "Ordner der geladen wird: 1\n",
      "Ordner der geladen wird: 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ordner der geladen wird: 3\n",
      "Ordner der geladen wird: 4\n",
      "Ordner der geladen wird: 5\n",
      "Ordner der geladen wird: 6\n",
      "Ordner der geladen wird: 7\n",
      "Ordner der geladen wird: 8\n",
      "Ordner der geladen wird: 9\n",
      "Ordner der geladen wird: 10\n",
      "C:/Users/morro/Documents/datenRoh/48/zugeschnitten/\n",
      "Bilder aus folgenden Ordnern werden geladen: ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10']\n",
      "Ordner der geladen wird: 1\n",
      "Ordner der geladen wird: 2\n",
      "Ordner der geladen wird: 3\n",
      "Ordner der geladen wird: 4\n",
      "Ordner der geladen wird: 5\n",
      "Ordner der geladen wird: 6\n",
      "Ordner der geladen wird: 7\n",
      "Ordner der geladen wird: 8\n",
      "Ordner der geladen wird: 9\n",
      "Ordner der geladen wird: 10\n",
      "C:/Users/morro/Documents/datenRoh/49/zugeschnitten/\n",
      "Bilder aus folgenden Ordnern werden geladen: ['1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
      "Ordner der geladen wird: 1\n",
      "Ordner der geladen wird: 2\n",
      "Ordner der geladen wird: 3\n",
      "Ordner der geladen wird: 4\n",
      "Ordner der geladen wird: 5\n",
      "Ordner der geladen wird: 6\n",
      "Ordner der geladen wird: 7\n",
      "Ordner der geladen wird: 8\n",
      "Ordner der geladen wird: 9\n",
      "C:/Users/morro/Documents/datenRoh/50/zugeschnitten/\n",
      "Bilder aus folgenden Ordnern werden geladen: ['1', '2', '3', '4', '5']\n",
      "Ordner der geladen wird: 1\n",
      "Ordner der geladen wird: 2\n",
      "Ordner der geladen wird: 3\n",
      "Ordner der geladen wird: 4\n",
      "Ordner der geladen wird: 5\n",
      "C:/Users/morro/Documents/datenRoh/51/zugeschnitten/\n",
      "Bilder aus folgenden Ordnern werden geladen: ['1', '2', '3', '4', '5']\n",
      "Ordner der geladen wird: 1\n",
      "Ordner der geladen wird: 2\n",
      "Ordner der geladen wird: 3\n",
      "Ordner der geladen wird: 4\n",
      "Ordner der geladen wird: 5\n",
      "Stapelgroesse (batchSize): 32\n",
      "Anzahl der Faltungsschichten: 4Layer\n",
      "Aktivierungsfunktion: elu\n",
      "Optimierungsfunktion: RMSprop\n",
      "Dropout-Rate Faltungsschicht 1: 0.36650111313577477\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 64\n",
      "Dropout-Rate Faltungsschicht 2: 0.5910892819828057\n",
      "Anzahl der Faltungsschichten: 4Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 128\n",
      "Dropout-Rate Faltungsschicht 3: 0.31073308941714595\n",
      "Anzahl der Filter-Maps Faltungsschicht 4: 128\n",
      "Dropout-Rate Faltungsschicht 4: 0.02787756283401593\n",
      "Anzahl der Neuronen des Fully Connected Layer: 64\n",
      "Dropout-Rate Fully Connected Layer: 0.22452694320321762\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "1521/1521 [==============================] - 108s 71ms/step - loss: 1.2174 - acc: 0.3969 - val_loss: 1.0893 - val_acc: 0.4145\n",
      "Epoch 2/10\n",
      "1521/1521 [==============================] - 143s 94ms/step - loss: 1.0000 - acc: 0.4847 - val_loss: 0.9058 - val_acc: 0.5399\n",
      "Epoch 3/10\n",
      "1521/1521 [==============================] - 173s 114ms/step - loss: 0.9269 - acc: 0.5401 - val_loss: 1.1243 - val_acc: 0.3870\n",
      "Epoch 4/10\n",
      "1521/1521 [==============================] - 105s 69ms/step - loss: 0.9000 - acc: 0.5576 - val_loss: 0.8534 - val_acc: 0.5890\n",
      "Epoch 5/10\n",
      "1521/1521 [==============================] - 105s 69ms/step - loss: 0.8841 - acc: 0.5681 - val_loss: 0.8903 - val_acc: 0.5624\n",
      "Epoch 6/10\n",
      "1521/1521 [==============================] - 105s 69ms/step - loss: 0.8758 - acc: 0.5797 - val_loss: 1.1957 - val_acc: 0.4783\n",
      "Epoch 7/10\n",
      "1521/1521 [==============================] - 105s 69ms/step - loss: 0.8672 - acc: 0.5807 - val_loss: 0.9035 - val_acc: 0.6017\n",
      "Epoch 8/10\n",
      "1521/1521 [==============================] - 105s 69ms/step - loss: 0.8626 - acc: 0.5843 - val_loss: 1.0176 - val_acc: 0.5696\n",
      "Epoch 9/10\n",
      "1521/1521 [==============================] - 105s 69ms/step - loss: 0.8636 - acc: 0.5834 - val_loss: 1.1003 - val_acc: 0.5476\n",
      "Epoch 10/10\n",
      "1521/1521 [==============================] - 105s 69ms/step - loss: 0.8587 - acc: 0.5880 - val_loss: 0.9441 - val_acc: 0.6005\n",
      "Test score: 0.9450120011442587\n",
      "Test accuracy: 0.6002467105263158\n",
      "Stapelgroesse (batchSize): 8\n",
      "Anzahl der Faltungsschichten: 5Layer\n",
      "Aktivierungsfunktion: relu\n",
      "Optimierungsfunktion: RMSprop\n",
      "Dropout-Rate Faltungsschicht 1: 0.3823049904758277\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 32\n",
      "Dropout-Rate Faltungsschicht 2: 0.04027144697613922\n",
      "Anzahl der Faltungsschichten: 5Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 128\n",
      "Dropout-Rate Faltungsschicht 3: 0.11001387944772216\n",
      "Anzahl der Filter-Maps Faltungsschicht 4: 64\n",
      "Dropout-Rate Faltungsschicht 4: 0.3809694643867013\n",
      "Anzahl der Filter-Maps Faltungsschicht 5: 64\n",
      "Dropout-Rate Faltungsschicht 5: 0.3038288829951559\n",
      "Anzahl der Neuronen des Fully Connected Layer: 128\n",
      "Dropout-Rate Fully Connected Layer: 0.48825528701971843\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "6084/6084 [==============================] - 139s 23ms/step - loss: 1.0654 - acc: 0.4259 - val_loss: 0.9995 - val_acc: 0.4756\n",
      "Epoch 2/10\n",
      "6084/6084 [==============================] - 138s 23ms/step - loss: 1.0450 - acc: 0.4697 - val_loss: 1.0295 - val_acc: 0.4346\n",
      "Epoch 3/10\n",
      "6084/6084 [==============================] - 136s 22ms/step - loss: 1.0672 - acc: 0.4602 - val_loss: 1.1404 - val_acc: 0.3376\n",
      "Epoch 4/10\n",
      "6084/6084 [==============================] - 136s 22ms/step - loss: 1.0892 - acc: 0.4414 - val_loss: 1.1144 - val_acc: 0.3393\n",
      "Epoch 5/10\n",
      "6084/6084 [==============================] - 135s 22ms/step - loss: 1.0950 - acc: 0.4430 - val_loss: 1.1337 - val_acc: 0.3336\n",
      "Epoch 6/10\n",
      "6084/6084 [==============================] - 135s 22ms/step - loss: 1.0928 - acc: 0.4549 - val_loss: 1.1301 - val_acc: 0.3496\n",
      "Epoch 7/10\n",
      "6084/6084 [==============================] - 137s 23ms/step - loss: 1.1431 - acc: 0.4543 - val_loss: 1.0707 - val_acc: 0.4120\n",
      "Epoch 8/10\n",
      "6084/6084 [==============================] - 137s 23ms/step - loss: 2.5590 - acc: 0.4345 - val_loss: 10.7321 - val_acc: 0.3342\n",
      "Epoch 9/10\n",
      "6084/6084 [==============================] - 136s 22ms/step - loss: 10.7494 - acc: 0.3331 - val_loss: 10.7335 - val_acc: 0.3341\n",
      "Epoch 10/10\n",
      "6084/6084 [==============================] - 136s 22ms/step - loss: 10.7457 - acc: 0.3333 - val_loss: 10.7321 - val_acc: 0.3342\n",
      "Test score: 10.732150633973092\n",
      "Test accuracy: 0.334155161078238\n",
      "Stapelgroesse (batchSize): 16\n",
      "Anzahl der Faltungsschichten: 5Layer\n",
      "Aktivierungsfunktion: elu\n",
      "Optimierungsfunktion: Adam\n",
      "Dropout-Rate Faltungsschicht 1: 0.4930316398594563\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 64\n",
      "Dropout-Rate Faltungsschicht 2: 0.5337907116491443\n",
      "Anzahl der Faltungsschichten: 5Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 128\n",
      "Dropout-Rate Faltungsschicht 3: 0.48407428062707253\n",
      "Anzahl der Filter-Maps Faltungsschicht 4: 128\n",
      "Dropout-Rate Faltungsschicht 4: 0.6068703632453584\n",
      "Anzahl der Filter-Maps Faltungsschicht 5: 64\n",
      "Dropout-Rate Faltungsschicht 5: 0.6803014606088199\n",
      "Anzahl der Neuronen des Fully Connected Layer: 64\n",
      "Dropout-Rate Fully Connected Layer: 0.10160055304815652\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "3042/3042 [==============================] - 125s 41ms/step - loss: 1.0797 - acc: 0.4143 - val_loss: 1.0112 - val_acc: 0.4655\n",
      "Epoch 2/10\n",
      "3042/3042 [==============================] - 124s 41ms/step - loss: 1.0341 - acc: 0.4486 - val_loss: 1.0249 - val_acc: 0.4699\n",
      "Epoch 3/10\n",
      "3042/3042 [==============================] - 128s 42ms/step - loss: 1.0109 - acc: 0.4744 - val_loss: 1.1377 - val_acc: 0.3840\n",
      "Epoch 4/10\n",
      "3042/3042 [==============================] - 129s 42ms/step - loss: 0.9997 - acc: 0.4947 - val_loss: 1.0241 - val_acc: 0.4103\n",
      "Epoch 5/10\n",
      "3042/3042 [==============================] - 127s 42ms/step - loss: 0.9893 - acc: 0.5016 - val_loss: 0.9399 - val_acc: 0.5420\n",
      "Epoch 6/10\n",
      "3042/3042 [==============================] - 124s 41ms/step - loss: 0.9811 - acc: 0.5047 - val_loss: 0.9524 - val_acc: 0.4707\n",
      "Epoch 7/10\n",
      "3042/3042 [==============================] - 124s 41ms/step - loss: 0.9680 - acc: 0.5132 - val_loss: 0.9800 - val_acc: 0.5176\n",
      "Epoch 8/10\n",
      "3042/3042 [==============================] - 124s 41ms/step - loss: 0.9557 - acc: 0.5227 - val_loss: 0.9143 - val_acc: 0.5691\n",
      "Epoch 9/10\n",
      "3042/3042 [==============================] - 124s 41ms/step - loss: 0.9535 - acc: 0.5233 - val_loss: 0.9815 - val_acc: 0.5237\n",
      "Epoch 10/10\n",
      "3042/3042 [==============================] - 124s 41ms/step - loss: 0.9457 - acc: 0.5276 - val_loss: 0.9239 - val_acc: 0.5506\n",
      "Test score: 0.9238097608873719\n",
      "Test accuracy: 0.5508223684210526\n",
      "Stapelgroesse (batchSize): 16\n",
      "Anzahl der Faltungsschichten: 4Layer\n",
      "Aktivierungsfunktion: relu\n",
      "Optimierungsfunktion: Adam\n",
      "Dropout-Rate Faltungsschicht 1: 0.5510035245500235\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 64\n",
      "Dropout-Rate Faltungsschicht 2: 0.1532124532686945\n",
      "Anzahl der Faltungsschichten: 4Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 64\n",
      "Dropout-Rate Faltungsschicht 3: 0.17944072462772032\n",
      "Anzahl der Filter-Maps Faltungsschicht 4: 128\n",
      "Dropout-Rate Faltungsschicht 4: 0.08445402539120138\n",
      "Anzahl der Neuronen des Fully Connected Layer: 128\n",
      "Dropout-Rate Fully Connected Layer: 0.08210828672230966\n",
      "Faltungsnetz wird trainiert...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3042/3042 [==============================] - 115s 38ms/step - loss: 1.0787 - acc: 0.3852 - val_loss: 1.0160 - val_acc: 0.4714\n",
      "Epoch 2/10\n",
      "3042/3042 [==============================] - 115s 38ms/step - loss: 1.0064 - acc: 0.4737 - val_loss: 0.9918 - val_acc: 0.4863\n",
      "Epoch 3/10\n",
      "3042/3042 [==============================] - 114s 38ms/step - loss: 0.9796 - acc: 0.4936 - val_loss: 1.0061 - val_acc: 0.4633\n",
      "Epoch 4/10\n",
      "3042/3042 [==============================] - 114s 37ms/step - loss: 0.9616 - acc: 0.5083 - val_loss: 0.9940 - val_acc: 0.4930\n",
      "Epoch 5/10\n",
      "3042/3042 [==============================] - 113s 37ms/step - loss: 0.9510 - acc: 0.5154 - val_loss: 1.0409 - val_acc: 0.4408\n",
      "Epoch 6/10\n",
      "3042/3042 [==============================] - 113s 37ms/step - loss: 0.9280 - acc: 0.5363 - val_loss: 1.1132 - val_acc: 0.3883\n",
      "Epoch 7/10\n",
      "3042/3042 [==============================] - 114s 37ms/step - loss: 0.9119 - acc: 0.5474 - val_loss: 1.1050 - val_acc: 0.4045\n",
      "Epoch 8/10\n",
      "3042/3042 [==============================] - 114s 37ms/step - loss: 0.8943 - acc: 0.5599 - val_loss: 1.1845 - val_acc: 0.3817\n",
      "Epoch 9/10\n",
      "3042/3042 [==============================] - 114s 37ms/step - loss: 0.8801 - acc: 0.5712 - val_loss: 1.1841 - val_acc: 0.3589\n",
      "Epoch 10/10\n",
      "3042/3042 [==============================] - 113s 37ms/step - loss: 0.8690 - acc: 0.5763 - val_loss: 1.2188 - val_acc: 0.3736\n",
      "Test score: 1.2191633364871928\n",
      "Test accuracy: 0.37351973684210527\n",
      "Stapelgroesse (batchSize): 32\n",
      "Anzahl der Faltungsschichten: 3Layer\n",
      "Aktivierungsfunktion: elu\n",
      "Optimierungsfunktion: Adam\n",
      "Dropout-Rate Faltungsschicht 1: 0.26045732045458037\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 32\n",
      "Dropout-Rate Faltungsschicht 2: 0.5553204526749792\n",
      "Anzahl der Faltungsschichten: 3Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 128\n",
      "Dropout-Rate Faltungsschicht 3: 0.3761602688998059\n",
      "Anzahl der Neuronen des Fully Connected Layer: 128\n",
      "Dropout-Rate Fully Connected Layer: 0.023263294966867562\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "1521/1521 [==============================] - 96s 63ms/step - loss: 1.1056 - acc: 0.4323 - val_loss: 2.3848 - val_acc: 0.4178\n",
      "Epoch 2/10\n",
      "1521/1521 [==============================] - 94s 62ms/step - loss: 1.0081 - acc: 0.4726 - val_loss: 1.9240 - val_acc: 0.3503\n",
      "Epoch 3/10\n",
      "1521/1521 [==============================] - 95s 62ms/step - loss: 0.9892 - acc: 0.4841 - val_loss: 1.7529 - val_acc: 0.4871\n",
      "Epoch 4/10\n",
      "1521/1521 [==============================] - 95s 62ms/step - loss: 0.9783 - acc: 0.4915 - val_loss: 1.7475 - val_acc: 0.3976\n",
      "Epoch 5/10\n",
      "1521/1521 [==============================] - 94s 62ms/step - loss: 0.9961 - acc: 0.4619 - val_loss: 0.9394 - val_acc: 0.5386\n",
      "Epoch 6/10\n",
      "1521/1521 [==============================] - 94s 62ms/step - loss: 0.9046 - acc: 0.5462 - val_loss: 0.9279 - val_acc: 0.5616\n",
      "Epoch 7/10\n",
      "1521/1521 [==============================] - 95s 62ms/step - loss: 0.8805 - acc: 0.5635 - val_loss: 0.8440 - val_acc: 0.5937\n",
      "Epoch 8/10\n",
      "1521/1521 [==============================] - 94s 62ms/step - loss: 0.8744 - acc: 0.5680 - val_loss: 0.8664 - val_acc: 0.5972\n",
      "Epoch 9/10\n",
      "1521/1521 [==============================] - 95s 62ms/step - loss: 0.8553 - acc: 0.5804 - val_loss: 0.8428 - val_acc: 0.6017\n",
      "Epoch 10/10\n",
      "1521/1521 [==============================] - 95s 62ms/step - loss: 0.8477 - acc: 0.5876 - val_loss: 0.8555 - val_acc: 0.5975\n",
      "Test score: 0.8553494884779579\n",
      "Test accuracy: 0.5974506578947368\n",
      "Stapelgroesse (batchSize): 8\n",
      "Anzahl der Faltungsschichten: 5Layer\n",
      "Aktivierungsfunktion: elu\n",
      "Optimierungsfunktion: RMSprop\n",
      "Dropout-Rate Faltungsschicht 1: 0.6861024010168819\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 64\n",
      "Dropout-Rate Faltungsschicht 2: 0.649927584574656\n",
      "Anzahl der Faltungsschichten: 5Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 64\n",
      "Dropout-Rate Faltungsschicht 3: 0.399162292849581\n",
      "Anzahl der Filter-Maps Faltungsschicht 4: 128\n",
      "Dropout-Rate Faltungsschicht 4: 0.4385232152116758\n",
      "Anzahl der Filter-Maps Faltungsschicht 5: 64\n",
      "Dropout-Rate Faltungsschicht 5: 0.13328636172166075\n",
      "Anzahl der Neuronen des Fully Connected Layer: 64\n",
      "Dropout-Rate Fully Connected Layer: 0.05644691958057099\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "6084/6084 [==============================] - 148s 24ms/step - loss: 1.1037 - acc: 0.4086 - val_loss: 1.1131 - val_acc: 0.4117\n",
      "Epoch 2/10\n",
      "6084/6084 [==============================] - 152s 25ms/step - loss: 1.0367 - acc: 0.4574 - val_loss: 0.9948 - val_acc: 0.4933\n",
      "Epoch 3/10\n",
      "6084/6084 [==============================] - 153s 25ms/step - loss: 1.0219 - acc: 0.4752 - val_loss: 0.9996 - val_acc: 0.5344\n",
      "Epoch 4/10\n",
      "6084/6084 [==============================] - 152s 25ms/step - loss: 1.0222 - acc: 0.4762 - val_loss: 1.3516 - val_acc: 0.3851\n",
      "Epoch 5/10\n",
      "6084/6084 [==============================] - 153s 25ms/step - loss: 1.0314 - acc: 0.4709 - val_loss: 1.1399 - val_acc: 0.3868\n",
      "Epoch 6/10\n",
      "6084/6084 [==============================] - 155s 26ms/step - loss: 1.0241 - acc: 0.4815 - val_loss: 1.1868 - val_acc: 0.3906\n",
      "Epoch 7/10\n",
      "6084/6084 [==============================] - 147s 24ms/step - loss: 1.0276 - acc: 0.4776 - val_loss: 4.4679 - val_acc: 0.4458\n",
      "Epoch 8/10\n",
      "6084/6084 [==============================] - 149s 24ms/step - loss: 1.0386 - acc: 0.4746 - val_loss: 2.7179 - val_acc: 0.4393\n",
      "Epoch 9/10\n",
      "6084/6084 [==============================] - 148s 24ms/step - loss: 1.0412 - acc: 0.4756 - val_loss: 3.8056 - val_acc: 0.4218\n",
      "Epoch 10/10\n",
      "6084/6084 [==============================] - 201s 33ms/step - loss: 1.0376 - acc: 0.4755 - val_loss: 7.1466 - val_acc: 0.3428\n",
      "Test score: 7.145932047168022\n",
      "Test accuracy: 0.3428665351742275\n",
      "Stapelgroesse (batchSize): 16\n",
      "Anzahl der Faltungsschichten: 3Layer\n",
      "Aktivierungsfunktion: elu\n",
      "Optimierungsfunktion: Adam\n",
      "Dropout-Rate Faltungsschicht 1: 0.3355825176936589\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 64\n",
      "Dropout-Rate Faltungsschicht 2: 0.2666132242226218\n",
      "Anzahl der Faltungsschichten: 3Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 64\n",
      "Dropout-Rate Faltungsschicht 3: 0.2965316398645662\n",
      "Anzahl der Neuronen des Fully Connected Layer: 128\n",
      "Dropout-Rate Fully Connected Layer: 0.15986691790317023\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "3042/3042 [==============================] - 166s 55ms/step - loss: 1.0726 - acc: 0.4375 - val_loss: 1.0835 - val_acc: 0.3900\n",
      "Epoch 2/10\n",
      "3042/3042 [==============================] - 115s 38ms/step - loss: 0.9759 - acc: 0.4934 - val_loss: 0.9151 - val_acc: 0.5355\n",
      "Epoch 3/10\n",
      "3042/3042 [==============================] - 115s 38ms/step - loss: 0.9158 - acc: 0.5424 - val_loss: 0.8812 - val_acc: 0.5500\n",
      "Epoch 4/10\n",
      "3042/3042 [==============================] - 115s 38ms/step - loss: 0.8866 - acc: 0.5615 - val_loss: 0.8708 - val_acc: 0.5476\n",
      "Epoch 5/10\n",
      "3042/3042 [==============================] - 116s 38ms/step - loss: 0.8665 - acc: 0.5734 - val_loss: 1.2010 - val_acc: 0.4517\n",
      "Epoch 6/10\n",
      "3042/3042 [==============================] - 116s 38ms/step - loss: 0.8489 - acc: 0.5836 - val_loss: 0.8936 - val_acc: 0.5722\n",
      "Epoch 7/10\n",
      "3042/3042 [==============================] - 115s 38ms/step - loss: 0.8303 - acc: 0.5897 - val_loss: 0.9416 - val_acc: 0.5667\n",
      "Epoch 8/10\n",
      "3042/3042 [==============================] - 115s 38ms/step - loss: 0.8153 - acc: 0.5991 - val_loss: 0.9194 - val_acc: 0.5602\n",
      "Epoch 9/10\n",
      "3042/3042 [==============================] - 116s 38ms/step - loss: 0.7988 - acc: 0.6069 - val_loss: 1.1296 - val_acc: 0.5299\n",
      "Epoch 10/10\n",
      "3042/3042 [==============================] - 119s 39ms/step - loss: 0.7823 - acc: 0.6177 - val_loss: 0.9882 - val_acc: 0.5555\n",
      "Test score: 0.988474285837851\n",
      "Test accuracy: 0.5553453947368421\n",
      "Stapelgroesse (batchSize): 32\n",
      "Anzahl der Faltungsschichten: 4Layer\n",
      "Aktivierungsfunktion: relu\n",
      "Optimierungsfunktion: RMSprop\n",
      "Dropout-Rate Faltungsschicht 1: 0.5248675502930554\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 32\n",
      "Dropout-Rate Faltungsschicht 2: 0.6521929535480597\n",
      "Anzahl der Faltungsschichten: 4Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 64\n",
      "Dropout-Rate Faltungsschicht 3: 0.26105331432376633\n",
      "Anzahl der Filter-Maps Faltungsschicht 4: 64\n",
      "Dropout-Rate Faltungsschicht 4: 0.6957910099955584\n",
      "Anzahl der Neuronen des Fully Connected Layer: 128\n",
      "Dropout-Rate Fully Connected Layer: 0.21781501997478092\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1521/1521 [==============================] - 85s 56ms/step - loss: 1.0669 - acc: 0.4209 - val_loss: 1.0403 - val_acc: 0.4282\n",
      "Epoch 2/10\n",
      "1521/1521 [==============================] - 84s 55ms/step - loss: 1.0219 - acc: 0.4654 - val_loss: 1.0378 - val_acc: 0.4424\n",
      "Epoch 3/10\n",
      "1521/1521 [==============================] - 82s 54ms/step - loss: 1.0058 - acc: 0.4817 - val_loss: 1.0554 - val_acc: 0.4093\n",
      "Epoch 4/10\n",
      "1521/1521 [==============================] - 81s 53ms/step - loss: 0.9927 - acc: 0.4914 - val_loss: 1.0571 - val_acc: 0.4268\n",
      "Epoch 5/10\n",
      "1521/1521 [==============================] - 81s 53ms/step - loss: 0.9836 - acc: 0.5029 - val_loss: 1.0247 - val_acc: 0.4509\n",
      "Epoch 6/10\n",
      "1521/1521 [==============================] - 81s 53ms/step - loss: 0.9732 - acc: 0.5121 - val_loss: 1.0696 - val_acc: 0.3978\n",
      "Epoch 7/10\n",
      "1521/1521 [==============================] - 81s 53ms/step - loss: 0.9717 - acc: 0.5151 - val_loss: 1.0801 - val_acc: 0.3997\n",
      "Epoch 8/10\n",
      "1521/1521 [==============================] - 81s 53ms/step - loss: 0.9607 - acc: 0.5216 - val_loss: 1.1391 - val_acc: 0.3509\n",
      "Epoch 9/10\n",
      "1521/1521 [==============================] - 81s 53ms/step - loss: 0.9576 - acc: 0.5229 - val_loss: 1.0624 - val_acc: 0.4076\n",
      "Epoch 10/10\n",
      "1521/1521 [==============================] - 81s 53ms/step - loss: 1.1343 - acc: 0.5213 - val_loss: 1.0462 - val_acc: 0.4253\n",
      "Test score: 1.046307374929127\n",
      "Test accuracy: 0.4252467105263158\n",
      "Stapelgroesse (batchSize): 16\n",
      "Anzahl der Faltungsschichten: 5Layer\n",
      "Aktivierungsfunktion: relu\n",
      "Optimierungsfunktion: RMSprop\n",
      "Dropout-Rate Faltungsschicht 1: 0.2888234898361773\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 32\n",
      "Dropout-Rate Faltungsschicht 2: 0.23421018558252513\n",
      "Anzahl der Faltungsschichten: 5Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 128\n",
      "Dropout-Rate Faltungsschicht 3: 0.6236168941633805\n",
      "Anzahl der Filter-Maps Faltungsschicht 4: 128\n",
      "Dropout-Rate Faltungsschicht 4: 0.45319110742946134\n",
      "Anzahl der Filter-Maps Faltungsschicht 5: 128\n",
      "Dropout-Rate Faltungsschicht 5: 0.5711831948196173\n",
      "Anzahl der Neuronen des Fully Connected Layer: 64\n",
      "Dropout-Rate Fully Connected Layer: 0.5509464626270063\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "3042/3042 [==============================] - 109s 36ms/step - loss: 1.0861 - acc: 0.3775 - val_loss: 1.0567 - val_acc: 0.4133\n",
      "Epoch 2/10\n",
      "3042/3042 [==============================] - 107s 35ms/step - loss: 1.0414 - acc: 0.4534 - val_loss: 1.0377 - val_acc: 0.4620\n",
      "Epoch 3/10\n",
      "3042/3042 [==============================] - 107s 35ms/step - loss: 1.0306 - acc: 0.4731 - val_loss: 1.1059 - val_acc: 0.3921\n",
      "Epoch 4/10\n",
      "3042/3042 [==============================] - 107s 35ms/step - loss: 1.0210 - acc: 0.4900 - val_loss: 1.0836 - val_acc: 0.4339\n",
      "Epoch 5/10\n",
      "3042/3042 [==============================] - 107s 35ms/step - loss: 1.0152 - acc: 0.4942 - val_loss: 1.0947 - val_acc: 0.4219\n",
      "Epoch 6/10\n",
      "3042/3042 [==============================] - 107s 35ms/step - loss: 1.0335 - acc: 0.4954 - val_loss: 1.0540 - val_acc: 0.4485\n",
      "Epoch 7/10\n",
      "3042/3042 [==============================] - 107s 35ms/step - loss: 1.0296 - acc: 0.4960 - val_loss: 1.0560 - val_acc: 0.3617\n",
      "Epoch 8/10\n",
      "3042/3042 [==============================] - 107s 35ms/step - loss: 1.0571 - acc: 0.5004 - val_loss: 1.1364 - val_acc: 0.3774\n",
      "Epoch 9/10\n",
      "3042/3042 [==============================] - 107s 35ms/step - loss: 1.0278 - acc: 0.4973 - val_loss: 1.1721 - val_acc: 0.3455\n",
      "Epoch 10/10\n",
      "3042/3042 [==============================] - 107s 35ms/step - loss: 1.0172 - acc: 0.4964 - val_loss: 1.1942 - val_acc: 0.3645\n",
      "Test score: 1.1940337749688248\n",
      "Test accuracy: 0.36463815789473686\n",
      "Stapelgroesse (batchSize): 32\n",
      "Anzahl der Faltungsschichten: 3Layer\n",
      "Aktivierungsfunktion: elu\n",
      "Optimierungsfunktion: Adam\n",
      "Dropout-Rate Faltungsschicht 1: 0.1411873881737138\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 32\n",
      "Dropout-Rate Faltungsschicht 2: 0.43940119562733904\n",
      "Anzahl der Faltungsschichten: 3Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 64\n",
      "Dropout-Rate Faltungsschicht 3: 0.6827140018946355\n",
      "Anzahl der Neuronen des Fully Connected Layer: 128\n",
      "Dropout-Rate Fully Connected Layer: 0.5212403354707789\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "1521/1521 [==============================] - 86s 57ms/step - loss: 1.0631 - acc: 0.4466 - val_loss: 1.0066 - val_acc: 0.4583: 5s - loss: 1\n",
      "Epoch 2/10\n",
      "1521/1521 [==============================] - 405s 266ms/step - loss: 0.9914 - acc: 0.4871 - val_loss: 0.9325 - val_acc: 0.5371\n",
      "Epoch 3/10\n",
      "1521/1521 [==============================] - 141s 93ms/step - loss: 0.9486 - acc: 0.5180 - val_loss: 0.8693 - val_acc: 0.5791\n",
      "Epoch 4/10\n",
      "1521/1521 [==============================] - 84s 55ms/step - loss: 0.9146 - acc: 0.5455 - val_loss: 0.8649 - val_acc: 0.5821\n",
      "Epoch 5/10\n",
      "1521/1521 [==============================] - 84s 55ms/step - loss: 0.8935 - acc: 0.5576 - val_loss: 0.8608 - val_acc: 0.5867\n",
      "Epoch 6/10\n",
      "1521/1521 [==============================] - 84s 55ms/step - loss: 0.8842 - acc: 0.5644 - val_loss: 0.8476 - val_acc: 0.5914\n",
      "Epoch 7/10\n",
      "1521/1521 [==============================] - 84s 55ms/step - loss: 0.8766 - acc: 0.5695 - val_loss: 0.8311 - val_acc: 0.6072\n",
      "Epoch 8/10\n",
      "1521/1521 [==============================] - 84s 55ms/step - loss: 0.8749 - acc: 0.5694 - val_loss: 0.8381 - val_acc: 0.6030\n",
      "Epoch 9/10\n",
      "1521/1521 [==============================] - 84s 55ms/step - loss: 0.8713 - acc: 0.5737 - val_loss: 0.8228 - val_acc: 0.6064\n",
      "Epoch 10/10\n",
      "1521/1521 [==============================] - 84s 55ms/step - loss: 0.8710 - acc: 0.5755 - val_loss: 0.8329 - val_acc: 0.5973\n",
      "Test score: 0.8331705294157329\n",
      "Test accuracy: 0.5971217105263158\n",
      "Stapelgroesse (batchSize): 32\n",
      "Anzahl der Faltungsschichten: 4Layer\n",
      "Aktivierungsfunktion: elu\n",
      "Optimierungsfunktion: Adam\n",
      "Dropout-Rate Faltungsschicht 1: 0.5618022521290307\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 32\n",
      "Dropout-Rate Faltungsschicht 2: 0.43180700701104185\n",
      "Anzahl der Faltungsschichten: 4Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 64\n",
      "Dropout-Rate Faltungsschicht 3: 0.6300135909702449\n",
      "Anzahl der Filter-Maps Faltungsschicht 4: 64\n",
      "Dropout-Rate Faltungsschicht 4: 0.3705274468787073\n",
      "Anzahl der Neuronen des Fully Connected Layer: 128\n",
      "Dropout-Rate Fully Connected Layer: 0.0007036970434920242\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "1521/1521 [==============================] - 85s 56ms/step - loss: 1.0484 - acc: 0.4412 - val_loss: 1.0298 - val_acc: 0.4631\n",
      "Epoch 2/10\n",
      "1521/1521 [==============================] - 84s 55ms/step - loss: 1.0028 - acc: 0.4745 - val_loss: 1.0190 - val_acc: 0.4735\n",
      "Epoch 3/10\n",
      "1521/1521 [==============================] - 84s 55ms/step - loss: 0.9650 - acc: 0.5061 - val_loss: 0.9512 - val_acc: 0.5228\n",
      "Epoch 4/10\n",
      "1521/1521 [==============================] - 84s 55ms/step - loss: 0.9361 - acc: 0.5317 - val_loss: 0.9677 - val_acc: 0.5599\n",
      "Epoch 5/10\n",
      "1521/1521 [==============================] - 83s 55ms/step - loss: 0.9164 - acc: 0.5429 - val_loss: 0.9301 - val_acc: 0.5348\n",
      "Epoch 6/10\n",
      "1521/1521 [==============================] - 84s 55ms/step - loss: 0.8996 - acc: 0.5548 - val_loss: 1.1168 - val_acc: 0.4887\n",
      "Epoch 7/10\n",
      "1521/1521 [==============================] - 84s 55ms/step - loss: 0.8858 - acc: 0.5636 - val_loss: 0.8574 - val_acc: 0.5895\n",
      "Epoch 8/10\n",
      "1521/1521 [==============================] - 84s 55ms/step - loss: 0.8748 - acc: 0.5706 - val_loss: 0.8966 - val_acc: 0.5831\n",
      "Epoch 9/10\n",
      "1521/1521 [==============================] - 83s 55ms/step - loss: 0.8666 - acc: 0.5759 - val_loss: 0.8579 - val_acc: 0.5912\n",
      "Epoch 10/10\n",
      "1521/1521 [==============================] - 84s 55ms/step - loss: 0.8606 - acc: 0.5795 - val_loss: 0.9831 - val_acc: 0.5683\n",
      "Test score: 0.9832243426849968\n",
      "Test accuracy: 0.5679276315789473\n",
      "Stapelgroesse (batchSize): 8\n",
      "Anzahl der Faltungsschichten: 3Layer\n",
      "Aktivierungsfunktion: elu\n",
      "Optimierungsfunktion: Adam\n",
      "Dropout-Rate Faltungsschicht 1: 0.22696402957324682\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 64\n",
      "Dropout-Rate Faltungsschicht 2: 0.15295936298124646\n",
      "Anzahl der Faltungsschichten: 3Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 128\n",
      "Dropout-Rate Faltungsschicht 3: 0.04614877235030587\n",
      "Anzahl der Neuronen des Fully Connected Layer: 64\n",
      "Dropout-Rate Fully Connected Layer: 0.18949638026428298\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "6084/6084 [==============================] - 155s 26ms/step - loss: 10.7428 - acc: 0.3332 - val_loss: 10.7322 - val_acc: 0.3342\n",
      "Epoch 2/10\n",
      "6084/6084 [==============================] - 154s 25ms/step - loss: 10.7490 - acc: 0.3331 - val_loss: 10.7335 - val_acc: 0.3341\n",
      "Epoch 3/10\n",
      "6084/6084 [==============================] - 154s 25ms/step - loss: 10.7494 - acc: 0.3331 - val_loss: 10.7361 - val_acc: 0.3339\n",
      "Epoch 4/10\n",
      "6084/6084 [==============================] - 154s 25ms/step - loss: 10.7497 - acc: 0.3331 - val_loss: 10.7361 - val_acc: 0.3339\n",
      "Epoch 5/10\n",
      "6084/6084 [==============================] - 154s 25ms/step - loss: 10.7500 - acc: 0.3330 - val_loss: 10.7348 - val_acc: 0.3340\n",
      "Epoch 6/10\n",
      "6084/6084 [==============================] - 154s 25ms/step - loss: 10.7494 - acc: 0.3331 - val_loss: 10.7348 - val_acc: 0.3340\n",
      "Epoch 7/10\n",
      "6084/6084 [==============================] - 154s 25ms/step - loss: 10.7494 - acc: 0.3331 - val_loss: 10.7308 - val_acc: 0.3342\n",
      "Epoch 8/10\n",
      "6084/6084 [==============================] - 154s 25ms/step - loss: 10.7497 - acc: 0.3331 - val_loss: 10.7348 - val_acc: 0.3340\n",
      "Epoch 9/10\n",
      "6084/6084 [==============================] - 154s 25ms/step - loss: 10.7497 - acc: 0.3331 - val_loss: 10.7321 - val_acc: 0.3342\n",
      "Epoch 10/10\n",
      "6084/6084 [==============================] - 154s 25ms/step - loss: 10.7497 - acc: 0.3331 - val_loss: 10.7361 - val_acc: 0.3339\n",
      "Test score: 10.732150633973092\n",
      "Test accuracy: 0.334155161078238\n",
      "Stapelgroesse (batchSize): 16\n",
      "Anzahl der Faltungsschichten: 4Layer\n",
      "Aktivierungsfunktion: elu\n",
      "Optimierungsfunktion: Adam\n",
      "Dropout-Rate Faltungsschicht 1: 0.502566005987901\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 64\n",
      "Dropout-Rate Faltungsschicht 2: 0.1998550853505832\n",
      "Anzahl der Faltungsschichten: 4Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 64\n",
      "Dropout-Rate Faltungsschicht 3: 0.3759860620037276\n",
      "Anzahl der Filter-Maps Faltungsschicht 4: 128\n",
      "Dropout-Rate Faltungsschicht 4: 0.5196250585612635\n",
      "Anzahl der Neuronen des Fully Connected Layer: 128\n",
      "Dropout-Rate Fully Connected Layer: 0.008207557723527914\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "3042/3042 [==============================] - 117s 38ms/step - loss: 1.1192 - acc: 0.3521 - val_loss: 1.1120 - val_acc: 0.3323\n",
      "Epoch 2/10\n",
      "3042/3042 [==============================] - 116s 38ms/step - loss: 1.1132 - acc: 0.3300 - val_loss: 1.1105 - val_acc: 0.3325\n",
      "Epoch 3/10\n",
      "3042/3042 [==============================] - 116s 38ms/step - loss: 1.1133 - acc: 0.3291 - val_loss: 1.1119 - val_acc: 0.3325\n",
      "Epoch 4/10\n",
      "3042/3042 [==============================] - 115s 38ms/step - loss: 1.1127 - acc: 0.3284 - val_loss: 1.1081 - val_acc: 0.3325\n",
      "Epoch 5/10\n",
      "3042/3042 [==============================] - 115s 38ms/step - loss: 1.1120 - acc: 0.3296 - val_loss: 1.1049 - val_acc: 0.3323\n",
      "Epoch 6/10\n",
      "3042/3042 [==============================] - 116s 38ms/step - loss: 1.1117 - acc: 0.3304 - val_loss: 1.1019 - val_acc: 0.3323\n",
      "Epoch 7/10\n",
      "3042/3042 [==============================] - 116s 38ms/step - loss: 1.1108 - acc: 0.3323 - val_loss: 1.1007 - val_acc: 0.3323\n",
      "Epoch 8/10\n",
      "3042/3042 [==============================] - 116s 38ms/step - loss: 1.1114 - acc: 0.3331 - val_loss: 1.0998 - val_acc: 0.3323\n",
      "Epoch 9/10\n",
      "3042/3042 [==============================] - 116s 38ms/step - loss: 1.1111 - acc: 0.3290 - val_loss: 1.0987 - val_acc: 0.3325\n",
      "Epoch 10/10\n",
      "3042/3042 [==============================] - 115s 38ms/step - loss: 1.1108 - acc: 0.3318 - val_loss: 1.0989 - val_acc: 0.3337\n",
      "Test score: 1.0989086941668862\n",
      "Test accuracy: 0.3335526315789474\n",
      "Stapelgroesse (batchSize): 16\n",
      "Anzahl der Faltungsschichten: 4Layer\n",
      "Aktivierungsfunktion: elu\n",
      "Optimierungsfunktion: RMSprop\n",
      "Dropout-Rate Faltungsschicht 1: 0.20482279996588446\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 32\n",
      "Dropout-Rate Faltungsschicht 2: 0.5455088012568097\n",
      "Anzahl der Faltungsschichten: 4Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 64\n",
      "Dropout-Rate Faltungsschicht 3: 0.5763085244479565\n",
      "Anzahl der Filter-Maps Faltungsschicht 4: 128\n",
      "Dropout-Rate Faltungsschicht 4: 0.5310129254705536\n",
      "Anzahl der Neuronen des Fully Connected Layer: 64\n",
      "Dropout-Rate Fully Connected Layer: 0.004221254827684406\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "3042/3042 [==============================] - 102s 33ms/step - loss: 1.2013 - acc: 0.4181 - val_loss: 1.0200 - val_acc: 0.4454\n",
      "Epoch 2/10\n",
      "3042/3042 [==============================] - 100s 33ms/step - loss: 1.0288 - acc: 0.4651 - val_loss: 0.9104 - val_acc: 0.5439\n",
      "Epoch 3/10\n",
      "3042/3042 [==============================] - 101s 33ms/step - loss: 0.9536 - acc: 0.5269 - val_loss: 0.9624 - val_acc: 0.4843\n",
      "Epoch 4/10\n",
      "3042/3042 [==============================] - 100s 33ms/step - loss: 0.9220 - acc: 0.5434 - val_loss: 1.1371 - val_acc: 0.4627\n",
      "Epoch 5/10\n",
      "3042/3042 [==============================] - 100s 33ms/step - loss: 0.9079 - acc: 0.5553 - val_loss: 0.9897 - val_acc: 0.4613\n",
      "Epoch 6/10\n",
      "3042/3042 [==============================] - 101s 33ms/step - loss: 0.9044 - acc: 0.5554 - val_loss: 0.8598 - val_acc: 0.5930\n",
      "Epoch 7/10\n",
      "3042/3042 [==============================] - 101s 33ms/step - loss: 0.9007 - acc: 0.5577 - val_loss: 0.9805 - val_acc: 0.5606\n",
      "Epoch 8/10\n",
      "3042/3042 [==============================] - 101s 33ms/step - loss: 0.9028 - acc: 0.5574 - val_loss: 0.9284 - val_acc: 0.5383\n",
      "Epoch 9/10\n",
      "3042/3042 [==============================] - 101s 33ms/step - loss: 0.9044 - acc: 0.5554 - val_loss: 0.9108 - val_acc: 0.5561\n",
      "Epoch 10/10\n",
      "3042/3042 [==============================] - 101s 33ms/step - loss: 0.9034 - acc: 0.5578 - val_loss: 0.8931 - val_acc: 0.5538\n",
      "Test score: 0.8932697055371184\n",
      "Test accuracy: 0.5537006578947369\n",
      "Stapelgroesse (batchSize): 8\n",
      "Anzahl der Faltungsschichten: 3Layer\n",
      "Aktivierungsfunktion: relu\n",
      "Optimierungsfunktion: Adam\n",
      "Dropout-Rate Faltungsschicht 1: 0.6859801490268133\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 32\n",
      "Dropout-Rate Faltungsschicht 2: 0.3567663629232832\n",
      "Anzahl der Faltungsschichten: 3Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 128\n",
      "Dropout-Rate Faltungsschicht 3: 0.6398422280118664\n",
      "Anzahl der Neuronen des Fully Connected Layer: 128\n",
      "Dropout-Rate Fully Connected Layer: 0.3468418591094041\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "6084/6084 [==============================] - 161s 26ms/step - loss: 1.1058 - acc: 0.3347 - val_loss: 1.0988 - val_acc: 0.3323\n",
      "Epoch 2/10\n",
      "6084/6084 [==============================] - 159s 26ms/step - loss: 1.0988 - acc: 0.3330 - val_loss: 1.0988 - val_acc: 0.3325\n",
      "Epoch 3/10\n",
      "6084/6084 [==============================] - 160s 26ms/step - loss: 1.0988 - acc: 0.3330 - val_loss: 1.0988 - val_acc: 0.3323\n",
      "Epoch 4/10\n",
      "6084/6084 [==============================] - 159s 26ms/step - loss: 1.0989 - acc: 0.3319 - val_loss: 1.0989 - val_acc: 0.3323\n",
      "Epoch 5/10\n",
      "6084/6084 [==============================] - 159s 26ms/step - loss: 1.0988 - acc: 0.3321 - val_loss: 1.0988 - val_acc: 0.3323\n",
      "Epoch 6/10\n",
      "6084/6084 [==============================] - 159s 26ms/step - loss: 1.0988 - acc: 0.3321 - val_loss: 1.0988 - val_acc: 0.3322\n",
      "Epoch 7/10\n",
      "6084/6084 [==============================] - 159s 26ms/step - loss: 1.0988 - acc: 0.3321 - val_loss: 1.0988 - val_acc: 0.3324\n",
      "Epoch 8/10\n",
      "6084/6084 [==============================] - 160s 26ms/step - loss: 1.0988 - acc: 0.3321 - val_loss: 1.0988 - val_acc: 0.3324\n",
      "Epoch 9/10\n",
      "6084/6084 [==============================] - 159s 26ms/step - loss: 1.0988 - acc: 0.3321 - val_loss: 1.0988 - val_acc: 0.3323\n",
      "Epoch 10/10\n",
      "6084/6084 [==============================] - 159s 26ms/step - loss: 1.0988 - acc: 0.3321 - val_loss: 1.0988 - val_acc: 0.3324\n",
      "Test score: 1.0988241953727527\n",
      "Test accuracy: 0.33226495726495725\n",
      "Stapelgroesse (batchSize): 16\n",
      "Anzahl der Faltungsschichten: 4Layer\n",
      "Aktivierungsfunktion: relu\n",
      "Optimierungsfunktion: Adam\n",
      "Dropout-Rate Faltungsschicht 1: 0.5307933999683194\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 32\n",
      "Dropout-Rate Faltungsschicht 2: 0.1449955280636636\n",
      "Anzahl der Faltungsschichten: 4Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 128\n",
      "Dropout-Rate Faltungsschicht 3: 0.04730330293000361\n",
      "Anzahl der Filter-Maps Faltungsschicht 4: 64\n",
      "Dropout-Rate Faltungsschicht 4: 0.37732292891116026\n",
      "Anzahl der Neuronen des Fully Connected Layer: 64\n",
      "Dropout-Rate Fully Connected Layer: 0.10028084027027029\n",
      "Faltungsnetz wird trainiert...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3042/3042 [==============================] - 104s 34ms/step - loss: 1.0990 - acc: 0.3322 - val_loss: 1.0988 - val_acc: 0.3323\n",
      "Epoch 2/10\n",
      "3042/3042 [==============================] - 102s 34ms/step - loss: 1.0987 - acc: 0.3334 - val_loss: 1.0988 - val_acc: 0.3325\n",
      "Epoch 3/10\n",
      "3042/3042 [==============================] - 102s 34ms/step - loss: 1.0987 - acc: 0.3339 - val_loss: 1.0988 - val_acc: 0.3323\n",
      "Epoch 4/10\n",
      "3042/3042 [==============================] - 102s 34ms/step - loss: 1.0987 - acc: 0.3337 - val_loss: 1.0988 - val_acc: 0.3323\n",
      "Epoch 5/10\n",
      "3042/3042 [==============================] - 102s 34ms/step - loss: 1.0987 - acc: 0.3337 - val_loss: 1.0988 - val_acc: 0.3323\n",
      "Epoch 6/10\n",
      "3042/3042 [==============================] - 102s 34ms/step - loss: 1.0987 - acc: 0.3337 - val_loss: 1.0988 - val_acc: 0.3323\n",
      "Epoch 7/10\n",
      "3042/3042 [==============================] - 103s 34ms/step - loss: 1.0987 - acc: 0.3337 - val_loss: 1.0988 - val_acc: 0.3323\n",
      "Epoch 8/10\n",
      "3042/3042 [==============================] - 102s 34ms/step - loss: 1.0987 - acc: 0.3338 - val_loss: 1.0988 - val_acc: 0.3323\n",
      "Epoch 9/10\n",
      "3042/3042 [==============================] - 102s 34ms/step - loss: 1.0987 - acc: 0.3337 - val_loss: 1.0988 - val_acc: 0.3323\n",
      "Epoch 10/10\n",
      "3042/3042 [==============================] - 103s 34ms/step - loss: 1.0987 - acc: 0.3338 - val_loss: 1.0988 - val_acc: 0.3323\n",
      "Test score: 1.0987519544990438\n",
      "Test accuracy: 0.3323190789473684\n",
      "Stapelgroesse (batchSize): 16\n",
      "Anzahl der Faltungsschichten: 4Layer\n",
      "Aktivierungsfunktion: elu\n",
      "Optimierungsfunktion: Adam\n",
      "Dropout-Rate Faltungsschicht 1: 0.4831880841923193\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 32\n",
      "Dropout-Rate Faltungsschicht 2: 0.4218928627980383\n",
      "Anzahl der Faltungsschichten: 4Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 64\n",
      "Dropout-Rate Faltungsschicht 3: 0.4360259891153708\n",
      "Anzahl der Filter-Maps Faltungsschicht 4: 128\n",
      "Dropout-Rate Faltungsschicht 4: 0.4074207844976787\n",
      "Anzahl der Neuronen des Fully Connected Layer: 128\n",
      "Dropout-Rate Fully Connected Layer: 0.5411342377700985\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "3042/3042 [==============================] - 105s 34ms/step - loss: 1.0582 - acc: 0.4389 - val_loss: 1.0110 - val_acc: 0.4437\n",
      "Epoch 2/10\n",
      "3042/3042 [==============================] - 104s 34ms/step - loss: 1.0256 - acc: 0.4574 - val_loss: 1.0219 - val_acc: 0.4442\n",
      "Epoch 3/10\n",
      "3042/3042 [==============================] - 104s 34ms/step - loss: 1.0143 - acc: 0.4719 - val_loss: 0.9639 - val_acc: 0.5209\n",
      "Epoch 4/10\n",
      "3042/3042 [==============================] - 103s 34ms/step - loss: 0.9919 - acc: 0.4864 - val_loss: 0.9873 - val_acc: 0.4055\n",
      "Epoch 5/10\n",
      "3042/3042 [==============================] - 104s 34ms/step - loss: 0.9850 - acc: 0.4905 - val_loss: 0.9750 - val_acc: 0.4695\n",
      "Epoch 6/10\n",
      "3042/3042 [==============================] - 104s 34ms/step - loss: 0.9814 - acc: 0.4938 - val_loss: 0.8979 - val_acc: 0.5662\n",
      "Epoch 7/10\n",
      "3042/3042 [==============================] - 104s 34ms/step - loss: 0.9811 - acc: 0.4921 - val_loss: 0.9779 - val_acc: 0.5186\n",
      "Epoch 8/10\n",
      "3042/3042 [==============================] - 104s 34ms/step - loss: 0.9856 - acc: 0.4845 - val_loss: 1.0374 - val_acc: 0.4282\n",
      "Epoch 9/10\n",
      "3042/3042 [==============================] - 104s 34ms/step - loss: 0.9919 - acc: 0.4798 - val_loss: 0.9703 - val_acc: 0.4708\n",
      "Epoch 10/10\n",
      "3042/3042 [==============================] - 104s 34ms/step - loss: 0.9970 - acc: 0.4751 - val_loss: 2.5703 - val_acc: 0.3865\n",
      "Test score: 2.5708897152229357\n",
      "Test accuracy: 0.38626644736842103\n",
      "Stapelgroesse (batchSize): 8\n",
      "Anzahl der Faltungsschichten: 3Layer\n",
      "Aktivierungsfunktion: relu\n",
      "Optimierungsfunktion: Adam\n",
      "Dropout-Rate Faltungsschicht 1: 0.004402839931825553\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 64\n",
      "Dropout-Rate Faltungsschicht 2: 0.0016145402093660887\n",
      "Anzahl der Faltungsschichten: 3Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 64\n",
      "Dropout-Rate Faltungsschicht 3: 0.0908218196803085\n",
      "Anzahl der Neuronen des Fully Connected Layer: 128\n",
      "Dropout-Rate Fully Connected Layer: 0.2833657173581164\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "6084/6084 [==============================] - 148s 24ms/step - loss: 1.0991 - acc: 0.3342 - val_loss: 1.0988 - val_acc: 0.3323\n",
      "Epoch 2/10\n",
      "6084/6084 [==============================] - 147s 24ms/step - loss: 1.0988 - acc: 0.3321 - val_loss: 1.0988 - val_acc: 0.3323\n",
      "Epoch 3/10\n",
      "6084/6084 [==============================] - 147s 24ms/step - loss: 1.0988 - acc: 0.3321 - val_loss: 1.0988 - val_acc: 0.3323\n",
      "Epoch 4/10\n",
      "6084/6084 [==============================] - 146s 24ms/step - loss: 1.0988 - acc: 0.3321 - val_loss: 1.0988 - val_acc: 0.3322\n",
      "Epoch 5/10\n",
      "6084/6084 [==============================] - 146s 24ms/step - loss: 1.0988 - acc: 0.3321 - val_loss: 1.0988 - val_acc: 0.3322\n",
      "Epoch 6/10\n",
      "6084/6084 [==============================] - 147s 24ms/step - loss: 1.0988 - acc: 0.3321 - val_loss: 1.0988 - val_acc: 0.3322\n",
      "Epoch 7/10\n",
      "6084/6084 [==============================] - 147s 24ms/step - loss: 1.0988 - acc: 0.3321 - val_loss: 1.0988 - val_acc: 0.3322\n",
      "Epoch 8/10\n",
      "6084/6084 [==============================] - 147s 24ms/step - loss: 1.0988 - acc: 0.3321 - val_loss: 1.0988 - val_acc: 0.3322\n",
      "Epoch 9/10\n",
      "6084/6084 [==============================] - 146s 24ms/step - loss: 1.0988 - acc: 0.3321 - val_loss: 1.0988 - val_acc: 0.3324\n",
      "Epoch 10/10\n",
      "6084/6084 [==============================] - 147s 24ms/step - loss: 1.0988 - acc: 0.3321 - val_loss: 1.0988 - val_acc: 0.3324\n",
      "Test score: 1.0988241953727527\n",
      "Test accuracy: 0.33226495726495725\n",
      "Stapelgroesse (batchSize): 8\n",
      "Anzahl der Faltungsschichten: 3Layer\n",
      "Aktivierungsfunktion: elu\n",
      "Optimierungsfunktion: RMSprop\n",
      "Dropout-Rate Faltungsschicht 1: 0.16247928184812713\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 64\n",
      "Dropout-Rate Faltungsschicht 2: 0.5862336116278009\n",
      "Anzahl der Faltungsschichten: 3Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 128\n",
      "Dropout-Rate Faltungsschicht 3: 0.3396749535127678\n",
      "Anzahl der Neuronen des Fully Connected Layer: 64\n",
      "Dropout-Rate Fully Connected Layer: 0.271304795333835\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "6084/6084 [==============================] - 153s 25ms/step - loss: 10.7386 - acc: 0.3336 - val_loss: 10.7414 - val_acc: 0.3336\n",
      "Epoch 2/10\n",
      "6084/6084 [==============================] - 152s 25ms/step - loss: 10.7501 - acc: 0.3330 - val_loss: 10.7414 - val_acc: 0.3336\n",
      "Epoch 3/10\n",
      "6084/6084 [==============================] - 152s 25ms/step - loss: 10.7508 - acc: 0.3330 - val_loss: 10.7414 - val_acc: 0.3336\n",
      "Epoch 4/10\n",
      "6084/6084 [==============================] - 152s 25ms/step - loss: 10.7462 - acc: 0.3331 - val_loss: 10.7414 - val_acc: 0.3336\n",
      "Epoch 5/10\n",
      "6084/6084 [==============================] - 152s 25ms/step - loss: 10.7099 - acc: 0.3354 - val_loss: 10.7321 - val_acc: 0.3342\n",
      "Epoch 6/10\n",
      "6084/6084 [==============================] - 152s 25ms/step - loss: 10.7494 - acc: 0.3331 - val_loss: 10.7361 - val_acc: 0.3339\n",
      "Epoch 7/10\n",
      "6084/6084 [==============================] - 152s 25ms/step - loss: 10.7313 - acc: 0.3342 - val_loss: 10.7613 - val_acc: 0.3323\n",
      "Epoch 8/10\n",
      "6084/6084 [==============================] - 152s 25ms/step - loss: 10.7367 - acc: 0.3339 - val_loss: 10.7600 - val_acc: 0.3324\n",
      "Epoch 9/10\n",
      "6084/6084 [==============================] - 152s 25ms/step - loss: 10.7361 - acc: 0.3339 - val_loss: 10.7600 - val_acc: 0.3324\n",
      "Epoch 10/10\n",
      "6084/6084 [==============================] - 152s 25ms/step - loss: 10.7570 - acc: 0.3325 - val_loss: 10.7348 - val_acc: 0.3340\n",
      "Test score: 10.732150633973092\n",
      "Test accuracy: 0.334155161078238\n",
      "Stapelgroesse (batchSize): 32\n",
      "Anzahl der Faltungsschichten: 5Layer\n",
      "Aktivierungsfunktion: elu\n",
      "Optimierungsfunktion: Adam\n",
      "Dropout-Rate Faltungsschicht 1: 0.4610984734470713\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 64\n",
      "Dropout-Rate Faltungsschicht 2: 0.16081779868236015\n",
      "Anzahl der Faltungsschichten: 5Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 64\n",
      "Dropout-Rate Faltungsschicht 3: 0.08805334360109915\n",
      "Anzahl der Filter-Maps Faltungsschicht 4: 128\n",
      "Dropout-Rate Faltungsschicht 4: 0.6708566424053988\n",
      "Anzahl der Filter-Maps Faltungsschicht 5: 64\n",
      "Dropout-Rate Faltungsschicht 5: 0.6710317480025905\n",
      "Anzahl der Neuronen des Fully Connected Layer: 64\n",
      "Dropout-Rate Fully Connected Layer: 0.5758588201404047\n",
      "Faltungsnetz wird trainiert...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1521/1521 [==============================] - 101s 67ms/step - loss: 1.0682 - acc: 0.4294 - val_loss: 1.0246 - val_acc: 0.4584\n",
      "Epoch 2/10\n",
      "1521/1521 [==============================] - 100s 65ms/step - loss: 0.9934 - acc: 0.4879 - val_loss: 0.9054 - val_acc: 0.5486\n",
      "Epoch 3/10\n",
      "1521/1521 [==============================] - 99s 65ms/step - loss: 0.9388 - acc: 0.5286 - val_loss: 0.8705 - val_acc: 0.5648\n",
      "Epoch 4/10\n",
      "1521/1521 [==============================] - 99s 65ms/step - loss: 0.9218 - acc: 0.5430 - val_loss: 0.8608 - val_acc: 0.5792\n",
      "Epoch 5/10\n",
      "1521/1521 [==============================] - 99s 65ms/step - loss: 0.9191 - acc: 0.5406 - val_loss: 0.8794 - val_acc: 0.5704\n",
      "Epoch 6/10\n",
      "1521/1521 [==============================] - 100s 65ms/step - loss: 0.9204 - acc: 0.5420 - val_loss: 0.8384 - val_acc: 0.5832\n",
      "Epoch 7/10\n",
      "1521/1521 [==============================] - 100s 65ms/step - loss: 0.9243 - acc: 0.5415 - val_loss: 0.8425 - val_acc: 0.5799\n",
      "Epoch 8/10\n",
      "1521/1521 [==============================] - 100s 65ms/step - loss: 0.9252 - acc: 0.5410 - val_loss: 0.8397 - val_acc: 0.5955\n",
      "Epoch 9/10\n",
      "1521/1521 [==============================] - 100s 65ms/step - loss: 0.9270 - acc: 0.5388 - val_loss: 0.8675 - val_acc: 0.5935\n",
      "Epoch 10/10\n",
      "1521/1521 [==============================] - 100s 65ms/step - loss: 0.9280 - acc: 0.5405 - val_loss: 0.8334 - val_acc: 0.6059\n",
      "Test score: 0.8335627453891854\n",
      "Test accuracy: 0.6059210526315789\n",
      "Stapelgroesse (batchSize): 32\n",
      "Anzahl der Faltungsschichten: 4Layer\n",
      "Aktivierungsfunktion: elu\n",
      "Optimierungsfunktion: Adam\n",
      "Dropout-Rate Faltungsschicht 1: 0.6427027259508195\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 64\n",
      "Dropout-Rate Faltungsschicht 2: 0.4169443998086015\n",
      "Anzahl der Faltungsschichten: 4Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 64\n",
      "Dropout-Rate Faltungsschicht 3: 0.18710814475088694\n",
      "Anzahl der Filter-Maps Faltungsschicht 4: 64\n",
      "Dropout-Rate Faltungsschicht 4: 0.13477514402321533\n",
      "Anzahl der Neuronen des Fully Connected Layer: 128\n",
      "Dropout-Rate Fully Connected Layer: 0.3423376321763028\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "1521/1521 [==============================] - 98s 64ms/step - loss: 1.0483 - acc: 0.4486 - val_loss: 0.9845 - val_acc: 0.4867\n",
      "Epoch 2/10\n",
      "1521/1521 [==============================] - 96s 63ms/step - loss: 0.9907 - acc: 0.4888 - val_loss: 0.9219 - val_acc: 0.5291\n",
      "Epoch 3/10\n",
      "1521/1521 [==============================] - 96s 63ms/step - loss: 0.9425 - acc: 0.5291 - val_loss: 0.9552 - val_acc: 0.5060\n",
      "Epoch 4/10\n",
      "1521/1521 [==============================] - 96s 63ms/step - loss: 0.9164 - acc: 0.5468 - val_loss: 0.9187 - val_acc: 0.5344\n",
      "Epoch 5/10\n",
      "1521/1521 [==============================] - 96s 63ms/step - loss: 0.8953 - acc: 0.5596 - val_loss: 0.9177 - val_acc: 0.5339\n",
      "Epoch 6/10\n",
      "1521/1521 [==============================] - 96s 63ms/step - loss: 0.8884 - acc: 0.5653 - val_loss: 0.9549 - val_acc: 0.5520\n",
      "Epoch 7/10\n",
      "1521/1521 [==============================] - 96s 63ms/step - loss: 0.8777 - acc: 0.5723 - val_loss: 1.0137 - val_acc: 0.5543\n",
      "Epoch 8/10\n",
      "1521/1521 [==============================] - 96s 63ms/step - loss: 0.8701 - acc: 0.5750 - val_loss: 0.9515 - val_acc: 0.5781\n",
      "Epoch 9/10\n",
      "1521/1521 [==============================] - 96s 63ms/step - loss: 0.8652 - acc: 0.5743 - val_loss: 1.0182 - val_acc: 0.5359\n",
      "Epoch 10/10\n",
      "1521/1521 [==============================] - 96s 63ms/step - loss: 0.8591 - acc: 0.5785 - val_loss: 0.9136 - val_acc: 0.5857\n",
      "Test score: 0.9139016013396414\n",
      "Test accuracy: 0.5855263157894737\n",
      "Stapelgroesse (batchSize): 32\n",
      "Anzahl der Faltungsschichten: 5Layer\n",
      "Aktivierungsfunktion: elu\n",
      "Optimierungsfunktion: Adam\n",
      "Dropout-Rate Faltungsschicht 1: 0.15659987172287998\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 32\n",
      "Dropout-Rate Faltungsschicht 2: 0.2129190258346184\n",
      "Anzahl der Faltungsschichten: 5Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 64\n",
      "Dropout-Rate Faltungsschicht 3: 0.442275237188643\n",
      "Anzahl der Filter-Maps Faltungsschicht 4: 64\n",
      "Dropout-Rate Faltungsschicht 4: 0.13420926237033706\n",
      "Anzahl der Filter-Maps Faltungsschicht 5: 64\n",
      "Dropout-Rate Faltungsschicht 5: 0.40227014875377326\n",
      "Anzahl der Neuronen des Fully Connected Layer: 64\n",
      "Dropout-Rate Fully Connected Layer: 0.07500448226106743\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "1521/1521 [==============================] - 87s 57ms/step - loss: 1.0318 - acc: 0.4482 - val_loss: 0.9615 - val_acc: 0.5008\n",
      "Epoch 2/10\n",
      "1521/1521 [==============================] - 86s 56ms/step - loss: 0.9443 - acc: 0.5232 - val_loss: 0.8557 - val_acc: 0.5830\n",
      "Epoch 3/10\n",
      "1521/1521 [==============================] - 86s 56ms/step - loss: 0.8860 - acc: 0.5623 - val_loss: 0.8573 - val_acc: 0.5744\n",
      "Epoch 4/10\n",
      "1521/1521 [==============================] - 86s 56ms/step - loss: 0.8579 - acc: 0.5833 - val_loss: 0.8502 - val_acc: 0.5737\n",
      "Epoch 5/10\n",
      "1521/1521 [==============================] - 85s 56ms/step - loss: 0.8448 - acc: 0.5920 - val_loss: 0.8667 - val_acc: 0.5947\n",
      "Epoch 6/10\n",
      "1521/1521 [==============================] - 86s 56ms/step - loss: 0.8349 - acc: 0.5995 - val_loss: 0.8991 - val_acc: 0.5808\n",
      "Epoch 7/10\n",
      "1521/1521 [==============================] - 86s 56ms/step - loss: 0.8296 - acc: 0.6030 - val_loss: 0.8624 - val_acc: 0.6032\n",
      "Epoch 8/10\n",
      "1521/1521 [==============================] - 86s 56ms/step - loss: 0.8262 - acc: 0.6042 - val_loss: 0.8031 - val_acc: 0.6228\n",
      "Epoch 9/10\n",
      "1521/1521 [==============================] - 85s 56ms/step - loss: 0.8173 - acc: 0.6082 - val_loss: 0.7961 - val_acc: 0.6269\n",
      "Epoch 10/10\n",
      "1521/1521 [==============================] - 86s 56ms/step - loss: 0.8148 - acc: 0.6086 - val_loss: 0.7944 - val_acc: 0.6231\n",
      "Test score: 0.7944222856509058\n",
      "Test accuracy: 0.6231085526315789\n",
      "Stapelgroesse (batchSize): 32\n",
      "Anzahl der Faltungsschichten: 5Layer\n",
      "Aktivierungsfunktion: relu\n",
      "Optimierungsfunktion: Adam\n",
      "Dropout-Rate Faltungsschicht 1: 0.03784859630175669\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 64\n",
      "Dropout-Rate Faltungsschicht 2: 0.6475624675409367\n",
      "Anzahl der Faltungsschichten: 5Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 64\n",
      "Dropout-Rate Faltungsschicht 3: 0.07605493406267955\n",
      "Anzahl der Filter-Maps Faltungsschicht 4: 64\n",
      "Dropout-Rate Faltungsschicht 4: 0.36656570380794323\n",
      "Anzahl der Filter-Maps Faltungsschicht 5: 64\n",
      "Dropout-Rate Faltungsschicht 5: 0.3645909423573867\n",
      "Anzahl der Neuronen des Fully Connected Layer: 64\n",
      "Dropout-Rate Fully Connected Layer: 0.4688951121094675\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "1521/1521 [==============================] - 98s 64ms/step - loss: 1.0996 - acc: 0.3337 - val_loss: 1.0988 - val_acc: 0.3323\n",
      "Epoch 2/10\n",
      "1521/1521 [==============================] - 96s 63ms/step - loss: 1.0988 - acc: 0.3320 - val_loss: 1.0987 - val_acc: 0.3325\n",
      "Epoch 3/10\n",
      "1521/1521 [==============================] - 95s 63ms/step - loss: 1.0987 - acc: 0.3319 - val_loss: 1.0987 - val_acc: 0.3325\n",
      "Epoch 4/10\n",
      "1521/1521 [==============================] - 95s 63ms/step - loss: 1.0987 - acc: 0.3318 - val_loss: 1.0987 - val_acc: 0.3325\n",
      "Epoch 5/10\n",
      "1521/1521 [==============================] - 95s 63ms/step - loss: 1.0987 - acc: 0.3328 - val_loss: 1.0987 - val_acc: 0.3325\n",
      "Epoch 6/10\n",
      "1521/1521 [==============================] - 95s 63ms/step - loss: 1.0987 - acc: 0.3325 - val_loss: 1.0987 - val_acc: 0.3325\n",
      "Epoch 7/10\n",
      "1521/1521 [==============================] - 95s 63ms/step - loss: 1.0987 - acc: 0.3324 - val_loss: 1.0987 - val_acc: 0.3325\n",
      "Epoch 8/10\n",
      "1521/1521 [==============================] - 95s 63ms/step - loss: 1.0987 - acc: 0.3322 - val_loss: 1.0987 - val_acc: 0.3325\n",
      "Epoch 9/10\n",
      "1521/1521 [==============================] - 95s 63ms/step - loss: 1.0987 - acc: 0.3325 - val_loss: 1.0987 - val_acc: 0.3325\n",
      "Epoch 10/10\n",
      "1521/1521 [==============================] - 95s 63ms/step - loss: 1.0987 - acc: 0.3325 - val_loss: 1.0987 - val_acc: 0.3325\n",
      "Test score: 1.0986950698651765\n",
      "Test accuracy: 0.3323190789473684\n",
      "Stapelgroesse (batchSize): 32\n",
      "Anzahl der Faltungsschichten: 5Layer\n",
      "Aktivierungsfunktion: relu\n",
      "Optimierungsfunktion: RMSprop\n",
      "Dropout-Rate Faltungsschicht 1: 0.4063029446461945\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 64\n",
      "Dropout-Rate Faltungsschicht 2: 0.04103941739720152\n",
      "Anzahl der Faltungsschichten: 5Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 64\n",
      "Dropout-Rate Faltungsschicht 3: 0.5298945883736625\n",
      "Anzahl der Filter-Maps Faltungsschicht 4: 64\n",
      "Dropout-Rate Faltungsschicht 4: 0.1126017636345988\n",
      "Anzahl der Filter-Maps Faltungsschicht 5: 128\n",
      "Dropout-Rate Faltungsschicht 5: 0.1951722566868454\n",
      "Anzahl der Neuronen des Fully Connected Layer: 64\n",
      "Dropout-Rate Fully Connected Layer: 0.6301271821968158\n",
      "Faltungsnetz wird trainiert...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1521/1521 [==============================] - 98s 64ms/step - loss: 1.0762 - acc: 0.4135 - val_loss: 0.9976 - val_acc: 0.4824\n",
      "Epoch 2/10\n",
      "1521/1521 [==============================] - 96s 63ms/step - loss: 1.0234 - acc: 0.4743 - val_loss: 0.9937 - val_acc: 0.4802\n",
      "Epoch 3/10\n",
      "1521/1521 [==============================] - 96s 63ms/step - loss: 0.9810 - acc: 0.5086 - val_loss: 0.9948 - val_acc: 0.5046\n",
      "Epoch 4/10\n",
      "1521/1521 [==============================] - 96s 63ms/step - loss: 0.9649 - acc: 0.5217 - val_loss: 1.0470 - val_acc: 0.4467\n",
      "Epoch 5/10\n",
      "1521/1521 [==============================] - 96s 63ms/step - loss: 0.9588 - acc: 0.5343 - val_loss: 0.9032 - val_acc: 0.5624\n",
      "Epoch 6/10\n",
      "1521/1521 [==============================] - 96s 63ms/step - loss: 0.9391 - acc: 0.5441 - val_loss: 0.9486 - val_acc: 0.5248\n",
      "Epoch 7/10\n",
      "1521/1521 [==============================] - 96s 63ms/step - loss: 0.9338 - acc: 0.5505 - val_loss: 0.9198 - val_acc: 0.5455\n",
      "Epoch 8/10\n",
      "1521/1521 [==============================] - 96s 63ms/step - loss: 0.9378 - acc: 0.5449 - val_loss: 0.9984 - val_acc: 0.4666\n",
      "Epoch 9/10\n",
      "1521/1521 [==============================] - 96s 63ms/step - loss: 0.9445 - acc: 0.5476 - val_loss: 1.1074 - val_acc: 0.3908\n",
      "Epoch 10/10\n",
      "1521/1521 [==============================] - 96s 63ms/step - loss: 0.9398 - acc: 0.5450 - val_loss: 0.9674 - val_acc: 0.5526\n",
      "Test score: 0.9676417040197473\n",
      "Test accuracy: 0.5525493421052632\n",
      "Stapelgroesse (batchSize): 16\n",
      "Anzahl der Faltungsschichten: 4Layer\n",
      "Aktivierungsfunktion: elu\n",
      "Optimierungsfunktion: RMSprop\n",
      "Dropout-Rate Faltungsschicht 1: 0.6117517831509787\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 64\n",
      "Dropout-Rate Faltungsschicht 2: 0.07874046469699175\n",
      "Anzahl der Faltungsschichten: 4Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 128\n",
      "Dropout-Rate Faltungsschicht 3: 0.38879746602236\n",
      "Anzahl der Filter-Maps Faltungsschicht 4: 64\n",
      "Dropout-Rate Faltungsschicht 4: 0.10165264323193196\n",
      "Anzahl der Neuronen des Fully Connected Layer: 64\n",
      "Dropout-Rate Fully Connected Layer: 0.30294519188552504\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "3042/3042 [==============================] - 121s 40ms/step - loss: 1.1448 - acc: 0.4293 - val_loss: 1.0504 - val_acc: 0.4509\n",
      "Epoch 2/10\n",
      "3042/3042 [==============================] - 119s 39ms/step - loss: 0.9728 - acc: 0.5048 - val_loss: 0.9170 - val_acc: 0.5436\n",
      "Epoch 3/10\n",
      "3042/3042 [==============================] - 119s 39ms/step - loss: 0.9453 - acc: 0.5272 - val_loss: 1.1011 - val_acc: 0.4846\n",
      "Epoch 4/10\n",
      "3042/3042 [==============================] - 119s 39ms/step - loss: 0.9416 - acc: 0.5306 - val_loss: 0.9562 - val_acc: 0.5065\n",
      "Epoch 5/10\n",
      "3042/3042 [==============================] - 119s 39ms/step - loss: 0.9381 - acc: 0.5336 - val_loss: 0.9248 - val_acc: 0.5368\n",
      "Epoch 6/10\n",
      "3042/3042 [==============================] - 119s 39ms/step - loss: 0.9408 - acc: 0.5354 - val_loss: 1.1272 - val_acc: 0.5056\n",
      "Epoch 7/10\n",
      "3042/3042 [==============================] - 119s 39ms/step - loss: 0.9475 - acc: 0.5304 - val_loss: 0.8805 - val_acc: 0.5861\n",
      "Epoch 8/10\n",
      "3042/3042 [==============================] - 119s 39ms/step - loss: 0.9557 - acc: 0.5196 - val_loss: 0.9508 - val_acc: 0.4935\n",
      "Epoch 9/10\n",
      "3042/3042 [==============================] - 119s 39ms/step - loss: 0.9521 - acc: 0.5214 - val_loss: 0.9309 - val_acc: 0.5573\n",
      "Epoch 10/10\n",
      "3042/3042 [==============================] - 119s 39ms/step - loss: 0.9602 - acc: 0.5146 - val_loss: 1.1408 - val_acc: 0.4751\n",
      "Test score: 1.140751149309309\n",
      "Test accuracy: 0.4752467105263158\n",
      "Stapelgroesse (batchSize): 32\n",
      "Anzahl der Faltungsschichten: 4Layer\n",
      "Aktivierungsfunktion: relu\n",
      "Optimierungsfunktion: RMSprop\n",
      "Dropout-Rate Faltungsschicht 1: 0.2636404947056047\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 64\n",
      "Dropout-Rate Faltungsschicht 2: 0.6836210943752541\n",
      "Anzahl der Faltungsschichten: 4Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 128\n",
      "Dropout-Rate Faltungsschicht 3: 0.22966076346119307\n",
      "Anzahl der Filter-Maps Faltungsschicht 4: 128\n",
      "Dropout-Rate Faltungsschicht 4: 0.6582916558341014\n",
      "Anzahl der Neuronen des Fully Connected Layer: 128\n",
      "Dropout-Rate Fully Connected Layer: 0.3055128722061833\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "1521/1521 [==============================] - 106s 70ms/step - loss: 1.0632 - acc: 0.4315 - val_loss: 1.0696 - val_acc: 0.3931\n",
      "Epoch 2/10\n",
      "1521/1521 [==============================] - 104s 68ms/step - loss: 1.0264 - acc: 0.4675 - val_loss: 1.0037 - val_acc: 0.4858\n",
      "Epoch 3/10\n",
      "1521/1521 [==============================] - 104s 68ms/step - loss: 0.9995 - acc: 0.4917 - val_loss: 1.0496 - val_acc: 0.4595\n",
      "Epoch 4/10\n",
      "1521/1521 [==============================] - 104s 68ms/step - loss: 0.9841 - acc: 0.5047 - val_loss: 1.0282 - val_acc: 0.4797\n",
      "Epoch 5/10\n",
      "1521/1521 [==============================] - 104s 68ms/step - loss: 0.9694 - acc: 0.5191 - val_loss: 0.9679 - val_acc: 0.5188\n",
      "Epoch 6/10\n",
      "1521/1521 [==============================] - 104s 68ms/step - loss: 0.9621 - acc: 0.5287 - val_loss: 0.9850 - val_acc: 0.5000\n",
      "Epoch 7/10\n",
      "1521/1521 [==============================] - 104s 68ms/step - loss: 0.9484 - acc: 0.5333 - val_loss: 0.9769 - val_acc: 0.4925\n",
      "Epoch 8/10\n",
      "1521/1521 [==============================] - 104s 68ms/step - loss: 0.9500 - acc: 0.5362 - val_loss: 1.0261 - val_acc: 0.4497\n",
      "Epoch 9/10\n",
      "1521/1521 [==============================] - 104s 68ms/step - loss: 0.9523 - acc: 0.5366 - val_loss: 1.0203 - val_acc: 0.4538\n",
      "Epoch 10/10\n",
      "1521/1521 [==============================] - 104s 68ms/step - loss: 0.9466 - acc: 0.5406 - val_loss: 0.9873 - val_acc: 0.5175\n",
      "Test score: 0.9873156003261867\n",
      "Test accuracy: 0.5174342105263158\n",
      "Stapelgroesse (batchSize): 8\n",
      "Anzahl der Faltungsschichten: 4Layer\n",
      "Aktivierungsfunktion: elu\n",
      "Optimierungsfunktion: RMSprop\n",
      "Dropout-Rate Faltungsschicht 1: 0.04094223486538234\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 32\n",
      "Dropout-Rate Faltungsschicht 2: 0.5501173450933593\n",
      "Anzahl der Faltungsschichten: 4Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 128\n",
      "Dropout-Rate Faltungsschicht 3: 0.24209449821680215\n",
      "Anzahl der Filter-Maps Faltungsschicht 4: 128\n",
      "Dropout-Rate Faltungsschicht 4: 0.47065615224871427\n",
      "Anzahl der Neuronen des Fully Connected Layer: 128\n",
      "Dropout-Rate Fully Connected Layer: 0.3212478490516954\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "6084/6084 [==============================] - 144s 24ms/step - loss: 10.7476 - acc: 0.3329 - val_loss: 10.7414 - val_acc: 0.3336\n",
      "Epoch 2/10\n",
      "6084/6084 [==============================] - 141s 23ms/step - loss: 10.7501 - acc: 0.3330 - val_loss: 10.7401 - val_acc: 0.3337\n",
      "Epoch 3/10\n",
      "6084/6084 [==============================] - 142s 23ms/step - loss: 10.7508 - acc: 0.3330 - val_loss: 10.7414 - val_acc: 0.3336\n",
      "Epoch 4/10\n",
      "6084/6084 [==============================] - 141s 23ms/step - loss: 10.7504 - acc: 0.3330 - val_loss: 10.7414 - val_acc: 0.3336\n",
      "Epoch 5/10\n",
      "6084/6084 [==============================] - 141s 23ms/step - loss: 10.7733 - acc: 0.3315 - val_loss: 10.7308 - val_acc: 0.3342\n",
      "Epoch 6/10\n",
      "6084/6084 [==============================] - 141s 23ms/step - loss: 10.7721 - acc: 0.3316 - val_loss: 10.7441 - val_acc: 0.3334\n",
      "Epoch 7/10\n",
      "6084/6084 [==============================] - 141s 23ms/step - loss: 10.7508 - acc: 0.3330 - val_loss: 10.7401 - val_acc: 0.3337\n",
      "Epoch 8/10\n",
      "6084/6084 [==============================] - 141s 23ms/step - loss: 10.7498 - acc: 0.3331 - val_loss: 10.7401 - val_acc: 0.3337\n",
      "Epoch 9/10\n",
      "6084/6084 [==============================] - 141s 23ms/step - loss: 10.7504 - acc: 0.3330 - val_loss: 10.7414 - val_acc: 0.3336\n",
      "Epoch 10/10\n",
      "6084/6084 [==============================] - 141s 23ms/step - loss: 10.7504 - acc: 0.3330 - val_loss: 10.7401 - val_acc: 0.3337\n",
      "Test score: 10.741423042568229\n",
      "Test accuracy: 0.33357988165680474\n",
      "Stapelgroesse (batchSize): 16\n",
      "Anzahl der Faltungsschichten: 5Layer\n",
      "Aktivierungsfunktion: relu\n",
      "Optimierungsfunktion: RMSprop\n",
      "Dropout-Rate Faltungsschicht 1: 0.1611811624500729\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 64\n",
      "Dropout-Rate Faltungsschicht 2: 0.26908596491655257\n",
      "Anzahl der Faltungsschichten: 5Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 128\n",
      "Dropout-Rate Faltungsschicht 3: 0.034636694400194755\n",
      "Anzahl der Filter-Maps Faltungsschicht 4: 64\n",
      "Dropout-Rate Faltungsschicht 4: 0.6907288802727536\n",
      "Anzahl der Filter-Maps Faltungsschicht 5: 64\n",
      "Dropout-Rate Faltungsschicht 5: 0.13406699491026297\n",
      "Anzahl der Neuronen des Fully Connected Layer: 64\n",
      "Dropout-Rate Fully Connected Layer: 0.10524234787979274\n",
      "Faltungsnetz wird trainiert...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3042/3042 [==============================] - 123s 41ms/step - loss: 1.0499 - acc: 0.4482 - val_loss: 0.9863 - val_acc: 0.4961\n",
      "Epoch 2/10\n",
      "3042/3042 [==============================] - 121s 40ms/step - loss: 0.9862 - acc: 0.5074 - val_loss: 0.9347 - val_acc: 0.5422\n",
      "Epoch 3/10\n",
      "3042/3042 [==============================] - 121s 40ms/step - loss: 0.9663 - acc: 0.5188 - val_loss: 0.9681 - val_acc: 0.5244\n",
      "Epoch 4/10\n",
      "3042/3042 [==============================] - 121s 40ms/step - loss: 0.9625 - acc: 0.5266 - val_loss: 0.9434 - val_acc: 0.5613\n",
      "Epoch 5/10\n",
      "3042/3042 [==============================] - 121s 40ms/step - loss: 0.9599 - acc: 0.5292 - val_loss: 0.9766 - val_acc: 0.5221\n",
      "Epoch 6/10\n",
      "3042/3042 [==============================] - 121s 40ms/step - loss: 0.9579 - acc: 0.5310 - val_loss: 1.0370 - val_acc: 0.5120\n",
      "Epoch 7/10\n",
      "3042/3042 [==============================] - 121s 40ms/step - loss: 0.9642 - acc: 0.5290 - val_loss: 1.0147 - val_acc: 0.5318\n",
      "Epoch 8/10\n",
      "3042/3042 [==============================] - 121s 40ms/step - loss: 0.9740 - acc: 0.5277 - val_loss: 1.0407 - val_acc: 0.4742\n",
      "Epoch 9/10\n",
      "3042/3042 [==============================] - 121s 40ms/step - loss: 3.6360 - acc: 0.4692 - val_loss: 1.0534 - val_acc: 0.4190\n",
      "Epoch 10/10\n",
      "3042/3042 [==============================] - 121s 40ms/step - loss: 0.9769 - acc: 0.5183 - val_loss: 0.9985 - val_acc: 0.4961\n",
      "Test score: 0.9984505639264458\n",
      "Test accuracy: 0.49613486842105264\n",
      "Stapelgroesse (batchSize): 16\n",
      "Anzahl der Faltungsschichten: 3Layer\n",
      "Aktivierungsfunktion: elu\n",
      "Optimierungsfunktion: Adam\n",
      "Dropout-Rate Faltungsschicht 1: 0.5430043368754298\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 64\n",
      "Dropout-Rate Faltungsschicht 2: 0.06715690813728312\n",
      "Anzahl der Faltungsschichten: 3Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 128\n",
      "Dropout-Rate Faltungsschicht 3: 0.1964720930822093\n",
      "Anzahl der Neuronen des Fully Connected Layer: 64\n",
      "Dropout-Rate Fully Connected Layer: 0.5045534914382528\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "3042/3042 [==============================] - 125s 41ms/step - loss: 1.2157 - acc: 0.3352 - val_loss: 1.0979 - val_acc: 0.3405\n",
      "Epoch 2/10\n",
      "3042/3042 [==============================] - 123s 41ms/step - loss: 1.1069 - acc: 0.3312 - val_loss: 1.0989 - val_acc: 0.3336\n",
      "Epoch 3/10\n",
      "3042/3042 [==============================] - 123s 41ms/step - loss: 1.1069 - acc: 0.3298 - val_loss: 1.0993 - val_acc: 0.3335\n",
      "Epoch 4/10\n",
      "3042/3042 [==============================] - 123s 41ms/step - loss: 1.1145 - acc: 0.3319 - val_loss: 1.0995 - val_acc: 0.3335\n",
      "Epoch 5/10\n",
      "3042/3042 [==============================] - 123s 40ms/step - loss: 1.1070 - acc: 0.3306 - val_loss: 1.1003 - val_acc: 0.3335\n",
      "Epoch 6/10\n",
      "3042/3042 [==============================] - 123s 41ms/step - loss: 1.1069 - acc: 0.3313 - val_loss: 1.1005 - val_acc: 0.3336\n",
      "Epoch 7/10\n",
      "3042/3042 [==============================] - 123s 41ms/step - loss: 1.1067 - acc: 0.3321 - val_loss: 1.1005 - val_acc: 0.3334\n",
      "Epoch 8/10\n",
      "3042/3042 [==============================] - 123s 41ms/step - loss: 1.1067 - acc: 0.3319 - val_loss: 1.1014 - val_acc: 0.3337\n",
      "Epoch 9/10\n",
      "3042/3042 [==============================] - 123s 40ms/step - loss: 1.1070 - acc: 0.3314 - val_loss: 1.1012 - val_acc: 0.3337\n",
      "Epoch 10/10\n",
      "3042/3042 [==============================] - 123s 41ms/step - loss: 1.1072 - acc: 0.3295 - val_loss: 1.1019 - val_acc: 0.3337\n",
      "Test score: 1.101934928172513\n",
      "Test accuracy: 0.3335526315789474\n",
      "Stapelgroesse (batchSize): 32\n",
      "Anzahl der Faltungsschichten: 4Layer\n",
      "Aktivierungsfunktion: relu\n",
      "Optimierungsfunktion: Adam\n",
      "Dropout-Rate Faltungsschicht 1: 0.4161322928479168\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 64\n",
      "Dropout-Rate Faltungsschicht 2: 0.07400274876473123\n",
      "Anzahl der Faltungsschichten: 4Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 64\n",
      "Dropout-Rate Faltungsschicht 3: 0.5207759314862476\n",
      "Anzahl der Filter-Maps Faltungsschicht 4: 128\n",
      "Dropout-Rate Faltungsschicht 4: 0.2108262930092021\n",
      "Anzahl der Neuronen des Fully Connected Layer: 128\n",
      "Dropout-Rate Fully Connected Layer: 0.3102813339921929\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "1521/1521 [==============================] - 100s 66ms/step - loss: 1.0506 - acc: 0.4258 - val_loss: 1.0350 - val_acc: 0.4490\n",
      "Epoch 2/10\n",
      "1521/1521 [==============================] - 97s 64ms/step - loss: 1.0082 - acc: 0.4688 - val_loss: 1.0301 - val_acc: 0.4502\n",
      "Epoch 3/10\n",
      "1521/1521 [==============================] - 97s 64ms/step - loss: 0.9921 - acc: 0.4856 - val_loss: 1.0138 - val_acc: 0.4565\n",
      "Epoch 4/10\n",
      "1521/1521 [==============================] - 97s 64ms/step - loss: 0.9786 - acc: 0.4961 - val_loss: 0.9828 - val_acc: 0.4942\n",
      "Epoch 5/10\n",
      "1521/1521 [==============================] - 97s 64ms/step - loss: 0.9568 - acc: 0.5144 - val_loss: 1.0000 - val_acc: 0.4883\n",
      "Epoch 6/10\n",
      "1521/1521 [==============================] - 97s 64ms/step - loss: 0.9395 - acc: 0.5271 - val_loss: 1.0226 - val_acc: 0.4627\n",
      "Epoch 7/10\n",
      "1521/1521 [==============================] - 97s 64ms/step - loss: 0.9209 - acc: 0.5420 - val_loss: 1.0006 - val_acc: 0.4868\n",
      "Epoch 8/10\n",
      "1521/1521 [==============================] - 97s 64ms/step - loss: 0.9050 - acc: 0.5554 - val_loss: 0.9671 - val_acc: 0.5143\n",
      "Epoch 9/10\n",
      "1521/1521 [==============================] - 97s 64ms/step - loss: 0.8942 - acc: 0.5592 - val_loss: 1.0089 - val_acc: 0.4814\n",
      "Epoch 10/10\n",
      "1521/1521 [==============================] - 97s 64ms/step - loss: 0.8864 - acc: 0.5665 - val_loss: 1.0394 - val_acc: 0.4585\n",
      "Test score: 1.0398296306007786\n",
      "Test accuracy: 0.4586348684210526\n",
      "Stapelgroesse (batchSize): 16\n",
      "Anzahl der Faltungsschichten: 3Layer\n",
      "Aktivierungsfunktion: relu\n",
      "Optimierungsfunktion: RMSprop\n",
      "Dropout-Rate Faltungsschicht 1: 0.33451399159802375\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 32\n",
      "Dropout-Rate Faltungsschicht 2: 0.25767222312424426\n",
      "Anzahl der Faltungsschichten: 3Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 64\n",
      "Dropout-Rate Faltungsschicht 3: 0.36484964265506686\n",
      "Anzahl der Neuronen des Fully Connected Layer: 64\n",
      "Dropout-Rate Fully Connected Layer: 0.4207538587369814\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "3042/3042 [==============================] - 98s 32ms/step - loss: 1.0518 - acc: 0.4436 - val_loss: 1.0106 - val_acc: 0.4676\n",
      "Epoch 2/10\n",
      "3042/3042 [==============================] - 95s 31ms/step - loss: 0.9885 - acc: 0.5013 - val_loss: 0.9928 - val_acc: 0.4707\n",
      "Epoch 3/10\n",
      "3042/3042 [==============================] - 95s 31ms/step - loss: 0.9662 - acc: 0.5167 - val_loss: 0.9830 - val_acc: 0.4844\n",
      "Epoch 4/10\n",
      "3042/3042 [==============================] - 95s 31ms/step - loss: 0.9627 - acc: 0.5244 - val_loss: 0.9866 - val_acc: 0.4787\n",
      "Epoch 5/10\n",
      "3042/3042 [==============================] - 95s 31ms/step - loss: 0.9557 - acc: 0.5265 - val_loss: 1.0501 - val_acc: 0.3993\n",
      "Epoch 6/10\n",
      "3042/3042 [==============================] - 95s 31ms/step - loss: 0.9494 - acc: 0.5306 - val_loss: 0.9687 - val_acc: 0.5005\n",
      "Epoch 7/10\n",
      "3042/3042 [==============================] - 95s 31ms/step - loss: 0.9501 - acc: 0.5280 - val_loss: 0.9489 - val_acc: 0.5220\n",
      "Epoch 8/10\n",
      "3042/3042 [==============================] - 95s 31ms/step - loss: 0.9498 - acc: 0.5268 - val_loss: 0.9717 - val_acc: 0.4835\n",
      "Epoch 9/10\n",
      "3042/3042 [==============================] - 95s 31ms/step - loss: 0.9510 - acc: 0.5296 - val_loss: 0.9978 - val_acc: 0.4459\n",
      "Epoch 10/10\n",
      "3042/3042 [==============================] - 95s 31ms/step - loss: 0.9554 - acc: 0.5306 - val_loss: 0.9912 - val_acc: 0.4460\n",
      "Test score: 0.9913722431973407\n",
      "Test accuracy: 0.44564144736842104\n",
      "Stapelgroesse (batchSize): 32\n",
      "Anzahl der Faltungsschichten: 3Layer\n",
      "Aktivierungsfunktion: relu\n",
      "Optimierungsfunktion: RMSprop\n",
      "Dropout-Rate Faltungsschicht 1: 0.5898991773841765\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 32\n",
      "Dropout-Rate Faltungsschicht 2: 0.15111715163692865\n",
      "Anzahl der Faltungsschichten: 3Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 128\n",
      "Dropout-Rate Faltungsschicht 3: 0.0037666668932191723\n",
      "Anzahl der Neuronen des Fully Connected Layer: 64\n",
      "Dropout-Rate Fully Connected Layer: 0.23702840408628864\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "1521/1521 [==============================] - 173s 114ms/step - loss: 1.0611 - acc: 0.4398 - val_loss: 0.9999 - val_acc: 0.4726\n",
      "Epoch 2/10\n",
      "1521/1521 [==============================] - 123s 81ms/step - loss: 1.0067 - acc: 0.4783 - val_loss: 0.9713 - val_acc: 0.5002\n",
      "Epoch 3/10\n",
      "1521/1521 [==============================] - 87s 57ms/step - loss: 0.9818 - acc: 0.5005 - val_loss: 0.9715 - val_acc: 0.4992\n",
      "Epoch 4/10\n",
      "1521/1521 [==============================] - 87s 57ms/step - loss: 0.9593 - acc: 0.5211 - val_loss: 0.9602 - val_acc: 0.5231\n",
      "Epoch 5/10\n",
      "1521/1521 [==============================] - 87s 57ms/step - loss: 0.9392 - acc: 0.5337 - val_loss: 0.9319 - val_acc: 0.5543\n",
      "Epoch 6/10\n",
      "1521/1521 [==============================] - 88s 58ms/step - loss: 0.9311 - acc: 0.5416 - val_loss: 0.9338 - val_acc: 0.5478\n",
      "Epoch 7/10\n",
      "1521/1521 [==============================] - 88s 58ms/step - loss: 0.9168 - acc: 0.5499 - val_loss: 0.9647 - val_acc: 0.5096\n",
      "Epoch 8/10\n",
      "1521/1521 [==============================] - 87s 57ms/step - loss: 0.9126 - acc: 0.5559 - val_loss: 0.9750 - val_acc: 0.4991\n",
      "Epoch 9/10\n",
      "1521/1521 [==============================] - 87s 57ms/step - loss: 0.9077 - acc: 0.5563 - val_loss: 0.9300 - val_acc: 0.5661\n",
      "Epoch 10/10\n",
      "1521/1521 [==============================] - 87s 57ms/step - loss: 0.9068 - acc: 0.5605 - val_loss: 0.9012 - val_acc: 0.5651\n",
      "Test score: 0.9011892619885896\n",
      "Test accuracy: 0.5652960526315789\n",
      "Stapelgroesse (batchSize): 16\n",
      "Anzahl der Faltungsschichten: 4Layer\n",
      "Aktivierungsfunktion: elu\n",
      "Optimierungsfunktion: Adam\n",
      "Dropout-Rate Faltungsschicht 1: 0.6258522155693698\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 64\n",
      "Dropout-Rate Faltungsschicht 2: 0.12809110971442503\n",
      "Anzahl der Faltungsschichten: 4Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 128\n",
      "Dropout-Rate Faltungsschicht 3: 0.3483976272358616\n",
      "Anzahl der Filter-Maps Faltungsschicht 4: 64\n",
      "Dropout-Rate Faltungsschicht 4: 0.46689731505582\n",
      "Anzahl der Neuronen des Fully Connected Layer: 128\n",
      "Dropout-Rate Fully Connected Layer: 0.6432023455140354\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "3042/3042 [==============================] - 124s 41ms/step - loss: 1.0592 - acc: 0.4376 - val_loss: 1.0152 - val_acc: 0.4484\n",
      "Epoch 2/10\n",
      "3042/3042 [==============================] - 121s 40ms/step - loss: 1.0323 - acc: 0.4532 - val_loss: 1.1137 - val_acc: 0.4302\n",
      "Epoch 3/10\n",
      "3042/3042 [==============================] - 121s 40ms/step - loss: 1.0286 - acc: 0.4595 - val_loss: 1.0162 - val_acc: 0.4120\n",
      "Epoch 4/10\n",
      "3042/3042 [==============================] - 121s 40ms/step - loss: 1.0242 - acc: 0.4581 - val_loss: 0.9987 - val_acc: 0.3833\n",
      "Epoch 5/10\n",
      "3042/3042 [==============================] - 121s 40ms/step - loss: 1.0234 - acc: 0.4573 - val_loss: 0.9683 - val_acc: 0.5005\n",
      "Epoch 6/10\n",
      "3042/3042 [==============================] - 121s 40ms/step - loss: 1.0336 - acc: 0.4489 - val_loss: 1.0739 - val_acc: 0.3651\n",
      "Epoch 7/10\n",
      "3042/3042 [==============================] - 121s 40ms/step - loss: 1.0364 - acc: 0.4469 - val_loss: 0.9936 - val_acc: 0.4963\n",
      "Epoch 8/10\n",
      "3042/3042 [==============================] - 121s 40ms/step - loss: 1.0395 - acc: 0.4433 - val_loss: 1.0334 - val_acc: 0.4401\n",
      "Epoch 9/10\n",
      "3042/3042 [==============================] - 121s 40ms/step - loss: 1.0478 - acc: 0.4317 - val_loss: 1.0651 - val_acc: 0.3818\n",
      "Epoch 10/10\n",
      "3042/3042 [==============================] - 121s 40ms/step - loss: 1.0558 - acc: 0.4248 - val_loss: 1.0140 - val_acc: 0.4314\n",
      "Test score: 1.0140524523822885\n",
      "Test accuracy: 0.43116776315789473\n",
      "Stapelgroesse (batchSize): 16\n",
      "Anzahl der Faltungsschichten: 5Layer\n",
      "Aktivierungsfunktion: elu\n",
      "Optimierungsfunktion: RMSprop\n",
      "Dropout-Rate Faltungsschicht 1: 0.6719647718091494\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 32\n",
      "Dropout-Rate Faltungsschicht 2: 0.4334039533095252\n",
      "Anzahl der Faltungsschichten: 5Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 64\n",
      "Dropout-Rate Faltungsschicht 3: 0.3852434388296144\n",
      "Anzahl der Filter-Maps Faltungsschicht 4: 128\n",
      "Dropout-Rate Faltungsschicht 4: 0.05792646430464581\n",
      "Anzahl der Filter-Maps Faltungsschicht 5: 128\n",
      "Dropout-Rate Faltungsschicht 5: 0.6201374941396659\n",
      "Anzahl der Neuronen des Fully Connected Layer: 128\n",
      "Dropout-Rate Fully Connected Layer: 0.057200250266939485\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "3042/3042 [==============================] - 108s 36ms/step - loss: 1.1545 - acc: 0.4220 - val_loss: 1.1245 - val_acc: 0.4169\n",
      "Epoch 2/10\n",
      "3042/3042 [==============================] - 106s 35ms/step - loss: 1.0064 - acc: 0.4852 - val_loss: 1.0680 - val_acc: 0.4448\n",
      "Epoch 3/10\n",
      "3042/3042 [==============================] - 106s 35ms/step - loss: 0.9908 - acc: 0.4959 - val_loss: 1.2316 - val_acc: 0.4600\n",
      "Epoch 4/10\n",
      "3042/3042 [==============================] - 106s 35ms/step - loss: 0.9813 - acc: 0.5031 - val_loss: 1.0822 - val_acc: 0.4164\n",
      "Epoch 5/10\n",
      "3042/3042 [==============================] - 106s 35ms/step - loss: 0.9770 - acc: 0.5064 - val_loss: 0.9839 - val_acc: 0.4306\n",
      "Epoch 6/10\n",
      "3042/3042 [==============================] - 106s 35ms/step - loss: 0.9787 - acc: 0.5086 - val_loss: 1.4179 - val_acc: 0.4737\n",
      "Epoch 7/10\n",
      "3042/3042 [==============================] - 106s 35ms/step - loss: 0.9793 - acc: 0.5038 - val_loss: 1.0846 - val_acc: 0.5188\n",
      "Epoch 8/10\n",
      "3042/3042 [==============================] - 106s 35ms/step - loss: 0.9777 - acc: 0.5034 - val_loss: 1.2037 - val_acc: 0.4999\n",
      "Epoch 9/10\n",
      "3042/3042 [==============================] - 106s 35ms/step - loss: 0.9786 - acc: 0.5042 - val_loss: 0.9833 - val_acc: 0.5549\n",
      "Epoch 10/10\n",
      "3042/3042 [==============================] - 106s 35ms/step - loss: 0.9836 - acc: 0.5045 - val_loss: 0.9231 - val_acc: 0.5286\n",
      "Test score: 0.9235436964191889\n",
      "Test accuracy: 0.5282894736842105\n",
      "Stapelgroesse (batchSize): 16\n",
      "Anzahl der Faltungsschichten: 3Layer\n",
      "Aktivierungsfunktion: elu\n",
      "Optimierungsfunktion: RMSprop\n",
      "Dropout-Rate Faltungsschicht 1: 0.3389652104130198\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 32\n",
      "Dropout-Rate Faltungsschicht 2: 0.1104098405551635\n",
      "Anzahl der Faltungsschichten: 3Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 128\n",
      "Dropout-Rate Faltungsschicht 3: 0.5296528997881041\n",
      "Anzahl der Neuronen des Fully Connected Layer: 64\n",
      "Dropout-Rate Fully Connected Layer: 0.13987297505909094\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "3042/3042 [==============================] - 110s 36ms/step - loss: 10.7411 - acc: 0.3333 - val_loss: 10.7326 - val_acc: 0.3341\n",
      "Epoch 2/10\n",
      "3042/3042 [==============================] - 107s 35ms/step - loss: 10.7497 - acc: 0.3331 - val_loss: 10.7339 - val_acc: 0.3340\n",
      "Epoch 3/10\n",
      "3042/3042 [==============================] - 107s 35ms/step - loss: 10.7510 - acc: 0.3330 - val_loss: 10.7339 - val_acc: 0.3340\n",
      "Epoch 4/10\n",
      "3042/3042 [==============================] - 107s 35ms/step - loss: 10.7500 - acc: 0.3330 - val_loss: 10.7339 - val_acc: 0.3340\n",
      "Epoch 5/10\n",
      "3042/3042 [==============================] - 107s 35ms/step - loss: 10.7507 - acc: 0.3330 - val_loss: 10.7326 - val_acc: 0.3341\n",
      "Epoch 6/10\n",
      "3042/3042 [==============================] - 107s 35ms/step - loss: 10.7497 - acc: 0.3331 - val_loss: 10.7326 - val_acc: 0.3341\n",
      "Epoch 7/10\n",
      "3042/3042 [==============================] - 107s 35ms/step - loss: 10.7507 - acc: 0.3330 - val_loss: 10.7299 - val_acc: 0.3343\n",
      "Epoch 8/10\n",
      "3042/3042 [==============================] - 107s 35ms/step - loss: 10.7514 - acc: 0.3330 - val_loss: 10.7299 - val_acc: 0.3343\n",
      "Epoch 9/10\n",
      "3042/3042 [==============================] - 107s 35ms/step - loss: 10.7497 - acc: 0.3331 - val_loss: 10.7379 - val_acc: 0.3338\n",
      "Epoch 10/10\n",
      "3042/3042 [==============================] - 107s 35ms/step - loss: 10.7507 - acc: 0.3330 - val_loss: 10.7366 - val_acc: 0.3339\n",
      "Test score: 10.732583765607131\n",
      "Test accuracy: 0.3341282894736842\n",
      "Stapelgroesse (batchSize): 32\n",
      "Anzahl der Faltungsschichten: 4Layer\n",
      "Aktivierungsfunktion: elu\n",
      "Optimierungsfunktion: RMSprop\n",
      "Dropout-Rate Faltungsschicht 1: 0.19303830922066545\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 64\n",
      "Dropout-Rate Faltungsschicht 2: 0.0840127538592151\n",
      "Anzahl der Faltungsschichten: 4Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 128\n",
      "Dropout-Rate Faltungsschicht 3: 0.4642820273620739\n",
      "Anzahl der Filter-Maps Faltungsschicht 4: 64\n",
      "Dropout-Rate Faltungsschicht 4: 0.09875567645458869\n",
      "Anzahl der Neuronen des Fully Connected Layer: 128\n",
      "Dropout-Rate Fully Connected Layer: 0.6570201177778964\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1521/1521 [==============================] - 107s 70ms/step - loss: 1.1086 - acc: 0.4226 - val_loss: 0.9242 - val_acc: 0.5343\n",
      "Epoch 2/10\n",
      "1521/1521 [==============================] - 104s 68ms/step - loss: 0.9278 - acc: 0.5384 - val_loss: 0.8901 - val_acc: 0.5444\n",
      "Epoch 3/10\n",
      "1521/1521 [==============================] - 104s 68ms/step - loss: 0.9014 - acc: 0.5574 - val_loss: 0.9102 - val_acc: 0.4960\n",
      "Epoch 4/10\n",
      "1521/1521 [==============================] - 104s 68ms/step - loss: 0.8913 - acc: 0.5646 - val_loss: 0.8546 - val_acc: 0.5732\n",
      "Epoch 5/10\n",
      "1521/1521 [==============================] - 104s 68ms/step - loss: 0.8850 - acc: 0.5691 - val_loss: 0.8387 - val_acc: 0.5987\n",
      "Epoch 6/10\n",
      "1521/1521 [==============================] - 104s 68ms/step - loss: 0.8809 - acc: 0.5731 - val_loss: 0.9636 - val_acc: 0.5258\n",
      "Epoch 7/10\n",
      "1521/1521 [==============================] - 104s 68ms/step - loss: 0.8799 - acc: 0.5731 - val_loss: 0.8485 - val_acc: 0.6052\n",
      "Epoch 8/10\n",
      "1521/1521 [==============================] - 104s 68ms/step - loss: 0.8818 - acc: 0.5750 - val_loss: 0.8655 - val_acc: 0.5843\n",
      "Epoch 9/10\n",
      "1521/1521 [==============================] - 104s 68ms/step - loss: 0.8820 - acc: 0.5752 - val_loss: 0.8415 - val_acc: 0.6050\n",
      "Epoch 10/10\n",
      "1521/1521 [==============================] - 104s 68ms/step - loss: 0.8821 - acc: 0.5746 - val_loss: 0.8229 - val_acc: 0.6147\n",
      "Test score: 0.8230994994703092\n",
      "Test accuracy: 0.6147203947368421\n",
      "Stapelgroesse (batchSize): 32\n",
      "Anzahl der Faltungsschichten: 5Layer\n",
      "Aktivierungsfunktion: elu\n",
      "Optimierungsfunktion: RMSprop\n",
      "Dropout-Rate Faltungsschicht 1: 0.0790967978439296\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 32\n",
      "Dropout-Rate Faltungsschicht 2: 0.5028747003251189\n",
      "Anzahl der Faltungsschichten: 5Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 128\n",
      "Dropout-Rate Faltungsschicht 3: 0.317826785536165\n",
      "Anzahl der Filter-Maps Faltungsschicht 4: 128\n",
      "Dropout-Rate Faltungsschicht 4: 0.007773182337481476\n",
      "Anzahl der Filter-Maps Faltungsschicht 5: 64\n",
      "Dropout-Rate Faltungsschicht 5: 0.6840469439004162\n",
      "Anzahl der Neuronen des Fully Connected Layer: 64\n",
      "Dropout-Rate Fully Connected Layer: 0.5558192810676943\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "1521/1521 [==============================] - 96s 63ms/step - loss: 1.0891 - acc: 0.4054 - val_loss: 0.9994 - val_acc: 0.4720\n",
      "Epoch 2/10\n",
      "1521/1521 [==============================] - 94s 61ms/step - loss: 0.9817 - acc: 0.5001 - val_loss: 0.9274 - val_acc: 0.5219\n",
      "Epoch 3/10\n",
      "1521/1521 [==============================] - 93s 61ms/step - loss: 0.9201 - acc: 0.5458 - val_loss: 0.8562 - val_acc: 0.5982\n",
      "Epoch 4/10\n",
      "1521/1521 [==============================] - 93s 61ms/step - loss: 0.9039 - acc: 0.5592 - val_loss: 0.8308 - val_acc: 0.6097\n",
      "Epoch 5/10\n",
      "1521/1521 [==============================] - 93s 61ms/step - loss: 0.8991 - acc: 0.5631 - val_loss: 0.8329 - val_acc: 0.6023\n",
      "Epoch 6/10\n",
      "1521/1521 [==============================] - 93s 61ms/step - loss: 0.8970 - acc: 0.5623 - val_loss: 0.8690 - val_acc: 0.5592\n",
      "Epoch 7/10\n",
      "1521/1521 [==============================] - 93s 61ms/step - loss: 0.8979 - acc: 0.5626 - val_loss: 0.8198 - val_acc: 0.6144\n",
      "Epoch 8/10\n",
      "1521/1521 [==============================] - 93s 61ms/step - loss: 0.8990 - acc: 0.5605 - val_loss: 0.9058 - val_acc: 0.5578\n",
      "Epoch 9/10\n",
      "1521/1521 [==============================] - 94s 61ms/step - loss: 0.9010 - acc: 0.5581 - val_loss: 0.9686 - val_acc: 0.5443\n",
      "Epoch 10/10\n",
      "1521/1521 [==============================] - 93s 61ms/step - loss: 0.9029 - acc: 0.5588 - val_loss: 0.8369 - val_acc: 0.5978\n",
      "Test score: 0.8367964957889757\n",
      "Test accuracy: 0.5979440789473685\n",
      "Stapelgroesse (batchSize): 32\n",
      "Anzahl der Faltungsschichten: 4Layer\n",
      "Aktivierungsfunktion: elu\n",
      "Optimierungsfunktion: RMSprop\n",
      "Dropout-Rate Faltungsschicht 1: 0.1318344215856727\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 32\n",
      "Dropout-Rate Faltungsschicht 2: 0.17097711507589985\n",
      "Anzahl der Faltungsschichten: 4Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 128\n",
      "Dropout-Rate Faltungsschicht 3: 0.13585476438335367\n",
      "Anzahl der Filter-Maps Faltungsschicht 4: 64\n",
      "Dropout-Rate Faltungsschicht 4: 0.0016035241808834422\n",
      "Anzahl der Neuronen des Fully Connected Layer: 128\n",
      "Dropout-Rate Fully Connected Layer: 0.19487524692545927\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "1521/1521 [==============================] - 93s 61ms/step - loss: 10.7223 - acc: 0.3340 - val_loss: 10.7617 - val_acc: 0.3323\n",
      "Epoch 2/10\n",
      "1521/1521 [==============================] - 90s 59ms/step - loss: 10.7355 - acc: 0.3339 - val_loss: 10.7587 - val_acc: 0.3325\n",
      "Epoch 3/10\n",
      "1521/1521 [==============================] - 90s 59ms/step - loss: 10.7362 - acc: 0.3339 - val_loss: 10.7587 - val_acc: 0.3325\n",
      "Epoch 4/10\n",
      "1521/1521 [==============================] - 90s 59ms/step - loss: 10.7372 - acc: 0.3338 - val_loss: 10.7587 - val_acc: 0.3325\n",
      "Epoch 5/10\n",
      "1521/1521 [==============================] - 90s 59ms/step - loss: 10.7349 - acc: 0.3340 - val_loss: 10.7587 - val_acc: 0.3325\n",
      "Epoch 6/10\n",
      "1521/1521 [==============================] - 90s 59ms/step - loss: 10.7355 - acc: 0.3339 - val_loss: 10.7587 - val_acc: 0.3325\n",
      "Epoch 7/10\n",
      "1521/1521 [==============================] - 90s 59ms/step - loss: 10.7375 - acc: 0.3338 - val_loss: 10.7587 - val_acc: 0.3325\n",
      "Epoch 8/10\n",
      "1521/1521 [==============================] - 90s 59ms/step - loss: 10.7355 - acc: 0.3339 - val_loss: 10.7587 - val_acc: 0.3325\n",
      "Epoch 9/10\n",
      "1521/1521 [==============================] - 90s 59ms/step - loss: 10.7359 - acc: 0.3339 - val_loss: 10.7587 - val_acc: 0.3325\n",
      "Epoch 10/10\n",
      "1521/1521 [==============================] - 90s 59ms/step - loss: 10.7359 - acc: 0.3339 - val_loss: 10.7587 - val_acc: 0.3325\n",
      "Test score: 10.76174474264446\n",
      "Test accuracy: 0.3323190789473684\n",
      "Stapelgroesse (batchSize): 8\n",
      "Anzahl der Faltungsschichten: 5Layer\n",
      "Aktivierungsfunktion: relu\n",
      "Optimierungsfunktion: Adam\n",
      "Dropout-Rate Faltungsschicht 1: 0.13872893014276017\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 32\n",
      "Dropout-Rate Faltungsschicht 2: 0.3829173687339869\n",
      "Anzahl der Faltungsschichten: 5Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 128\n",
      "Dropout-Rate Faltungsschicht 3: 0.07484323080037802\n",
      "Anzahl der Filter-Maps Faltungsschicht 4: 128\n",
      "Dropout-Rate Faltungsschicht 4: 0.1051655735427622\n",
      "Anzahl der Filter-Maps Faltungsschicht 5: 128\n",
      "Dropout-Rate Faltungsschicht 5: 0.682767227755717\n",
      "Anzahl der Neuronen des Fully Connected Layer: 64\n",
      "Dropout-Rate Fully Connected Layer: 0.5310225268328831\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "6084/6084 [==============================] - 149s 24ms/step - loss: 1.0992 - acc: 0.3319 - val_loss: 1.0988 - val_acc: 0.3323\n",
      "Epoch 2/10\n",
      "6084/6084 [==============================] - 146s 24ms/step - loss: 1.0988 - acc: 0.3344 - val_loss: 1.0988 - val_acc: 0.3325\n",
      "Epoch 3/10\n",
      "6084/6084 [==============================] - 146s 24ms/step - loss: 1.0988 - acc: 0.3321 - val_loss: 1.0988 - val_acc: 0.3323\n",
      "Epoch 4/10\n",
      "6084/6084 [==============================] - 146s 24ms/step - loss: 1.0988 - acc: 0.3321 - val_loss: 1.0988 - val_acc: 0.3322\n",
      "Epoch 5/10\n",
      "6084/6084 [==============================] - 146s 24ms/step - loss: 1.0988 - acc: 0.3321 - val_loss: 1.0988 - val_acc: 0.3324\n",
      "Epoch 6/10\n",
      "6084/6084 [==============================] - 146s 24ms/step - loss: 1.0988 - acc: 0.3321 - val_loss: 1.0988 - val_acc: 0.3324\n",
      "Epoch 7/10\n",
      "6084/6084 [==============================] - 146s 24ms/step - loss: 1.0988 - acc: 0.3321 - val_loss: 1.0988 - val_acc: 0.3324\n",
      "Epoch 8/10\n",
      "6084/6084 [==============================] - 146s 24ms/step - loss: 1.0988 - acc: 0.3321 - val_loss: 1.0988 - val_acc: 0.3324\n",
      "Epoch 9/10\n",
      "6084/6084 [==============================] - 146s 24ms/step - loss: 1.0988 - acc: 0.3321 - val_loss: 1.0988 - val_acc: 0.3324\n",
      "Epoch 10/10\n",
      "6084/6084 [==============================] - 146s 24ms/step - loss: 1.0988 - acc: 0.3321 - val_loss: 1.0988 - val_acc: 0.3323\n",
      "Test score: 1.0988241953727527\n",
      "Test accuracy: 0.33226495726495725\n",
      "Stapelgroesse (batchSize): 32\n",
      "Anzahl der Faltungsschichten: 4Layer\n",
      "Aktivierungsfunktion: elu\n",
      "Optimierungsfunktion: Adam\n",
      "Dropout-Rate Faltungsschicht 1: 0.3069084930878262\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 32\n",
      "Dropout-Rate Faltungsschicht 2: 0.37556917170016396\n",
      "Anzahl der Faltungsschichten: 4Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 64\n",
      "Dropout-Rate Faltungsschicht 3: 0.3627725845511256\n",
      "Anzahl der Filter-Maps Faltungsschicht 4: 64\n",
      "Dropout-Rate Faltungsschicht 4: 0.01796643755811378\n",
      "Anzahl der Neuronen des Fully Connected Layer: 128\n",
      "Dropout-Rate Fully Connected Layer: 0.21273340082487735\n",
      "Faltungsnetz wird trainiert...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1521/1521 [==============================] - 88s 58ms/step - loss: 1.0317 - acc: 0.4521 - val_loss: 0.9688 - val_acc: 0.4936\n",
      "Epoch 2/10\n",
      "1521/1521 [==============================] - 85s 56ms/step - loss: 0.9774 - acc: 0.4965 - val_loss: 0.9296 - val_acc: 0.5235\n",
      "Epoch 3/10\n",
      "1521/1521 [==============================] - 85s 56ms/step - loss: 0.9127 - acc: 0.5476 - val_loss: 0.9145 - val_acc: 0.5537\n",
      "Epoch 4/10\n",
      "1521/1521 [==============================] - 85s 56ms/step - loss: 0.8767 - acc: 0.5712 - val_loss: 0.8829 - val_acc: 0.5727\n",
      "Epoch 5/10\n",
      "1521/1521 [==============================] - 85s 56ms/step - loss: 0.8584 - acc: 0.5843 - val_loss: 0.9264 - val_acc: 0.5671\n",
      "Epoch 6/10\n",
      "1521/1521 [==============================] - 85s 56ms/step - loss: 0.8422 - acc: 0.5926 - val_loss: 0.8925 - val_acc: 0.5793\n",
      "Epoch 7/10\n",
      "1521/1521 [==============================] - 85s 56ms/step - loss: 0.8266 - acc: 0.6045 - val_loss: 0.8873 - val_acc: 0.5923\n",
      "Epoch 8/10\n",
      "1521/1521 [==============================] - 85s 56ms/step - loss: 0.8128 - acc: 0.6110 - val_loss: 0.8749 - val_acc: 0.5963\n",
      "Epoch 9/10\n",
      "1521/1521 [==============================] - 85s 56ms/step - loss: 0.7954 - acc: 0.6228 - val_loss: 0.9075 - val_acc: 0.5823\n",
      "Epoch 10/10\n",
      "1521/1521 [==============================] - 85s 56ms/step - loss: 0.7829 - acc: 0.6257 - val_loss: 0.9202 - val_acc: 0.5970\n",
      "Test score: 0.9202187770291379\n",
      "Test accuracy: 0.5972039473684211\n",
      "Stapelgroesse (batchSize): 8\n",
      "Anzahl der Faltungsschichten: 3Layer\n",
      "Aktivierungsfunktion: elu\n",
      "Optimierungsfunktion: Adam\n",
      "Dropout-Rate Faltungsschicht 1: 0.3941294727270916\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 64\n",
      "Dropout-Rate Faltungsschicht 2: 0.35096049291960096\n",
      "Anzahl der Faltungsschichten: 3Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 128\n",
      "Dropout-Rate Faltungsschicht 3: 0.5756135869158329\n",
      "Anzahl der Neuronen des Fully Connected Layer: 64\n",
      "Dropout-Rate Fully Connected Layer: 0.16966042548763868\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "6084/6084 [==============================] - 164s 27ms/step - loss: 10.7412 - acc: 0.3333 - val_loss: 10.7322 - val_acc: 0.3342\n",
      "Epoch 2/10\n",
      "6084/6084 [==============================] - 160s 26ms/step - loss: 10.7490 - acc: 0.3331 - val_loss: 10.7335 - val_acc: 0.3341\n",
      "Epoch 3/10\n",
      "6084/6084 [==============================] - 160s 26ms/step - loss: 10.7494 - acc: 0.3331 - val_loss: 10.7335 - val_acc: 0.3341\n",
      "Epoch 4/10\n",
      "6084/6084 [==============================] - 160s 26ms/step - loss: 10.7497 - acc: 0.3331 - val_loss: 10.7321 - val_acc: 0.3342\n",
      "Epoch 5/10\n",
      "6084/6084 [==============================] - 160s 26ms/step - loss: 10.7500 - acc: 0.3330 - val_loss: 10.7335 - val_acc: 0.3341\n",
      "Epoch 6/10\n",
      "6084/6084 [==============================] - 160s 26ms/step - loss: 10.7494 - acc: 0.3331 - val_loss: 10.7335 - val_acc: 0.3341\n",
      "Epoch 7/10\n",
      "6084/6084 [==============================] - 160s 26ms/step - loss: 10.7494 - acc: 0.3331 - val_loss: 10.7308 - val_acc: 0.3342\n",
      "Epoch 8/10\n",
      "6084/6084 [==============================] - 160s 26ms/step - loss: 10.7497 - acc: 0.3331 - val_loss: 10.7361 - val_acc: 0.3339\n",
      "Epoch 9/10\n",
      "6084/6084 [==============================] - 160s 26ms/step - loss: 10.7497 - acc: 0.3331 - val_loss: 10.7361 - val_acc: 0.3339\n",
      "Epoch 10/10\n",
      "6084/6084 [==============================] - 160s 26ms/step - loss: 10.7497 - acc: 0.3331 - val_loss: 10.7361 - val_acc: 0.3339\n",
      "Test score: 10.732150633973092\n",
      "Test accuracy: 0.334155161078238\n",
      "Stapelgroesse (batchSize): 8\n",
      "Anzahl der Faltungsschichten: 4Layer\n",
      "Aktivierungsfunktion: elu\n",
      "Optimierungsfunktion: Adam\n",
      "Dropout-Rate Faltungsschicht 1: 0.6376219441266817\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 64\n",
      "Dropout-Rate Faltungsschicht 2: 0.22525415691307785\n",
      "Anzahl der Faltungsschichten: 4Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 64\n",
      "Dropout-Rate Faltungsschicht 3: 0.204913954813469\n",
      "Anzahl der Filter-Maps Faltungsschicht 4: 128\n",
      "Dropout-Rate Faltungsschicht 4: 0.3863542731207837\n",
      "Anzahl der Neuronen des Fully Connected Layer: 128\n",
      "Dropout-Rate Fully Connected Layer: 0.019006025011648452\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "6084/6084 [==============================] - 155s 26ms/step - loss: 7.6646 - acc: 0.3365 - val_loss: 10.7626 - val_acc: 0.3323\n",
      "Epoch 2/10\n",
      "6084/6084 [==============================] - 152s 25ms/step - loss: 10.7371 - acc: 0.3339 - val_loss: 10.7586 - val_acc: 0.3325\n",
      "Epoch 3/10\n",
      "6084/6084 [==============================] - 152s 25ms/step - loss: 10.7361 - acc: 0.3339 - val_loss: 10.7613 - val_acc: 0.3323\n",
      "Epoch 4/10\n",
      "6084/6084 [==============================] - 152s 25ms/step - loss: 10.7361 - acc: 0.3339 - val_loss: 10.7613 - val_acc: 0.3323\n",
      "Epoch 5/10\n",
      "6084/6084 [==============================] - 152s 25ms/step - loss: 10.7357 - acc: 0.3339 - val_loss: 10.7640 - val_acc: 0.3322\n",
      "Epoch 6/10\n",
      "6084/6084 [==============================] - 152s 25ms/step - loss: 10.7367 - acc: 0.3339 - val_loss: 10.7600 - val_acc: 0.3324\n",
      "Epoch 7/10\n",
      "6084/6084 [==============================] - 152s 25ms/step - loss: 10.7361 - acc: 0.3339 - val_loss: 10.7600 - val_acc: 0.3324\n",
      "Epoch 8/10\n",
      "6084/6084 [==============================] - 152s 25ms/step - loss: 10.7367 - acc: 0.3339 - val_loss: 10.7600 - val_acc: 0.3324\n",
      "Epoch 9/10\n",
      "6084/6084 [==============================] - 152s 25ms/step - loss: 10.7361 - acc: 0.3339 - val_loss: 10.7613 - val_acc: 0.3323\n",
      "Epoch 10/10\n",
      "6084/6084 [==============================] - 152s 25ms/step - loss: 10.7361 - acc: 0.3339 - val_loss: 10.7600 - val_acc: 0.3324\n",
      "Test score: 10.762617119357117\n",
      "Test accuracy: 0.33226495726495725\n",
      "Stapelgroesse (batchSize): 32\n",
      "Anzahl der Faltungsschichten: 3Layer\n",
      "Aktivierungsfunktion: elu\n",
      "Optimierungsfunktion: Adam\n",
      "Dropout-Rate Faltungsschicht 1: 0.025863278157258317\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 64\n",
      "Dropout-Rate Faltungsschicht 2: 0.09769179708236805\n",
      "Anzahl der Faltungsschichten: 3Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 64\n",
      "Dropout-Rate Faltungsschicht 3: 0.34928374397389894\n",
      "Anzahl der Neuronen des Fully Connected Layer: 128\n",
      "Dropout-Rate Fully Connected Layer: 0.2827461472638423\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "1521/1521 [==============================] - 101s 67ms/step - loss: 1.0457 - acc: 0.4586 - val_loss: 0.9385 - val_acc: 0.5318\n",
      "Epoch 2/10\n",
      "1521/1521 [==============================] - 98s 65ms/step - loss: 0.9139 - acc: 0.5435 - val_loss: 0.8549 - val_acc: 0.5876\n",
      "Epoch 3/10\n",
      "1521/1521 [==============================] - 98s 65ms/step - loss: 0.8554 - acc: 0.5851 - val_loss: 0.8285 - val_acc: 0.6052\n",
      "Epoch 4/10\n",
      "1521/1521 [==============================] - 98s 64ms/step - loss: 0.8158 - acc: 0.6124 - val_loss: 0.8515 - val_acc: 0.5975\n",
      "Epoch 5/10\n",
      "1521/1521 [==============================] - 98s 64ms/step - loss: 0.7554 - acc: 0.6473 - val_loss: 0.8934 - val_acc: 0.5849\n",
      "Epoch 6/10\n",
      "1521/1521 [==============================] - 98s 64ms/step - loss: 0.6925 - acc: 0.6822 - val_loss: 0.9650 - val_acc: 0.5609\n",
      "Epoch 7/10\n",
      "1521/1521 [==============================] - 98s 65ms/step - loss: 0.6278 - acc: 0.7157 - val_loss: 1.0624 - val_acc: 0.5774\n",
      "Epoch 8/10\n",
      "1521/1521 [==============================] - 98s 64ms/step - loss: 0.5847 - acc: 0.7436 - val_loss: 1.1098 - val_acc: 0.5552\n",
      "Epoch 9/10\n",
      "1521/1521 [==============================] - 98s 64ms/step - loss: 0.5359 - acc: 0.7699 - val_loss: 1.1037 - val_acc: 0.5536\n",
      "Epoch 10/10\n",
      "1521/1521 [==============================] - 98s 64ms/step - loss: 0.4973 - acc: 0.7873 - val_loss: 1.2783 - val_acc: 0.5568\n",
      "Test score: 1.278306133025571\n",
      "Test accuracy: 0.5564144736842105\n",
      "Stapelgroesse (batchSize): 8\n",
      "Anzahl der Faltungsschichten: 3Layer\n",
      "Aktivierungsfunktion: relu\n",
      "Optimierungsfunktion: RMSprop\n",
      "Dropout-Rate Faltungsschicht 1: 0.34423045967715427\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 32\n",
      "Dropout-Rate Faltungsschicht 2: 0.5220785099718168\n",
      "Anzahl der Faltungsschichten: 3Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 128\n",
      "Dropout-Rate Faltungsschicht 3: 0.6787324686833386\n",
      "Anzahl der Neuronen des Fully Connected Layer: 128\n",
      "Dropout-Rate Fully Connected Layer: 0.07605034525389257\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "6084/6084 [==============================] - 159s 26ms/step - loss: 10.7502 - acc: 0.3329 - val_loss: 9.5530 - val_acc: 0.3336\n",
      "Epoch 2/10\n",
      "6084/6084 [==============================] - 156s 26ms/step - loss: 3.8370 - acc: 0.3999 - val_loss: 1.0619 - val_acc: 0.4286\n",
      "Epoch 3/10\n",
      "6084/6084 [==============================] - 156s 26ms/step - loss: 1.0516 - acc: 0.4450 - val_loss: 1.0702 - val_acc: 0.4328\n",
      "Epoch 4/10\n",
      "6084/6084 [==============================] - 156s 26ms/step - loss: 1.0474 - acc: 0.4495 - val_loss: 1.0613 - val_acc: 0.3848\n",
      "Epoch 5/10\n",
      "6084/6084 [==============================] - 155s 26ms/step - loss: 1.0419 - acc: 0.4587 - val_loss: 1.0994 - val_acc: 0.3351\n",
      "Epoch 6/10\n",
      "6084/6084 [==============================] - 155s 26ms/step - loss: 1.0482 - acc: 0.4636 - val_loss: 1.1160 - val_acc: 0.3334\n",
      "Epoch 7/10\n",
      "6084/6084 [==============================] - 155s 26ms/step - loss: 1.0406 - acc: 0.4610 - val_loss: 1.1133 - val_acc: 0.3337\n",
      "Epoch 8/10\n",
      "6084/6084 [==============================] - 155s 25ms/step - loss: 1.0400 - acc: 0.4591 - val_loss: 1.1087 - val_acc: 0.3337\n",
      "Epoch 9/10\n",
      "6084/6084 [==============================] - 155s 25ms/step - loss: 1.0417 - acc: 0.4611 - val_loss: 1.1144 - val_acc: 0.3338\n",
      "Epoch 10/10\n",
      "6084/6084 [==============================] - 155s 26ms/step - loss: 1.0440 - acc: 0.4619 - val_loss: 1.1234 - val_acc: 0.3342\n",
      "Test score: 1.1234502656768608\n",
      "Test accuracy: 0.33407297830374755\n",
      "Stapelgroesse (batchSize): 8\n",
      "Anzahl der Faltungsschichten: 3Layer\n",
      "Aktivierungsfunktion: elu\n",
      "Optimierungsfunktion: RMSprop\n",
      "Dropout-Rate Faltungsschicht 1: 0.574501349360165\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 32\n",
      "Dropout-Rate Faltungsschicht 2: 0.6881741885335925\n",
      "Anzahl der Faltungsschichten: 3Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 128\n",
      "Dropout-Rate Faltungsschicht 3: 0.13519528146110238\n",
      "Anzahl der Neuronen des Fully Connected Layer: 128\n",
      "Dropout-Rate Fully Connected Layer: 0.23283440746125775\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "6084/6084 [==============================] - 160s 26ms/step - loss: 10.7502 - acc: 0.3329 - val_loss: 10.7396 - val_acc: 0.3336\n",
      "Epoch 2/10\n",
      "6084/6084 [==============================] - 157s 26ms/step - loss: 10.7501 - acc: 0.3330 - val_loss: 10.7422 - val_acc: 0.3334\n",
      "Epoch 3/10\n",
      "6084/6084 [==============================] - 157s 26ms/step - loss: 10.7508 - acc: 0.3330 - val_loss: 10.7396 - val_acc: 0.3336\n",
      "Epoch 4/10\n",
      "6084/6084 [==============================] - 157s 26ms/step - loss: 10.7504 - acc: 0.3330 - val_loss: 10.7396 - val_acc: 0.3336\n",
      "Epoch 5/10\n",
      "6084/6084 [==============================] - 157s 26ms/step - loss: 10.7504 - acc: 0.3330 - val_loss: 10.7396 - val_acc: 0.3336\n",
      "Epoch 6/10\n",
      "6084/6084 [==============================] - 158s 26ms/step - loss: 10.7501 - acc: 0.3330 - val_loss: 10.7422 - val_acc: 0.3334\n",
      "Epoch 7/10\n",
      "6084/6084 [==============================] - 158s 26ms/step - loss: 10.7508 - acc: 0.3330 - val_loss: 10.7422 - val_acc: 0.3334\n",
      "Epoch 8/10\n",
      "6084/6084 [==============================] - 157s 26ms/step - loss: 10.7498 - acc: 0.3331 - val_loss: 10.7382 - val_acc: 0.3337\n",
      "Epoch 9/10\n",
      "6084/6084 [==============================] - 157s 26ms/step - loss: 10.7504 - acc: 0.3330 - val_loss: 10.7382 - val_acc: 0.3337\n",
      "Epoch 10/10\n",
      "6084/6084 [==============================] - 158s 26ms/step - loss: 10.7504 - acc: 0.3330 - val_loss: 10.7396 - val_acc: 0.3336\n",
      "Test score: 10.739573832956449\n",
      "Test accuracy: 0.33357988165680474\n",
      "Stapelgroesse (batchSize): 16\n",
      "Anzahl der Faltungsschichten: 4Layer\n",
      "Aktivierungsfunktion: elu\n",
      "Optimierungsfunktion: Adam\n",
      "Dropout-Rate Faltungsschicht 1: 0.47100000809439185\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 64\n",
      "Dropout-Rate Faltungsschicht 2: 0.27609439041357026\n",
      "Anzahl der Faltungsschichten: 4Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 64\n",
      "Dropout-Rate Faltungsschicht 3: 0.1983065174980258\n",
      "Anzahl der Filter-Maps Faltungsschicht 4: 64\n",
      "Dropout-Rate Faltungsschicht 4: 0.10491229892012294\n",
      "Anzahl der Neuronen des Fully Connected Layer: 64\n",
      "Dropout-Rate Fully Connected Layer: 0.6381767709036781\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "3042/3042 [==============================] - 117s 38ms/step - loss: 1.0519 - acc: 0.4439 - val_loss: 0.9926 - val_acc: 0.4602\n",
      "Epoch 2/10\n",
      "3042/3042 [==============================] - 114s 38ms/step - loss: 0.9846 - acc: 0.4930 - val_loss: 0.9212 - val_acc: 0.5169\n",
      "Epoch 3/10\n",
      "3042/3042 [==============================] - 114s 38ms/step - loss: 0.9421 - acc: 0.5257 - val_loss: 0.9049 - val_acc: 0.5108\n",
      "Epoch 4/10\n",
      "3042/3042 [==============================] - 114s 38ms/step - loss: 0.9312 - acc: 0.5349 - val_loss: 0.8897 - val_acc: 0.5403\n",
      "Epoch 5/10\n",
      "3042/3042 [==============================] - 114s 37ms/step - loss: 0.9353 - acc: 0.5291 - val_loss: 0.8962 - val_acc: 0.5750\n",
      "Epoch 6/10\n",
      "3042/3042 [==============================] - 114s 38ms/step - loss: 0.9384 - acc: 0.5230 - val_loss: 0.8723 - val_acc: 0.5662\n",
      "Epoch 7/10\n",
      "3042/3042 [==============================] - 114s 38ms/step - loss: 0.9414 - acc: 0.5199 - val_loss: 0.8963 - val_acc: 0.5786\n",
      "Epoch 8/10\n",
      "3042/3042 [==============================] - 114s 37ms/step - loss: 0.9448 - acc: 0.5208 - val_loss: 0.9381 - val_acc: 0.4908\n",
      "Epoch 9/10\n",
      "3042/3042 [==============================] - 114s 37ms/step - loss: 0.9506 - acc: 0.5099 - val_loss: 0.8764 - val_acc: 0.5876\n",
      "Epoch 10/10\n",
      "3042/3042 [==============================] - 114s 37ms/step - loss: 0.9567 - acc: 0.5060 - val_loss: 0.9291 - val_acc: 0.5346\n",
      "Test score: 0.929176218650843\n",
      "Test accuracy: 0.5344572368421052\n",
      "Stapelgroesse (batchSize): 32\n",
      "Anzahl der Faltungsschichten: 5Layer\n",
      "Aktivierungsfunktion: elu\n",
      "Optimierungsfunktion: RMSprop\n",
      "Dropout-Rate Faltungsschicht 1: 0.14159238265557203\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 32\n",
      "Dropout-Rate Faltungsschicht 2: 0.6135572308481826\n",
      "Anzahl der Faltungsschichten: 5Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 128\n",
      "Dropout-Rate Faltungsschicht 3: 0.5760079672729373\n",
      "Anzahl der Filter-Maps Faltungsschicht 4: 128\n",
      "Dropout-Rate Faltungsschicht 4: 0.696122414469801\n",
      "Anzahl der Filter-Maps Faltungsschicht 5: 128\n",
      "Dropout-Rate Faltungsschicht 5: 0.10918026514337834\n",
      "Anzahl der Neuronen des Fully Connected Layer: 128\n",
      "Dropout-Rate Fully Connected Layer: 0.6415234621121069\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "1521/1521 [==============================] - 98s 65ms/step - loss: 1.1134 - acc: 0.3988 - val_loss: 1.0344 - val_acc: 0.4458\n",
      "Epoch 2/10\n",
      "1521/1521 [==============================] - 95s 62ms/step - loss: 1.0395 - acc: 0.4471 - val_loss: 1.0270 - val_acc: 0.4223\n",
      "Epoch 3/10\n",
      "1521/1521 [==============================] - 95s 62ms/step - loss: 1.0292 - acc: 0.4579 - val_loss: 0.9838 - val_acc: 0.4764\n",
      "Epoch 4/10\n",
      "1521/1521 [==============================] - 95s 62ms/step - loss: 1.0210 - acc: 0.4665 - val_loss: 1.0263 - val_acc: 0.4530\n",
      "Epoch 5/10\n",
      "1521/1521 [==============================] - 95s 62ms/step - loss: 1.0073 - acc: 0.4771 - val_loss: 1.0095 - val_acc: 0.4706\n",
      "Epoch 6/10\n",
      "1521/1521 [==============================] - 95s 62ms/step - loss: 0.9864 - acc: 0.4981 - val_loss: 0.9718 - val_acc: 0.5476\n",
      "Epoch 7/10\n",
      "1521/1521 [==============================] - 95s 62ms/step - loss: 0.9783 - acc: 0.5043 - val_loss: 0.9728 - val_acc: 0.5130\n",
      "Epoch 8/10\n",
      "1521/1521 [==============================] - 95s 62ms/step - loss: 0.9731 - acc: 0.5097 - val_loss: 0.8627 - val_acc: 0.5858\n",
      "Epoch 9/10\n",
      "1521/1521 [==============================] - 95s 62ms/step - loss: 0.9692 - acc: 0.5118 - val_loss: 0.8944 - val_acc: 0.5589\n",
      "Epoch 10/10\n",
      "1521/1521 [==============================] - 95s 62ms/step - loss: 0.9713 - acc: 0.5100 - val_loss: 0.8540 - val_acc: 0.5798\n",
      "Test score: 0.8541821313531776\n",
      "Test accuracy: 0.5799342105263158\n",
      "Stapelgroesse (batchSize): 32\n",
      "Anzahl der Faltungsschichten: 5Layer\n",
      "Aktivierungsfunktion: elu\n",
      "Optimierungsfunktion: RMSprop\n",
      "Dropout-Rate Faltungsschicht 1: 0.42664357921515855\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 64\n",
      "Dropout-Rate Faltungsschicht 2: 0.35339644344344656\n",
      "Anzahl der Faltungsschichten: 5Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 64\n",
      "Dropout-Rate Faltungsschicht 3: 0.4514951691589727\n",
      "Anzahl der Filter-Maps Faltungsschicht 4: 128\n",
      "Dropout-Rate Faltungsschicht 4: 0.5503352154343591\n",
      "Anzahl der Filter-Maps Faltungsschicht 5: 128\n",
      "Dropout-Rate Faltungsschicht 5: 0.038754255841003525\n",
      "Anzahl der Neuronen des Fully Connected Layer: 128\n",
      "Dropout-Rate Fully Connected Layer: 0.015249081192082901\n",
      "Faltungsnetz wird trainiert...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1521/1521 [==============================] - 105s 69ms/step - loss: 1.1180 - acc: 0.4117 - val_loss: 1.0364 - val_acc: 0.4278\n",
      "Epoch 2/10\n",
      "1521/1521 [==============================] - 102s 67ms/step - loss: 1.0030 - acc: 0.4795 - val_loss: 0.9917 - val_acc: 0.4712\n",
      "Epoch 3/10\n",
      "1521/1521 [==============================] - 101s 67ms/step - loss: 0.9516 - acc: 0.5236 - val_loss: 1.3049 - val_acc: 0.3628\n",
      "Epoch 4/10\n",
      "1521/1521 [==============================] - 101s 67ms/step - loss: 0.9230 - acc: 0.5412 - val_loss: 0.8984 - val_acc: 0.5400\n",
      "Epoch 5/10\n",
      "1521/1521 [==============================] - 101s 67ms/step - loss: 0.9073 - acc: 0.5518 - val_loss: 0.8548 - val_acc: 0.5809\n",
      "Epoch 6/10\n",
      "1521/1521 [==============================] - 101s 67ms/step - loss: 0.8937 - acc: 0.5643 - val_loss: 0.9704 - val_acc: 0.4938\n",
      "Epoch 7/10\n",
      "1521/1521 [==============================] - 102s 67ms/step - loss: 0.8812 - acc: 0.5700 - val_loss: 0.8990 - val_acc: 0.5770\n",
      "Epoch 8/10\n",
      "1521/1521 [==============================] - 101s 67ms/step - loss: 0.8747 - acc: 0.5759 - val_loss: 0.9366 - val_acc: 0.5380\n",
      "Epoch 9/10\n",
      "1521/1521 [==============================] - 101s 67ms/step - loss: 0.8709 - acc: 0.5762 - val_loss: 0.9932 - val_acc: 0.5007\n",
      "Epoch 10/10\n",
      "1521/1521 [==============================] - 101s 67ms/step - loss: 0.8680 - acc: 0.5782 - val_loss: 0.8353 - val_acc: 0.6123\n",
      "Test score: 0.8353744627613771\n",
      "Test accuracy: 0.6124177631578948\n",
      "Stapelgroesse (batchSize): 16\n",
      "Anzahl der Faltungsschichten: 3Layer\n",
      "Aktivierungsfunktion: relu\n",
      "Optimierungsfunktion: Adam\n",
      "Dropout-Rate Faltungsschicht 1: 0.2213089568208069\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 64\n",
      "Dropout-Rate Faltungsschicht 2: 0.23756832394413926\n",
      "Anzahl der Faltungsschichten: 3Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 128\n",
      "Dropout-Rate Faltungsschicht 3: 0.49086307725000733\n",
      "Anzahl der Neuronen des Fully Connected Layer: 128\n",
      "Dropout-Rate Fully Connected Layer: 0.32436239488469354\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "3042/3042 [==============================] - 136s 45ms/step - loss: 1.1017 - acc: 0.3342 - val_loss: 1.0989 - val_acc: 0.3323\n",
      "Epoch 2/10\n",
      "3042/3042 [==============================] - 133s 44ms/step - loss: 1.0866 - acc: 0.3618 - val_loss: 1.0147 - val_acc: 0.4747\n",
      "Epoch 3/10\n",
      "3042/3042 [==============================] - 133s 44ms/step - loss: 1.0090 - acc: 0.4798 - val_loss: 0.9582 - val_acc: 0.5174\n",
      "Epoch 4/10\n",
      "3042/3042 [==============================] - 133s 44ms/step - loss: 0.9520 - acc: 0.5268 - val_loss: 0.9485 - val_acc: 0.5324\n",
      "Epoch 5/10\n",
      "3042/3042 [==============================] - 133s 44ms/step - loss: 0.9266 - acc: 0.5427 - val_loss: 0.9728 - val_acc: 0.5186\n",
      "Epoch 6/10\n",
      "3042/3042 [==============================] - 133s 44ms/step - loss: 0.9106 - acc: 0.5509 - val_loss: 1.0108 - val_acc: 0.5059\n",
      "Epoch 7/10\n",
      "3042/3042 [==============================] - 133s 44ms/step - loss: 0.8987 - acc: 0.5610 - val_loss: 0.9845 - val_acc: 0.5097\n",
      "Epoch 8/10\n",
      "3042/3042 [==============================] - 133s 44ms/step - loss: 0.8871 - acc: 0.5667 - val_loss: 0.9495 - val_acc: 0.5352\n",
      "Epoch 9/10\n",
      "3042/3042 [==============================] - 133s 44ms/step - loss: 0.8808 - acc: 0.5721 - val_loss: 0.9674 - val_acc: 0.5190\n",
      "Epoch 10/10\n",
      "3042/3042 [==============================] - 133s 44ms/step - loss: 0.8732 - acc: 0.5770 - val_loss: 1.0028 - val_acc: 0.5027\n",
      "Test score: 1.0028241872003203\n",
      "Test accuracy: 0.5029605263157895\n",
      "Stapelgroesse (batchSize): 32\n",
      "Anzahl der Faltungsschichten: 4Layer\n",
      "Aktivierungsfunktion: relu\n",
      "Optimierungsfunktion: Adam\n",
      "Dropout-Rate Faltungsschicht 1: 0.5333709865046569\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 32\n",
      "Dropout-Rate Faltungsschicht 2: 0.018979800917943133\n",
      "Anzahl der Faltungsschichten: 4Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 128\n",
      "Dropout-Rate Faltungsschicht 3: 0.21477056697604296\n",
      "Anzahl der Filter-Maps Faltungsschicht 4: 128\n",
      "Dropout-Rate Faltungsschicht 4: 0.412194001775003\n",
      "Anzahl der Neuronen des Fully Connected Layer: 128\n",
      "Dropout-Rate Fully Connected Layer: 0.42627434890842325\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "1521/1521 [==============================] - 96s 63ms/step - loss: 1.0996 - acc: 0.3350 - val_loss: 1.0987 - val_acc: 0.3323\n",
      "Epoch 2/10\n",
      "1521/1521 [==============================] - 92s 60ms/step - loss: 1.0987 - acc: 0.3324 - val_loss: 1.0987 - val_acc: 0.3325\n",
      "Epoch 3/10\n",
      "1521/1521 [==============================] - 91s 60ms/step - loss: 1.0987 - acc: 0.3343 - val_loss: 1.0987 - val_acc: 0.3325\n",
      "Epoch 4/10\n",
      "1521/1521 [==============================] - 91s 60ms/step - loss: 1.0987 - acc: 0.3334 - val_loss: 1.0987 - val_acc: 0.3325\n",
      "Epoch 5/10\n",
      "1521/1521 [==============================] - 91s 60ms/step - loss: 1.0987 - acc: 0.3332 - val_loss: 1.0987 - val_acc: 0.3325\n",
      "Epoch 6/10\n",
      "1521/1521 [==============================] - 91s 60ms/step - loss: 1.0987 - acc: 0.3333 - val_loss: 1.0987 - val_acc: 0.3325\n",
      "Epoch 7/10\n",
      "1521/1521 [==============================] - 92s 60ms/step - loss: 1.0987 - acc: 0.3328 - val_loss: 1.0987 - val_acc: 0.3325\n",
      "Epoch 8/10\n",
      "1521/1521 [==============================] - 91s 60ms/step - loss: 1.0987 - acc: 0.3326 - val_loss: 1.0987 - val_acc: 0.3325\n",
      "Epoch 9/10\n",
      "1521/1521 [==============================] - 91s 60ms/step - loss: 1.0987 - acc: 0.3325 - val_loss: 1.0987 - val_acc: 0.3325\n",
      "Epoch 10/10\n",
      "1521/1521 [==============================] - 91s 60ms/step - loss: 1.0987 - acc: 0.3325 - val_loss: 1.0987 - val_acc: 0.3325\n",
      "Test score: 1.0986950698651765\n",
      "Test accuracy: 0.3323190789473684\n",
      "Stapelgroesse (batchSize): 8\n",
      "Anzahl der Faltungsschichten: 3Layer\n",
      "Aktivierungsfunktion: elu\n",
      "Optimierungsfunktion: RMSprop\n",
      "Dropout-Rate Faltungsschicht 1: 0.359044909299503\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 64\n",
      "Dropout-Rate Faltungsschicht 2: 0.024657376746418068\n",
      "Anzahl der Faltungsschichten: 3Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 128\n",
      "Dropout-Rate Faltungsschicht 3: 0.5453216256406395\n",
      "Anzahl der Neuronen des Fully Connected Layer: 64\n",
      "Dropout-Rate Fully Connected Layer: 0.6560403219765505\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "6084/6084 [==============================] - 161s 26ms/step - loss: 1.2559 - acc: 0.4127 - val_loss: 0.9715 - val_acc: 0.4942\n",
      "Epoch 2/10\n",
      "6084/6084 [==============================] - 158s 26ms/step - loss: 1.0382 - acc: 0.4792 - val_loss: 1.0532 - val_acc: 0.4778\n",
      "Epoch 3/10\n",
      "6084/6084 [==============================] - 157s 26ms/step - loss: 1.0389 - acc: 0.4841 - val_loss: 0.9375 - val_acc: 0.5030\n",
      "Epoch 4/10\n",
      "6084/6084 [==============================] - 157s 26ms/step - loss: 1.0401 - acc: 0.4797 - val_loss: 0.9060 - val_acc: 0.5308\n",
      "Epoch 5/10\n",
      "6084/6084 [==============================] - 157s 26ms/step - loss: 1.0527 - acc: 0.4777 - val_loss: 1.0554 - val_acc: 0.4771\n",
      "Epoch 6/10\n",
      "6084/6084 [==============================] - 157s 26ms/step - loss: 1.0575 - acc: 0.4788 - val_loss: 0.9768 - val_acc: 0.4966\n",
      "Epoch 7/10\n",
      "6084/6084 [==============================] - 158s 26ms/step - loss: 1.0620 - acc: 0.4715 - val_loss: 0.9467 - val_acc: 0.4738\n",
      "Epoch 8/10\n",
      "6084/6084 [==============================] - 157s 26ms/step - loss: 1.0688 - acc: 0.4700 - val_loss: 0.9503 - val_acc: 0.5670\n",
      "Epoch 9/10\n",
      "6084/6084 [==============================] - 157s 26ms/step - loss: 1.0776 - acc: 0.4652 - val_loss: 0.9733 - val_acc: 0.5221\n",
      "Epoch 10/10\n",
      "6084/6084 [==============================] - 157s 26ms/step - loss: 1.0868 - acc: 0.4648 - val_loss: 0.9295 - val_acc: 0.5016\n",
      "Test score: 0.9294394445920915\n",
      "Test accuracy: 0.5014792899408284\n",
      "Stapelgroesse (batchSize): 8\n",
      "Anzahl der Faltungsschichten: 5Layer\n",
      "Aktivierungsfunktion: elu\n",
      "Optimierungsfunktion: RMSprop\n",
      "Dropout-Rate Faltungsschicht 1: 0.25538765350136594\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 32\n",
      "Dropout-Rate Faltungsschicht 2: 0.6620047902547548\n",
      "Anzahl der Faltungsschichten: 5Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 64\n",
      "Dropout-Rate Faltungsschicht 3: 0.08908946831009872\n",
      "Anzahl der Filter-Maps Faltungsschicht 4: 128\n",
      "Dropout-Rate Faltungsschicht 4: 0.5913692508775148\n",
      "Anzahl der Filter-Maps Faltungsschicht 5: 128\n",
      "Dropout-Rate Faltungsschicht 5: 0.4335169964924364\n",
      "Anzahl der Neuronen des Fully Connected Layer: 64\n",
      "Dropout-Rate Fully Connected Layer: 0.6689639674061256\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6084/6084 [==============================] - 145s 24ms/step - loss: 1.1177 - acc: 0.3915 - val_loss: 1.0376 - val_acc: 0.4553\n",
      "Epoch 2/10\n",
      "6084/6084 [==============================] - 141s 23ms/step - loss: 1.1067 - acc: 0.3991 - val_loss: 1.0639 - val_acc: 0.4436\n",
      "Epoch 3/10\n",
      "6084/6084 [==============================] - 141s 23ms/step - loss: 1.1257 - acc: 0.3702 - val_loss: 2.4993 - val_acc: 0.3919\n",
      "Epoch 4/10\n",
      "6084/6084 [==============================] - 140s 23ms/step - loss: 1.1281 - acc: 0.3411 - val_loss: 1.1033 - val_acc: 0.3704\n",
      "Epoch 5/10\n",
      "6084/6084 [==============================] - 141s 23ms/step - loss: 1.1173 - acc: 0.3331 - val_loss: 1.1090 - val_acc: 0.3322\n",
      "Epoch 6/10\n",
      "6084/6084 [==============================] - 141s 23ms/step - loss: 1.1137 - acc: 0.3336 - val_loss: 1.1247 - val_acc: 0.3308\n",
      "Epoch 7/10\n",
      "6084/6084 [==============================] - 141s 23ms/step - loss: 1.1144 - acc: 0.3309 - val_loss: 1.1009 - val_acc: 0.3323\n",
      "Epoch 8/10\n",
      "6084/6084 [==============================] - 141s 23ms/step - loss: 1.1136 - acc: 0.3303 - val_loss: 1.0997 - val_acc: 0.3324\n",
      "Epoch 9/10\n",
      "6084/6084 [==============================] - 141s 23ms/step - loss: 1.1139 - acc: 0.3325 - val_loss: 1.2360 - val_acc: 0.2906\n",
      "Epoch 10/10\n",
      "6084/6084 [==============================] - 141s 23ms/step - loss: 1.1134 - acc: 0.3326 - val_loss: 1.2763 - val_acc: 0.3666\n",
      "Test score: 1.276227374636445\n",
      "Test accuracy: 0.36653517422748194\n",
      "Stapelgroesse (batchSize): 16\n",
      "Anzahl der Faltungsschichten: 5Layer\n",
      "Aktivierungsfunktion: elu\n",
      "Optimierungsfunktion: Adam\n",
      "Dropout-Rate Faltungsschicht 1: 0.10708794110871896\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 32\n",
      "Dropout-Rate Faltungsschicht 2: 0.16857689755736183\n",
      "Anzahl der Faltungsschichten: 5Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 64\n",
      "Dropout-Rate Faltungsschicht 3: 0.5916376099174318\n",
      "Anzahl der Filter-Maps Faltungsschicht 4: 64\n",
      "Dropout-Rate Faltungsschicht 4: 0.36718406309428453\n",
      "Anzahl der Filter-Maps Faltungsschicht 5: 128\n",
      "Dropout-Rate Faltungsschicht 5: 0.4829262941054116\n",
      "Anzahl der Neuronen des Fully Connected Layer: 64\n",
      "Dropout-Rate Fully Connected Layer: 0.02407284845771257\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "3042/3042 [==============================] - 110s 36ms/step - loss: 1.0546 - acc: 0.4333 - val_loss: 1.0122 - val_acc: 0.4558\n",
      "Epoch 2/10\n",
      "3042/3042 [==============================] - 106s 35ms/step - loss: 0.9970 - acc: 0.4758 - val_loss: 0.8915 - val_acc: 0.5607\n",
      "Epoch 3/10\n",
      "3042/3042 [==============================] - 106s 35ms/step - loss: 0.9121 - acc: 0.5472 - val_loss: 0.8934 - val_acc: 0.5488\n",
      "Epoch 4/10\n",
      "3042/3042 [==============================] - 106s 35ms/step - loss: 0.8821 - acc: 0.5663 - val_loss: 0.8371 - val_acc: 0.5945\n",
      "Epoch 5/10\n",
      "3042/3042 [==============================] - 106s 35ms/step - loss: 0.8712 - acc: 0.5720 - val_loss: 0.8333 - val_acc: 0.5995\n",
      "Epoch 6/10\n",
      "3042/3042 [==============================] - 106s 35ms/step - loss: 0.8662 - acc: 0.5762 - val_loss: 0.8402 - val_acc: 0.5779\n",
      "Epoch 7/10\n",
      "3042/3042 [==============================] - 106s 35ms/step - loss: 0.8620 - acc: 0.5779 - val_loss: 1.0499 - val_acc: 0.5611\n",
      "Epoch 8/10\n",
      "3042/3042 [==============================] - 106s 35ms/step - loss: 0.8560 - acc: 0.5812 - val_loss: 0.8624 - val_acc: 0.5825\n",
      "Epoch 9/10\n",
      "3042/3042 [==============================] - 106s 35ms/step - loss: 0.8581 - acc: 0.5789 - val_loss: 0.8670 - val_acc: 0.6075\n",
      "Epoch 10/10\n",
      "3042/3042 [==============================] - 106s 35ms/step - loss: 0.8531 - acc: 0.5840 - val_loss: 0.8675 - val_acc: 0.5939\n",
      "Test score: 0.867403960071112\n",
      "Test accuracy: 0.5938322368421053\n",
      "Stapelgroesse (batchSize): 16\n",
      "Anzahl der Faltungsschichten: 5Layer\n",
      "Aktivierungsfunktion: elu\n",
      "Optimierungsfunktion: RMSprop\n",
      "Dropout-Rate Faltungsschicht 1: 0.21698475914963355\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 64\n",
      "Dropout-Rate Faltungsschicht 2: 0.17289511557392406\n",
      "Anzahl der Faltungsschichten: 5Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 64\n",
      "Dropout-Rate Faltungsschicht 3: 0.09436418717566777\n",
      "Anzahl der Filter-Maps Faltungsschicht 4: 64\n",
      "Dropout-Rate Faltungsschicht 4: 0.20311099523053433\n",
      "Anzahl der Filter-Maps Faltungsschicht 5: 64\n",
      "Dropout-Rate Faltungsschicht 5: 0.271046632669442\n",
      "Anzahl der Neuronen des Fully Connected Layer: 128\n",
      "Dropout-Rate Fully Connected Layer: 0.42707347385220085\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "3042/3042 [==============================] - 122s 40ms/step - loss: 1.0504 - acc: 0.4441 - val_loss: 0.9306 - val_acc: 0.5327\n",
      "Epoch 2/10\n",
      "3042/3042 [==============================] - 118s 39ms/step - loss: 0.9274 - acc: 0.5397 - val_loss: 0.8552 - val_acc: 0.5885\n",
      "Epoch 3/10\n",
      "3042/3042 [==============================] - 118s 39ms/step - loss: 0.9043 - acc: 0.5558 - val_loss: 0.8988 - val_acc: 0.5255\n",
      "Epoch 4/10\n",
      "3042/3042 [==============================] - 118s 39ms/step - loss: 0.9021 - acc: 0.5576 - val_loss: 0.8434 - val_acc: 0.5959\n",
      "Epoch 5/10\n",
      "3042/3042 [==============================] - 118s 39ms/step - loss: 0.9034 - acc: 0.5603 - val_loss: 0.8856 - val_acc: 0.5222\n",
      "Epoch 6/10\n",
      "3042/3042 [==============================] - 118s 39ms/step - loss: 0.9069 - acc: 0.5543 - val_loss: 0.9279 - val_acc: 0.5457\n",
      "Epoch 7/10\n",
      "3042/3042 [==============================] - 118s 39ms/step - loss: 0.9089 - acc: 0.5531 - val_loss: 0.8454 - val_acc: 0.6076\n",
      "Epoch 8/10\n",
      "3042/3042 [==============================] - 118s 39ms/step - loss: 0.9166 - acc: 0.5502 - val_loss: 0.8778 - val_acc: 0.5871\n",
      "Epoch 9/10\n",
      "3042/3042 [==============================] - 118s 39ms/step - loss: 0.9221 - acc: 0.5475 - val_loss: 0.8903 - val_acc: 0.5716\n",
      "Epoch 10/10\n",
      "3042/3042 [==============================] - 118s 39ms/step - loss: 0.9310 - acc: 0.5409 - val_loss: 0.8718 - val_acc: 0.5565\n",
      "Test score: 0.8720221004203746\n",
      "Test accuracy: 0.5563322368421053\n",
      "Stapelgroesse (batchSize): 8\n",
      "Anzahl der Faltungsschichten: 5Layer\n",
      "Aktivierungsfunktion: relu\n",
      "Optimierungsfunktion: Adam\n",
      "Dropout-Rate Faltungsschicht 1: 0.1258157021033706\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 32\n",
      "Dropout-Rate Faltungsschicht 2: 0.13527286629130778\n",
      "Anzahl der Faltungsschichten: 5Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 64\n",
      "Dropout-Rate Faltungsschicht 3: 0.6382370746797458\n",
      "Anzahl der Filter-Maps Faltungsschicht 4: 64\n",
      "Dropout-Rate Faltungsschicht 4: 0.050726060935707945\n",
      "Anzahl der Filter-Maps Faltungsschicht 5: 64\n",
      "Dropout-Rate Faltungsschicht 5: 0.6740138303454847\n",
      "Anzahl der Neuronen des Fully Connected Layer: 128\n",
      "Dropout-Rate Fully Connected Layer: 0.187900543570194\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "6084/6084 [==============================] - 143s 23ms/step - loss: 1.0991 - acc: 0.3327 - val_loss: 1.0988 - val_acc: 0.3323\n",
      "Epoch 2/10\n",
      "6084/6084 [==============================] - 139s 23ms/step - loss: 1.0988 - acc: 0.3337 - val_loss: 1.0989 - val_acc: 0.3323\n",
      "Epoch 3/10\n",
      "6084/6084 [==============================] - 139s 23ms/step - loss: 1.0988 - acc: 0.3313 - val_loss: 1.0988 - val_acc: 0.3325\n",
      "Epoch 4/10\n",
      "6084/6084 [==============================] - 139s 23ms/step - loss: 1.0988 - acc: 0.3321 - val_loss: 1.0988 - val_acc: 0.3323\n",
      "Epoch 5/10\n",
      "6084/6084 [==============================] - 139s 23ms/step - loss: 1.0988 - acc: 0.3321 - val_loss: 1.0988 - val_acc: 0.3323\n",
      "Epoch 6/10\n",
      "6084/6084 [==============================] - 139s 23ms/step - loss: 1.0988 - acc: 0.3321 - val_loss: 1.0988 - val_acc: 0.3322\n",
      "Epoch 7/10\n",
      "6084/6084 [==============================] - 139s 23ms/step - loss: 1.0988 - acc: 0.3321 - val_loss: 1.0988 - val_acc: 0.3324\n",
      "Epoch 8/10\n",
      "6084/6084 [==============================] - 139s 23ms/step - loss: 1.0988 - acc: 0.3321 - val_loss: 1.0988 - val_acc: 0.3324\n",
      "Epoch 9/10\n",
      "6084/6084 [==============================] - 139s 23ms/step - loss: 1.0988 - acc: 0.3321 - val_loss: 1.0988 - val_acc: 0.3324\n",
      "Epoch 10/10\n",
      "6084/6084 [==============================] - 139s 23ms/step - loss: 1.0988 - acc: 0.3321 - val_loss: 1.0988 - val_acc: 0.3323\n",
      "Test score: 1.0988241953727527\n",
      "Test accuracy: 0.33226495726495725\n",
      "Stapelgroesse (batchSize): 16\n",
      "Anzahl der Faltungsschichten: 3Layer\n",
      "Aktivierungsfunktion: relu\n",
      "Optimierungsfunktion: RMSprop\n",
      "Dropout-Rate Faltungsschicht 1: 0.4951734864524238\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 32\n",
      "Dropout-Rate Faltungsschicht 2: 0.578122650697853\n",
      "Anzahl der Faltungsschichten: 3Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 64\n",
      "Dropout-Rate Faltungsschicht 3: 0.3900306299971166\n",
      "Anzahl der Neuronen des Fully Connected Layer: 64\n",
      "Dropout-Rate Fully Connected Layer: 0.3955417566913732\n",
      "Faltungsnetz wird trainiert...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3042/3042 [==============================] - 102s 33ms/step - loss: 1.0599 - acc: 0.4339 - val_loss: 1.0732 - val_acc: 0.3743\n",
      "Epoch 2/10\n",
      "3042/3042 [==============================] - 98s 32ms/step - loss: 1.0267 - acc: 0.4659 - val_loss: 1.0825 - val_acc: 0.3810\n",
      "Epoch 3/10\n",
      "3042/3042 [==============================] - 98s 32ms/step - loss: 0.9989 - acc: 0.4898 - val_loss: 1.1141 - val_acc: 0.3624\n",
      "Epoch 4/10\n",
      "3042/3042 [==============================] - 98s 32ms/step - loss: 0.9901 - acc: 0.4956 - val_loss: 1.1093 - val_acc: 0.3666\n",
      "Epoch 5/10\n",
      "3042/3042 [==============================] - 97s 32ms/step - loss: 0.9875 - acc: 0.5022 - val_loss: 1.1372 - val_acc: 0.3451\n",
      "Epoch 6/10\n",
      "3042/3042 [==============================] - 98s 32ms/step - loss: 0.9865 - acc: 0.5029 - val_loss: 1.1672 - val_acc: 0.3369\n",
      "Epoch 7/10\n",
      "3042/3042 [==============================] - 98s 32ms/step - loss: 0.9909 - acc: 0.5002 - val_loss: 1.1061 - val_acc: 0.3627\n",
      "Epoch 8/10\n",
      "3042/3042 [==============================] - 97s 32ms/step - loss: 0.9937 - acc: 0.4985 - val_loss: 1.1506 - val_acc: 0.3338\n",
      "Epoch 9/10\n",
      "3042/3042 [==============================] - 97s 32ms/step - loss: 0.9866 - acc: 0.5017 - val_loss: 1.1169 - val_acc: 0.3626\n",
      "Epoch 10/10\n",
      "3042/3042 [==============================] - 98s 32ms/step - loss: 0.9869 - acc: 0.5021 - val_loss: 1.0783 - val_acc: 0.3832\n",
      "Test score: 1.078309458729468\n",
      "Test accuracy: 0.38330592105263156\n",
      "Stapelgroesse (batchSize): 8\n",
      "Anzahl der Faltungsschichten: 3Layer\n",
      "Aktivierungsfunktion: relu\n",
      "Optimierungsfunktion: Adam\n",
      "Dropout-Rate Faltungsschicht 1: 0.13222469701785414\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 64\n",
      "Dropout-Rate Faltungsschicht 2: 0.46155967454661834\n",
      "Anzahl der Faltungsschichten: 3Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 64\n",
      "Dropout-Rate Faltungsschicht 3: 0.36375913271863597\n",
      "Anzahl der Neuronen des Fully Connected Layer: 64\n",
      "Dropout-Rate Fully Connected Layer: 0.5908070034329119\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "6084/6084 [==============================] - 146s 24ms/step - loss: 1.1004 - acc: 0.3330 - val_loss: 1.0988 - val_acc: 0.3323\n",
      "Epoch 2/10\n",
      "6084/6084 [==============================] - 142s 23ms/step - loss: 1.0988 - acc: 0.3318 - val_loss: 1.0988 - val_acc: 0.3323\n",
      "Epoch 3/10\n",
      "6084/6084 [==============================] - 142s 23ms/step - loss: 1.0988 - acc: 0.3321 - val_loss: 1.0988 - val_acc: 0.3323\n",
      "Epoch 4/10\n",
      "6084/6084 [==============================] - 142s 23ms/step - loss: 1.0988 - acc: 0.3321 - val_loss: 1.0988 - val_acc: 0.3323\n",
      "Epoch 5/10\n",
      "6084/6084 [==============================] - 142s 23ms/step - loss: 1.0988 - acc: 0.3321 - val_loss: 1.0988 - val_acc: 0.3323\n",
      "Epoch 6/10\n",
      "6084/6084 [==============================] - 142s 23ms/step - loss: 1.0988 - acc: 0.3321 - val_loss: 1.0988 - val_acc: 0.3322\n",
      "Epoch 7/10\n",
      "6084/6084 [==============================] - 142s 23ms/step - loss: 1.0988 - acc: 0.3321 - val_loss: 1.0988 - val_acc: 0.3324\n",
      "Epoch 8/10\n",
      "6084/6084 [==============================] - 142s 23ms/step - loss: 1.0988 - acc: 0.3321 - val_loss: 1.0988 - val_acc: 0.3324\n",
      "Epoch 9/10\n",
      "6084/6084 [==============================] - 142s 23ms/step - loss: 1.0988 - acc: 0.3321 - val_loss: 1.0988 - val_acc: 0.3323\n",
      "Epoch 10/10\n",
      "6084/6084 [==============================] - 142s 23ms/step - loss: 1.0988 - acc: 0.3321 - val_loss: 1.0988 - val_acc: 0.3323\n",
      "Test score: 1.0988241953727527\n",
      "Test accuracy: 0.33226495726495725\n",
      "Stapelgroesse (batchSize): 8\n",
      "Anzahl der Faltungsschichten: 5Layer\n",
      "Aktivierungsfunktion: elu\n",
      "Optimierungsfunktion: RMSprop\n",
      "Dropout-Rate Faltungsschicht 1: 0.6792231737343013\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 64\n",
      "Dropout-Rate Faltungsschicht 2: 0.3093562096369121\n",
      "Anzahl der Faltungsschichten: 5Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 64\n",
      "Dropout-Rate Faltungsschicht 3: 0.4315036240930641\n",
      "Anzahl der Filter-Maps Faltungsschicht 4: 64\n",
      "Dropout-Rate Faltungsschicht 4: 0.4751575788425747\n",
      "Anzahl der Filter-Maps Faltungsschicht 5: 64\n",
      "Dropout-Rate Faltungsschicht 5: 0.06147997220119776\n",
      "Anzahl der Neuronen des Fully Connected Layer: 64\n",
      "Dropout-Rate Fully Connected Layer: 0.5038170608472976\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "6084/6084 [==============================] - 156s 26ms/step - loss: 1.0960 - acc: 0.4082 - val_loss: 1.0286 - val_acc: 0.4346\n",
      "Epoch 2/10\n",
      "6084/6084 [==============================] - 152s 25ms/step - loss: 1.0660 - acc: 0.4304 - val_loss: 1.0189 - val_acc: 0.4356\n",
      "Epoch 3/10\n",
      "6084/6084 [==============================] - 152s 25ms/step - loss: 1.0692 - acc: 0.4284 - val_loss: 1.0743 - val_acc: 0.4688\n",
      "Epoch 4/10\n",
      "6084/6084 [==============================] - 151s 25ms/step - loss: 1.0711 - acc: 0.4312 - val_loss: 6.5568 - val_acc: 0.3390\n",
      "Epoch 5/10\n",
      "6084/6084 [==============================] - 152s 25ms/step - loss: 1.0784 - acc: 0.4268 - val_loss: 7.0282 - val_acc: 0.3897\n",
      "Epoch 6/10\n",
      "6084/6084 [==============================] - 224s 37ms/step - loss: 1.0879 - acc: 0.4236 - val_loss: 7.6749 - val_acc: 0.3881\n",
      "Epoch 7/10\n",
      "6084/6084 [==============================] - 160s 26ms/step - loss: 6.2796 - acc: 0.3695 - val_loss: 10.7640 - val_acc: 0.3322\n",
      "Epoch 8/10\n",
      "6084/6084 [==============================] - 151s 25ms/step - loss: 10.7371 - acc: 0.3339 - val_loss: 10.7600 - val_acc: 0.3324\n",
      "Epoch 9/10\n",
      "6084/6084 [==============================] - 153s 25ms/step - loss: 10.7367 - acc: 0.3339 - val_loss: 10.7613 - val_acc: 0.3323\n",
      "Epoch 10/10\n",
      "6084/6084 [==============================] - 154s 25ms/step - loss: 10.7377 - acc: 0.3338 - val_loss: 10.7600 - val_acc: 0.3324\n",
      "Test score: 10.762617119357117\n",
      "Test accuracy: 0.33226495726495725\n",
      "Stapelgroesse (batchSize): 8\n",
      "Anzahl der Faltungsschichten: 5Layer\n",
      "Aktivierungsfunktion: relu\n",
      "Optimierungsfunktion: Adam\n",
      "Dropout-Rate Faltungsschicht 1: 0.30708821143141685\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 32\n",
      "Dropout-Rate Faltungsschicht 2: 0.425181051865627\n",
      "Anzahl der Faltungsschichten: 5Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 64\n",
      "Dropout-Rate Faltungsschicht 3: 0.14062459552915882\n",
      "Anzahl der Filter-Maps Faltungsschicht 4: 128\n",
      "Dropout-Rate Faltungsschicht 4: 0.590648620829584\n",
      "Anzahl der Filter-Maps Faltungsschicht 5: 64\n",
      "Dropout-Rate Faltungsschicht 5: 0.33525070377372224\n",
      "Anzahl der Neuronen des Fully Connected Layer: 128\n",
      "Dropout-Rate Fully Connected Layer: 0.46580928432226254\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "6084/6084 [==============================] - 150s 25ms/step - loss: 1.0994 - acc: 0.3339 - val_loss: 1.0988 - val_acc: 0.3323\n",
      "Epoch 2/10\n",
      "6084/6084 [==============================] - 146s 24ms/step - loss: 1.0989 - acc: 0.3327 - val_loss: 1.0988 - val_acc: 0.3325\n",
      "Epoch 3/10\n",
      "6084/6084 [==============================] - 144s 24ms/step - loss: 1.0988 - acc: 0.3366 - val_loss: 1.0988 - val_acc: 0.3323\n",
      "Epoch 4/10\n",
      "6084/6084 [==============================] - 145s 24ms/step - loss: 1.0988 - acc: 0.3322 - val_loss: 1.0988 - val_acc: 0.3323\n",
      "Epoch 5/10\n",
      "6084/6084 [==============================] - 145s 24ms/step - loss: 1.0988 - acc: 0.3321 - val_loss: 1.0988 - val_acc: 0.3322\n",
      "Epoch 6/10\n",
      "6084/6084 [==============================] - 143s 24ms/step - loss: 1.0988 - acc: 0.3321 - val_loss: 1.0988 - val_acc: 0.3322\n",
      "Epoch 7/10\n",
      "6084/6084 [==============================] - 145s 24ms/step - loss: 1.0988 - acc: 0.3321 - val_loss: 1.0988 - val_acc: 0.3324\n",
      "Epoch 8/10\n",
      "6084/6084 [==============================] - 143s 23ms/step - loss: 1.0988 - acc: 0.3321 - val_loss: 1.0988 - val_acc: 0.3323\n",
      "Epoch 9/10\n",
      "6084/6084 [==============================] - 143s 23ms/step - loss: 1.0988 - acc: 0.3321 - val_loss: 1.0988 - val_acc: 0.3324\n",
      "Epoch 10/10\n",
      "6084/6084 [==============================] - 143s 24ms/step - loss: 1.0988 - acc: 0.3321 - val_loss: 1.0988 - val_acc: 0.3324\n",
      "Test score: 1.0988241953727527\n",
      "Test accuracy: 0.33226495726495725\n",
      "Stapelgroesse (batchSize): 32\n",
      "Anzahl der Faltungsschichten: 4Layer\n",
      "Aktivierungsfunktion: elu\n",
      "Optimierungsfunktion: Adam\n",
      "Dropout-Rate Faltungsschicht 1: 0.5248153728682362\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 64\n",
      "Dropout-Rate Faltungsschicht 2: 0.013771620713159093\n",
      "Anzahl der Faltungsschichten: 4Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 64\n",
      "Dropout-Rate Faltungsschicht 3: 0.06296007551272016\n",
      "Anzahl der Filter-Maps Faltungsschicht 4: 64\n",
      "Dropout-Rate Faltungsschicht 4: 0.4130000695689919\n",
      "Anzahl der Neuronen des Fully Connected Layer: 128\n",
      "Dropout-Rate Fully Connected Layer: 0.5014865063866658\n",
      "Faltungsnetz wird trainiert...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1521/1521 [==============================] - 104s 69ms/step - loss: 1.0363 - acc: 0.4526 - val_loss: 0.9934 - val_acc: 0.4921\n",
      "Epoch 2/10\n",
      "1521/1521 [==============================] - 99s 65ms/step - loss: 0.9568 - acc: 0.5175 - val_loss: 0.9295 - val_acc: 0.5018\n",
      "Epoch 3/10\n",
      "1521/1521 [==============================] - 100s 66ms/step - loss: 0.9039 - acc: 0.5530 - val_loss: 0.8530 - val_acc: 0.5838\n",
      "Epoch 4/10\n",
      "1521/1521 [==============================] - 100s 66ms/step - loss: 0.8884 - acc: 0.5649 - val_loss: 0.8625 - val_acc: 0.5937\n",
      "Epoch 5/10\n",
      "1521/1521 [==============================] - 101s 66ms/step - loss: 0.8793 - acc: 0.5687 - val_loss: 0.8339 - val_acc: 0.6100\n",
      "Epoch 6/10\n",
      "1521/1521 [==============================] - 100s 66ms/step - loss: 0.8703 - acc: 0.5756 - val_loss: 0.9249 - val_acc: 0.5659\n",
      "Epoch 7/10\n",
      "1521/1521 [==============================] - 100s 66ms/step - loss: 0.8676 - acc: 0.5761 - val_loss: 0.8247 - val_acc: 0.6163\n",
      "Epoch 8/10\n",
      "1521/1521 [==============================] - 100s 66ms/step - loss: 0.8655 - acc: 0.5748 - val_loss: 0.9428 - val_acc: 0.5602\n",
      "Epoch 9/10\n",
      "1521/1521 [==============================] - 100s 66ms/step - loss: 0.8604 - acc: 0.5779 - val_loss: 0.8197 - val_acc: 0.6139\n",
      "Epoch 10/10\n",
      "1521/1521 [==============================] - 100s 66ms/step - loss: 0.8614 - acc: 0.5779 - val_loss: 0.9364 - val_acc: 0.5755\n",
      "Test score: 0.936642724746152\n",
      "Test accuracy: 0.5757401315789473\n",
      "Stapelgroesse (batchSize): 8\n",
      "Anzahl der Faltungsschichten: 3Layer\n",
      "Aktivierungsfunktion: relu\n",
      "Optimierungsfunktion: RMSprop\n",
      "Dropout-Rate Faltungsschicht 1: 0.06307711151849073\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 64\n",
      "Dropout-Rate Faltungsschicht 2: 0.20382445694216605\n",
      "Anzahl der Faltungsschichten: 3Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 128\n",
      "Dropout-Rate Faltungsschicht 3: 0.12248203532640545\n",
      "Anzahl der Neuronen des Fully Connected Layer: 64\n",
      "Dropout-Rate Fully Connected Layer: 0.6866387698783794\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "6084/6084 [==============================] - 163s 27ms/step - loss: 1.0883 - acc: 0.4074 - val_loss: 1.0784 - val_acc: 0.4647\n",
      "Epoch 2/10\n",
      "6084/6084 [==============================] - 159s 26ms/step - loss: 1.0619 - acc: 0.4464 - val_loss: 1.0335 - val_acc: 0.4391\n",
      "Epoch 3/10\n",
      "6084/6084 [==============================] - 158s 26ms/step - loss: 1.0698 - acc: 0.4417 - val_loss: 1.0663 - val_acc: 0.4113\n",
      "Epoch 4/10\n",
      "6084/6084 [==============================] - 158s 26ms/step - loss: 1.0861 - acc: 0.4334 - val_loss: 1.0981 - val_acc: 0.4171\n",
      "Epoch 5/10\n",
      "6084/6084 [==============================] - 158s 26ms/step - loss: 1.0820 - acc: 0.4377 - val_loss: 1.0952 - val_acc: 0.3439\n",
      "Epoch 6/10\n",
      "6084/6084 [==============================] - 158s 26ms/step - loss: 1.0936 - acc: 0.4367 - val_loss: 1.0980 - val_acc: 0.3333\n",
      "Epoch 7/10\n",
      "6084/6084 [==============================] - 157s 26ms/step - loss: 1.0812 - acc: 0.4325 - val_loss: 1.1060 - val_acc: 0.3457\n",
      "Epoch 8/10\n",
      "6084/6084 [==============================] - 157s 26ms/step - loss: 1.0919 - acc: 0.4384 - val_loss: 1.1063 - val_acc: 0.3337\n",
      "Epoch 9/10\n",
      "6084/6084 [==============================] - 158s 26ms/step - loss: 1.0943 - acc: 0.4309 - val_loss: 1.1143 - val_acc: 0.3337\n",
      "Epoch 10/10\n",
      "6084/6084 [==============================] - 157s 26ms/step - loss: 1.0932 - acc: 0.4323 - val_loss: 1.1891 - val_acc: 0.3340\n",
      "Test score: 1.1891658110182823\n",
      "Test accuracy: 0.3339086127547666\n",
      "Stapelgroesse (batchSize): 8\n",
      "Anzahl der Faltungsschichten: 3Layer\n",
      "Aktivierungsfunktion: relu\n",
      "Optimierungsfunktion: Adam\n",
      "Dropout-Rate Faltungsschicht 1: 0.36856104697946446\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 64\n",
      "Dropout-Rate Faltungsschicht 2: 0.5417435612067958\n",
      "Anzahl der Faltungsschichten: 3Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 128\n",
      "Dropout-Rate Faltungsschicht 3: 0.006998887818033605\n",
      "Anzahl der Neuronen des Fully Connected Layer: 64\n",
      "Dropout-Rate Fully Connected Layer: 0.378686634478515\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "6084/6084 [==============================] - 168s 28ms/step - loss: 1.1034 - acc: 0.3327 - val_loss: 1.0988 - val_acc: 0.3323\n",
      "Epoch 2/10\n",
      "6084/6084 [==============================] - 164s 27ms/step - loss: 1.0989 - acc: 0.3320 - val_loss: 1.0988 - val_acc: 0.3323\n",
      "Epoch 3/10\n",
      "6084/6084 [==============================] - 164s 27ms/step - loss: 1.0988 - acc: 0.3319 - val_loss: 1.0988 - val_acc: 0.3325\n",
      "Epoch 4/10\n",
      "6084/6084 [==============================] - 163s 27ms/step - loss: 1.0988 - acc: 0.3321 - val_loss: 1.0988 - val_acc: 0.3323\n",
      "Epoch 5/10\n",
      "6084/6084 [==============================] - 163s 27ms/step - loss: 1.0988 - acc: 0.3321 - val_loss: 1.0988 - val_acc: 0.3323\n",
      "Epoch 6/10\n",
      "6084/6084 [==============================] - 163s 27ms/step - loss: 1.0988 - acc: 0.3321 - val_loss: 1.0988 - val_acc: 0.3323\n",
      "Epoch 7/10\n",
      "6084/6084 [==============================] - 163s 27ms/step - loss: 1.0988 - acc: 0.3321 - val_loss: 1.0988 - val_acc: 0.3323\n",
      "Epoch 8/10\n",
      "6084/6084 [==============================] - 164s 27ms/step - loss: 1.0988 - acc: 0.3321 - val_loss: 1.0988 - val_acc: 0.3323\n",
      "Epoch 9/10\n",
      "6084/6084 [==============================] - 163s 27ms/step - loss: 1.0988 - acc: 0.3321 - val_loss: 1.0988 - val_acc: 0.3323\n",
      "Epoch 10/10\n",
      "6084/6084 [==============================] - 163s 27ms/step - loss: 1.0988 - acc: 0.3321 - val_loss: 1.0988 - val_acc: 0.3323\n",
      "Test score: 1.0988241953727527\n",
      "Test accuracy: 0.33226495726495725\n",
      "Stapelgroesse (batchSize): 8\n",
      "Anzahl der Faltungsschichten: 4Layer\n",
      "Aktivierungsfunktion: elu\n",
      "Optimierungsfunktion: Adam\n",
      "Dropout-Rate Faltungsschicht 1: 0.34597742155086353\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 32\n",
      "Dropout-Rate Faltungsschicht 2: 0.6186099992277365\n",
      "Anzahl der Faltungsschichten: 4Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 64\n",
      "Dropout-Rate Faltungsschicht 3: 0.13728851926848906\n",
      "Anzahl der Filter-Maps Faltungsschicht 4: 128\n",
      "Dropout-Rate Faltungsschicht 4: 0.13883546240860023\n",
      "Anzahl der Neuronen des Fully Connected Layer: 64\n",
      "Dropout-Rate Fully Connected Layer: 0.44849361739763\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "6084/6084 [==============================] - 146s 24ms/step - loss: 1.0764 - acc: 0.4290 - val_loss: 1.0140 - val_acc: 0.4649\n",
      "Epoch 2/10\n",
      "6084/6084 [==============================] - 143s 23ms/step - loss: 1.0248 - acc: 0.4605 - val_loss: 0.9890 - val_acc: 0.4840\n",
      "Epoch 3/10\n",
      "6084/6084 [==============================] - 142s 23ms/step - loss: 0.9979 - acc: 0.4818 - val_loss: 0.9164 - val_acc: 0.5212\n",
      "Epoch 4/10\n",
      "6084/6084 [==============================] - 141s 23ms/step - loss: 1.0005 - acc: 0.4734 - val_loss: 0.9408 - val_acc: 0.5308\n",
      "Epoch 5/10\n",
      "6084/6084 [==============================] - 231s 38ms/step - loss: 1.0042 - acc: 0.4647 - val_loss: 1.8098 - val_acc: 0.4454\n",
      "Epoch 6/10\n",
      "6084/6084 [==============================] - 163s 27ms/step - loss: 1.0342 - acc: 0.4310 - val_loss: 1.1010 - val_acc: 0.3343\n",
      "Epoch 7/10\n",
      "6084/6084 [==============================] - 142s 23ms/step - loss: 1.1104 - acc: 0.3314 - val_loss: 1.1014 - val_acc: 0.3322\n",
      "Epoch 8/10\n",
      "6084/6084 [==============================] - 141s 23ms/step - loss: 1.1095 - acc: 0.3316 - val_loss: 1.1007 - val_acc: 0.3324\n",
      "Epoch 9/10\n",
      "6084/6084 [==============================] - 140s 23ms/step - loss: 1.1097 - acc: 0.3333 - val_loss: 1.1005 - val_acc: 0.3323\n",
      "Epoch 10/10\n",
      "6084/6084 [==============================] - 141s 23ms/step - loss: 1.1096 - acc: 0.3292 - val_loss: 1.1012 - val_acc: 0.3324\n",
      "Test score: 1.1012044905989207\n",
      "Test accuracy: 0.33226495726495725\n",
      "Stapelgroesse (batchSize): 32\n",
      "Anzahl der Faltungsschichten: 3Layer\n",
      "Aktivierungsfunktion: elu\n",
      "Optimierungsfunktion: RMSprop\n",
      "Dropout-Rate Faltungsschicht 1: 0.39369408404795847\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 32\n",
      "Dropout-Rate Faltungsschicht 2: 0.04882610210582032\n",
      "Anzahl der Faltungsschichten: 3Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 64\n",
      "Dropout-Rate Faltungsschicht 3: 0.06540654020186992\n",
      "Anzahl der Neuronen des Fully Connected Layer: 128\n",
      "Dropout-Rate Fully Connected Layer: 0.2192185143600429\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "1521/1521 [==============================] - 92s 61ms/step - loss: 10.7455 - acc: 0.3329 - val_loss: 10.7419 - val_acc: 0.3336\n",
      "Epoch 2/10\n",
      "1521/1521 [==============================] - 87s 57ms/step - loss: 10.7486 - acc: 0.3331 - val_loss: 10.7441 - val_acc: 0.3334\n",
      "Epoch 3/10\n",
      "1521/1521 [==============================] - 87s 57ms/step - loss: 10.7480 - acc: 0.3332 - val_loss: 10.7441 - val_acc: 0.3334\n",
      "Epoch 4/10\n",
      "1521/1521 [==============================] - 87s 57ms/step - loss: 10.7473 - acc: 0.3332 - val_loss: 10.7441 - val_acc: 0.3334\n",
      "Epoch 5/10\n",
      "1521/1521 [==============================] - 87s 57ms/step - loss: 10.7490 - acc: 0.3331 - val_loss: 10.7441 - val_acc: 0.3334\n",
      "Epoch 6/10\n",
      "1521/1521 [==============================] - 87s 57ms/step - loss: 10.7477 - acc: 0.3332 - val_loss: 10.7441 - val_acc: 0.3334\n",
      "Epoch 7/10\n",
      "1521/1521 [==============================] - 87s 58ms/step - loss: 10.7473 - acc: 0.3332 - val_loss: 10.7441 - val_acc: 0.3334\n",
      "Epoch 8/10\n",
      "1521/1521 [==============================] - 88s 58ms/step - loss: 10.7486 - acc: 0.3331 - val_loss: 10.7441 - val_acc: 0.3334\n",
      "Epoch 9/10\n",
      "1521/1521 [==============================] - 87s 57ms/step - loss: 10.7493 - acc: 0.3331 - val_loss: 10.7441 - val_acc: 0.3334\n",
      "Epoch 10/10\n",
      "1521/1521 [==============================] - 88s 58ms/step - loss: 10.7490 - acc: 0.3331 - val_loss: 10.7441 - val_acc: 0.3334\n",
      "Test score: 10.74186221674869\n",
      "Test accuracy: 0.3335526315789474\n",
      "Stapelgroesse (batchSize): 32\n",
      "Anzahl der Faltungsschichten: 3Layer\n",
      "Aktivierungsfunktion: relu\n",
      "Optimierungsfunktion: Adam\n",
      "Dropout-Rate Faltungsschicht 1: 0.4938729716239127\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 64\n",
      "Dropout-Rate Faltungsschicht 2: 0.3577944426633437\n",
      "Anzahl der Faltungsschichten: 3Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 64\n",
      "Dropout-Rate Faltungsschicht 3: 0.648938910990447\n",
      "Anzahl der Neuronen des Fully Connected Layer: 128\n",
      "Dropout-Rate Fully Connected Layer: 0.09833143896930721\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "1521/1521 [==============================] - 103s 68ms/step - loss: 1.0908 - acc: 0.3653 - val_loss: 1.0192 - val_acc: 0.4586\n",
      "Epoch 2/10\n",
      "1521/1521 [==============================] - 97s 64ms/step - loss: 1.0201 - acc: 0.4638 - val_loss: 1.0127 - val_acc: 0.4680\n",
      "Epoch 3/10\n",
      "1521/1521 [==============================] - 97s 64ms/step - loss: 0.9945 - acc: 0.4834 - val_loss: 1.0275 - val_acc: 0.4507\n",
      "Epoch 4/10\n",
      "1521/1521 [==============================] - 97s 64ms/step - loss: 0.9801 - acc: 0.4911 - val_loss: 0.9819 - val_acc: 0.4976\n",
      "Epoch 5/10\n",
      "1521/1521 [==============================] - 98s 64ms/step - loss: 0.9649 - acc: 0.5057 - val_loss: 0.9819 - val_acc: 0.4885\n",
      "Epoch 6/10\n",
      "1521/1521 [==============================] - 99s 65ms/step - loss: 0.9480 - acc: 0.5221 - val_loss: 0.9864 - val_acc: 0.4967\n",
      "Epoch 7/10\n",
      "1521/1521 [==============================] - 98s 65ms/step - loss: 0.9314 - acc: 0.5359 - val_loss: 1.0021 - val_acc: 0.4988\n",
      "Epoch 8/10\n",
      "1521/1521 [==============================] - 98s 65ms/step - loss: 0.9180 - acc: 0.5438 - val_loss: 0.9714 - val_acc: 0.5081\n",
      "Epoch 9/10\n",
      "1521/1521 [==============================] - 98s 64ms/step - loss: 0.9076 - acc: 0.5527 - val_loss: 1.0126 - val_acc: 0.4810\n",
      "Epoch 10/10\n",
      "1521/1521 [==============================] - 97s 64ms/step - loss: 0.8962 - acc: 0.5597 - val_loss: 0.9989 - val_acc: 0.4797\n",
      "Test score: 0.9990403600429234\n",
      "Test accuracy: 0.47952302631578947\n",
      "Stapelgroesse (batchSize): 16\n",
      "Anzahl der Faltungsschichten: 3Layer\n",
      "Aktivierungsfunktion: elu\n",
      "Optimierungsfunktion: RMSprop\n",
      "Dropout-Rate Faltungsschicht 1: 0.0030492067550873855\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 32\n",
      "Dropout-Rate Faltungsschicht 2: 0.36724989130755226\n",
      "Anzahl der Faltungsschichten: 3Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 64\n",
      "Dropout-Rate Faltungsschicht 3: 0.09099759210319268\n",
      "Anzahl der Neuronen des Fully Connected Layer: 128\n",
      "Dropout-Rate Fully Connected Layer: 0.3253263629327289\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "3042/3042 [==============================] - 111s 36ms/step - loss: 10.7487 - acc: 0.3329 - val_loss: 10.7419 - val_acc: 0.3336\n",
      "Epoch 2/10\n",
      "3042/3042 [==============================] - 106s 35ms/step - loss: 10.7498 - acc: 0.3331 - val_loss: 10.7432 - val_acc: 0.3335\n",
      "Epoch 3/10\n",
      "3042/3042 [==============================] - 105s 35ms/step - loss: 10.7498 - acc: 0.3331 - val_loss: 10.7432 - val_acc: 0.3335\n",
      "Epoch 4/10\n",
      "3042/3042 [==============================] - 106s 35ms/step - loss: 10.7498 - acc: 0.3331 - val_loss: 10.7432 - val_acc: 0.3335\n",
      "Epoch 5/10\n",
      "3042/3042 [==============================] - 107s 35ms/step - loss: 10.7403 - acc: 0.3336 - val_loss: 10.7339 - val_acc: 0.3340\n",
      "Epoch 6/10\n",
      "3042/3042 [==============================] - 106s 35ms/step - loss: 10.7497 - acc: 0.3331 - val_loss: 10.7339 - val_acc: 0.3340\n",
      "Epoch 7/10\n",
      "3042/3042 [==============================] - 107s 35ms/step - loss: 10.7507 - acc: 0.3330 - val_loss: 10.7339 - val_acc: 0.3340\n",
      "Epoch 8/10\n",
      "3042/3042 [==============================] - 106s 35ms/step - loss: 10.7514 - acc: 0.3330 - val_loss: 10.7339 - val_acc: 0.3340\n",
      "Epoch 9/10\n",
      "3042/3042 [==============================] - 106s 35ms/step - loss: 10.7497 - acc: 0.3331 - val_loss: 10.7339 - val_acc: 0.3340\n",
      "Epoch 10/10\n",
      "3042/3042 [==============================] - 107s 35ms/step - loss: 10.7507 - acc: 0.3330 - val_loss: 10.7339 - val_acc: 0.3340\n",
      "Test score: 10.732583765607131\n",
      "Test accuracy: 0.3341282894736842\n",
      "Stapelgroesse (batchSize): 32\n",
      "Anzahl der Faltungsschichten: 3Layer\n",
      "Aktivierungsfunktion: relu\n",
      "Optimierungsfunktion: RMSprop\n",
      "Dropout-Rate Faltungsschicht 1: 0.630488342187824\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 64\n",
      "Dropout-Rate Faltungsschicht 2: 0.656156000747102\n",
      "Anzahl der Faltungsschichten: 3Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 64\n",
      "Dropout-Rate Faltungsschicht 3: 0.22607023985917013\n",
      "Anzahl der Neuronen des Fully Connected Layer: 64\n",
      "Dropout-Rate Fully Connected Layer: 0.07759341739291725\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "1521/1521 [==============================] - 101s 66ms/step - loss: 10.4328 - acc: 0.3330 - val_loss: 1.0958 - val_acc: 0.3865\n",
      "Epoch 2/10\n",
      "1521/1521 [==============================] - 96s 63ms/step - loss: 1.0401 - acc: 0.4478 - val_loss: 0.9832 - val_acc: 0.4862\n",
      "Epoch 3/10\n",
      "1521/1521 [==============================] - 95s 63ms/step - loss: 1.0078 - acc: 0.4727 - val_loss: 0.9902 - val_acc: 0.4912\n",
      "Epoch 4/10\n",
      "1521/1521 [==============================] - 95s 63ms/step - loss: 0.9820 - acc: 0.4989 - val_loss: 1.0821 - val_acc: 0.4136\n",
      "Epoch 5/10\n",
      "1521/1521 [==============================] - 95s 63ms/step - loss: 0.9589 - acc: 0.5203 - val_loss: 1.0072 - val_acc: 0.4722\n",
      "Epoch 6/10\n",
      "1521/1521 [==============================] - 95s 63ms/step - loss: 0.9491 - acc: 0.5263 - val_loss: 1.0327 - val_acc: 0.4467\n",
      "Epoch 7/10\n",
      "1521/1521 [==============================] - 96s 63ms/step - loss: 0.9418 - acc: 0.5312 - val_loss: 1.0634 - val_acc: 0.4013\n",
      "Epoch 8/10\n",
      "1521/1521 [==============================] - 95s 63ms/step - loss: 0.9384 - acc: 0.5370 - val_loss: 1.0973 - val_acc: 0.3536\n",
      "Epoch 9/10\n",
      "1521/1521 [==============================] - 95s 63ms/step - loss: 0.9328 - acc: 0.5404 - val_loss: 1.0625 - val_acc: 0.3927\n",
      "Epoch 10/10\n",
      "1521/1521 [==============================] - 95s 63ms/step - loss: 0.9334 - acc: 0.5413 - val_loss: 0.9990 - val_acc: 0.4909\n",
      "Test score: 0.9990915030241012\n",
      "Test accuracy: 0.4908717105263158\n",
      "Stapelgroesse (batchSize): 8\n",
      "Anzahl der Faltungsschichten: 4Layer\n",
      "Aktivierungsfunktion: relu\n",
      "Optimierungsfunktion: RMSprop\n",
      "Dropout-Rate Faltungsschicht 1: 0.2054401692759159\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 64\n",
      "Dropout-Rate Faltungsschicht 2: 0.6715296954099724\n",
      "Anzahl der Faltungsschichten: 4Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 128\n",
      "Dropout-Rate Faltungsschicht 3: 0.11478226426028586\n",
      "Anzahl der Filter-Maps Faltungsschicht 4: 128\n",
      "Dropout-Rate Faltungsschicht 4: 0.6160082475762262\n",
      "Anzahl der Neuronen des Fully Connected Layer: 64\n",
      "Dropout-Rate Fully Connected Layer: 0.23722270891400718\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "6084/6084 [==============================] - 164s 27ms/step - loss: 1.0693 - acc: 0.4380 - val_loss: 1.0084 - val_acc: 0.4857\n",
      "Epoch 2/10\n",
      "6084/6084 [==============================] - 159s 26ms/step - loss: 1.0428 - acc: 0.4610 - val_loss: 1.0630 - val_acc: 0.3880\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6084/6084 [==============================] - 159s 26ms/step - loss: 1.0482 - acc: 0.4575 - val_loss: 1.0619 - val_acc: 0.4248\n",
      "Epoch 4/10\n",
      "6084/6084 [==============================] - 159s 26ms/step - loss: 1.0515 - acc: 0.4591 - val_loss: 1.0160 - val_acc: 0.4598\n",
      "Epoch 5/10\n",
      "6084/6084 [==============================] - 157s 26ms/step - loss: 1.0519 - acc: 0.4571 - val_loss: 1.0409 - val_acc: 0.3927\n",
      "Epoch 6/10\n",
      "6084/6084 [==============================] - 159s 26ms/step - loss: 1.0565 - acc: 0.4505 - val_loss: 1.0585 - val_acc: 0.4151\n",
      "Epoch 7/10\n",
      "6084/6084 [==============================] - 160s 26ms/step - loss: 1.0721 - acc: 0.4382 - val_loss: 1.0193 - val_acc: 0.4819\n",
      "Epoch 8/10\n",
      "6084/6084 [==============================] - 157s 26ms/step - loss: 1.0766 - acc: 0.4362 - val_loss: 1.0561 - val_acc: 0.3813\n",
      "Epoch 9/10\n",
      "6084/6084 [==============================] - 159s 26ms/step - loss: 1.0700 - acc: 0.4369 - val_loss: 1.0254 - val_acc: 0.4828\n",
      "Epoch 10/10\n",
      "6084/6084 [==============================] - 158s 26ms/step - loss: 1.0760 - acc: 0.4386 - val_loss: 1.0370 - val_acc: 0.4201\n",
      "Test score: 1.0369844222680116\n",
      "Test accuracy: 0.42011834319526625\n",
      "Stapelgroesse (batchSize): 8\n",
      "Anzahl der Faltungsschichten: 4Layer\n",
      "Aktivierungsfunktion: relu\n",
      "Optimierungsfunktion: Adam\n",
      "Dropout-Rate Faltungsschicht 1: 0.5350208375977319\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 32\n",
      "Dropout-Rate Faltungsschicht 2: 0.07974928784802193\n",
      "Anzahl der Faltungsschichten: 4Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 128\n",
      "Dropout-Rate Faltungsschicht 3: 0.576047989074638\n",
      "Anzahl der Filter-Maps Faltungsschicht 4: 64\n",
      "Dropout-Rate Faltungsschicht 4: 0.3257886399132629\n",
      "Anzahl der Neuronen des Fully Connected Layer: 128\n",
      "Dropout-Rate Fully Connected Layer: 0.5671192786132916\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "6084/6084 [==============================] - 148s 24ms/step - loss: 1.0997 - acc: 0.3333 - val_loss: 1.0989 - val_acc: 0.3323\n",
      "Epoch 2/10\n",
      "6084/6084 [==============================] - 143s 24ms/step - loss: 1.0990 - acc: 0.3314 - val_loss: 1.0988 - val_acc: 0.3325\n",
      "Epoch 3/10\n",
      "6084/6084 [==============================] - 144s 24ms/step - loss: 1.0988 - acc: 0.3310 - val_loss: 1.0988 - val_acc: 0.3323\n",
      "Epoch 4/10\n",
      "6084/6084 [==============================] - 143s 24ms/step - loss: 1.0988 - acc: 0.3329 - val_loss: 1.0988 - val_acc: 0.3323\n",
      "Epoch 5/10\n",
      "6084/6084 [==============================] - 144s 24ms/step - loss: 1.0988 - acc: 0.3320 - val_loss: 1.0988 - val_acc: 0.3323\n",
      "Epoch 6/10\n",
      "6084/6084 [==============================] - 144s 24ms/step - loss: 1.0988 - acc: 0.3321 - val_loss: 1.0988 - val_acc: 0.3322\n",
      "Epoch 7/10\n",
      "6084/6084 [==============================] - 145s 24ms/step - loss: 1.0988 - acc: 0.3321 - val_loss: 1.0988 - val_acc: 0.3324\n",
      "Epoch 8/10\n",
      "6084/6084 [==============================] - 146s 24ms/step - loss: 1.0988 - acc: 0.3321 - val_loss: 1.0988 - val_acc: 0.3324\n",
      "Epoch 9/10\n",
      "6084/6084 [==============================] - 144s 24ms/step - loss: 1.0988 - acc: 0.3321 - val_loss: 1.0988 - val_acc: 0.3324\n",
      "Epoch 10/10\n",
      "6084/6084 [==============================] - 145s 24ms/step - loss: 1.0988 - acc: 0.3321 - val_loss: 1.0988 - val_acc: 0.3324\n",
      "Test score: 1.0988241953727527\n",
      "Test accuracy: 0.33226495726495725\n",
      "Stapelgroesse (batchSize): 32\n",
      "Anzahl der Faltungsschichten: 3Layer\n",
      "Aktivierungsfunktion: elu\n",
      "Optimierungsfunktion: Adam\n",
      "Dropout-Rate Faltungsschicht 1: 0.18858047055448576\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 32\n",
      "Dropout-Rate Faltungsschicht 2: 0.4476969990061666\n",
      "Anzahl der Faltungsschichten: 3Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 128\n",
      "Dropout-Rate Faltungsschicht 3: 0.6569615931883058\n",
      "Anzahl der Neuronen des Fully Connected Layer: 128\n",
      "Dropout-Rate Fully Connected Layer: 0.1546339895111094\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "1521/1521 [==============================] - 105s 69ms/step - loss: 10.7296 - acc: 0.3337 - val_loss: 10.7617 - val_acc: 0.3323\n",
      "Epoch 2/10\n",
      "1521/1521 [==============================] - 99s 65ms/step - loss: 10.7355 - acc: 0.3339 - val_loss: 10.7587 - val_acc: 0.3325\n",
      "Epoch 3/10\n",
      "1521/1521 [==============================] - 99s 65ms/step - loss: 10.7362 - acc: 0.3339 - val_loss: 10.7587 - val_acc: 0.3325\n",
      "Epoch 4/10\n",
      "1521/1521 [==============================] - 99s 65ms/step - loss: 10.7372 - acc: 0.3338 - val_loss: 10.7587 - val_acc: 0.3325\n",
      "Epoch 5/10\n",
      "1521/1521 [==============================] - 100s 66ms/step - loss: 10.7349 - acc: 0.3340 - val_loss: 10.7587 - val_acc: 0.3325\n",
      "Epoch 6/10\n",
      "1521/1521 [==============================] - 99s 65ms/step - loss: 10.7355 - acc: 0.3339 - val_loss: 10.7587 - val_acc: 0.3325\n",
      "Epoch 7/10\n",
      "1521/1521 [==============================] - 99s 65ms/step - loss: 10.7375 - acc: 0.3338 - val_loss: 10.7587 - val_acc: 0.3325\n",
      "Epoch 8/10\n",
      "1521/1521 [==============================] - 99s 65ms/step - loss: 10.7355 - acc: 0.3339 - val_loss: 10.7587 - val_acc: 0.3325\n",
      "Epoch 9/10\n",
      "1521/1521 [==============================] - 100s 66ms/step - loss: 10.7359 - acc: 0.3339 - val_loss: 10.7587 - val_acc: 0.3325\n",
      "Epoch 10/10\n",
      "1521/1521 [==============================] - 98s 65ms/step - loss: 10.7359 - acc: 0.3339 - val_loss: 10.7587 - val_acc: 0.3325\n",
      "Test score: 10.76174474264446\n",
      "Test accuracy: 0.3323190789473684\n",
      "Stapelgroesse (batchSize): 32\n",
      "Anzahl der Faltungsschichten: 3Layer\n",
      "Aktivierungsfunktion: relu\n",
      "Optimierungsfunktion: Adam\n",
      "Dropout-Rate Faltungsschicht 1: 0.5613716767868046\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 32\n",
      "Dropout-Rate Faltungsschicht 2: 0.5852087626516953\n",
      "Anzahl der Faltungsschichten: 3Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 128\n",
      "Dropout-Rate Faltungsschicht 3: 0.2755490740591204\n",
      "Anzahl der Neuronen des Fully Connected Layer: 64\n",
      "Dropout-Rate Fully Connected Layer: 0.6158482815230839\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "1521/1521 [==============================] - 98s 65ms/step - loss: 1.1127 - acc: 0.3419 - val_loss: 1.0988 - val_acc: 0.3323\n",
      "Epoch 2/10\n",
      "1521/1521 [==============================] - 93s 61ms/step - loss: 1.0945 - acc: 0.3480 - val_loss: 1.0912 - val_acc: 0.3521\n",
      "Epoch 3/10\n",
      "1521/1521 [==============================] - 92s 61ms/step - loss: 1.0539 - acc: 0.4249 - val_loss: 1.0541 - val_acc: 0.4114\n",
      "Epoch 4/10\n",
      "1521/1521 [==============================] - 94s 62ms/step - loss: 1.0338 - acc: 0.4536 - val_loss: 1.0433 - val_acc: 0.4053\n",
      "Epoch 5/10\n",
      "1521/1521 [==============================] - 91s 60ms/step - loss: 1.0258 - acc: 0.4641 - val_loss: 1.0305 - val_acc: 0.4759\n",
      "Epoch 6/10\n",
      "1521/1521 [==============================] - 92s 61ms/step - loss: 1.0112 - acc: 0.4738 - val_loss: 1.0174 - val_acc: 0.4816\n",
      "Epoch 7/10\n",
      "1521/1521 [==============================] - 93s 61ms/step - loss: 1.0035 - acc: 0.4861 - val_loss: 1.0299 - val_acc: 0.4360\n",
      "Epoch 8/10\n",
      "1521/1521 [==============================] - 91s 60ms/step - loss: 0.9954 - acc: 0.4913 - val_loss: 1.0233 - val_acc: 0.4491\n",
      "Epoch 9/10\n",
      "1521/1521 [==============================] - 92s 60ms/step - loss: 0.9898 - acc: 0.4975 - val_loss: 1.0372 - val_acc: 0.4639\n",
      "Epoch 10/10\n",
      "1521/1521 [==============================] - 92s 60ms/step - loss: 0.9870 - acc: 0.5009 - val_loss: 1.0391 - val_acc: 0.4509\n",
      "Test score: 1.0391624340885564\n",
      "Test accuracy: 0.4509046052631579\n",
      "Stapelgroesse (batchSize): 8\n",
      "Anzahl der Faltungsschichten: 3Layer\n",
      "Aktivierungsfunktion: relu\n",
      "Optimierungsfunktion: Adam\n",
      "Dropout-Rate Faltungsschicht 1: 0.21758720280497001\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 32\n",
      "Dropout-Rate Faltungsschicht 2: 0.3363371996167399\n",
      "Anzahl der Faltungsschichten: 3Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 64\n",
      "Dropout-Rate Faltungsschicht 3: 0.3304329619875355\n",
      "Anzahl der Neuronen des Fully Connected Layer: 128\n",
      "Dropout-Rate Fully Connected Layer: 0.05575408127472528\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "6084/6084 [==============================] - 151s 25ms/step - loss: 1.0790 - acc: 0.3753 - val_loss: 1.0250 - val_acc: 0.4648\n",
      "Epoch 2/10\n",
      "6084/6084 [==============================] - 147s 24ms/step - loss: 1.0086 - acc: 0.4694 - val_loss: 1.0140 - val_acc: 0.4638\n",
      "Epoch 3/10\n",
      "6084/6084 [==============================] - 146s 24ms/step - loss: 0.9897 - acc: 0.4825 - val_loss: 0.9855 - val_acc: 0.4969\n",
      "Epoch 4/10\n",
      "6084/6084 [==============================] - 146s 24ms/step - loss: 0.9772 - acc: 0.4922 - val_loss: 1.0020 - val_acc: 0.4974\n",
      "Epoch 5/10\n",
      "6084/6084 [==============================] - 147s 24ms/step - loss: 0.9641 - acc: 0.4991 - val_loss: 1.0066 - val_acc: 0.4927\n",
      "Epoch 6/10\n",
      "6084/6084 [==============================] - 146s 24ms/step - loss: 0.9555 - acc: 0.5119 - val_loss: 0.9894 - val_acc: 0.5003\n",
      "Epoch 7/10\n",
      "6084/6084 [==============================] - 146s 24ms/step - loss: 0.9469 - acc: 0.5164 - val_loss: 0.9961 - val_acc: 0.4854\n",
      "Epoch 8/10\n",
      "6084/6084 [==============================] - 145s 24ms/step - loss: 0.9440 - acc: 0.5192 - val_loss: 0.9678 - val_acc: 0.5283\n",
      "Epoch 9/10\n",
      "6084/6084 [==============================] - 145s 24ms/step - loss: 0.9343 - acc: 0.5263 - val_loss: 0.9879 - val_acc: 0.4996\n",
      "Epoch 10/10\n",
      "6084/6084 [==============================] - 144s 24ms/step - loss: 0.9203 - acc: 0.5365 - val_loss: 0.9888 - val_acc: 0.4946\n",
      "Test score: 0.9886837568743929\n",
      "Test accuracy: 0.4944937541091387\n",
      "Stapelgroesse (batchSize): 16\n",
      "Anzahl der Faltungsschichten: 5Layer\n",
      "Aktivierungsfunktion: relu\n",
      "Optimierungsfunktion: Adam\n",
      "Dropout-Rate Faltungsschicht 1: 0.3174428272991308\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 32\n",
      "Dropout-Rate Faltungsschicht 2: 0.621925032040878\n",
      "Anzahl der Faltungsschichten: 5Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 128\n",
      "Dropout-Rate Faltungsschicht 3: 0.5751935015974993\n",
      "Anzahl der Filter-Maps Faltungsschicht 4: 128\n",
      "Dropout-Rate Faltungsschicht 4: 0.4478389595207329\n",
      "Anzahl der Filter-Maps Faltungsschicht 5: 128\n",
      "Dropout-Rate Faltungsschicht 5: 0.35642409048822615\n",
      "Anzahl der Neuronen des Fully Connected Layer: 64\n",
      "Dropout-Rate Fully Connected Layer: 0.12907281413447744\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "3042/3042 [==============================] - 123s 40ms/step - loss: 1.1000 - acc: 0.3365 - val_loss: 1.0987 - val_acc: 0.3341\n",
      "Epoch 2/10\n",
      "3042/3042 [==============================] - 115s 38ms/step - loss: 1.0988 - acc: 0.3341 - val_loss: 1.0988 - val_acc: 0.3325\n",
      "Epoch 3/10\n",
      "3042/3042 [==============================] - 115s 38ms/step - loss: 1.0988 - acc: 0.3330 - val_loss: 1.0988 - val_acc: 0.3325\n",
      "Epoch 4/10\n",
      "3042/3042 [==============================] - 114s 38ms/step - loss: 1.0987 - acc: 0.3337 - val_loss: 1.0988 - val_acc: 0.3325\n",
      "Epoch 5/10\n",
      "3042/3042 [==============================] - 114s 37ms/step - loss: 1.0988 - acc: 0.3350 - val_loss: 1.0988 - val_acc: 0.3325\n",
      "Epoch 6/10\n",
      "3042/3042 [==============================] - 114s 37ms/step - loss: 1.0989 - acc: 0.3336 - val_loss: 1.0988 - val_acc: 0.3325\n",
      "Epoch 7/10\n",
      "3042/3042 [==============================] - 114s 37ms/step - loss: 1.0987 - acc: 0.3330 - val_loss: 1.0988 - val_acc: 0.3325\n",
      "Epoch 8/10\n",
      "3042/3042 [==============================] - 113s 37ms/step - loss: 1.0987 - acc: 0.3340 - val_loss: 1.0988 - val_acc: 0.3325\n",
      "Epoch 9/10\n",
      "3042/3042 [==============================] - 113s 37ms/step - loss: 1.0987 - acc: 0.3341 - val_loss: 1.0987 - val_acc: 0.3325\n",
      "Epoch 10/10\n",
      "3042/3042 [==============================] - 114s 37ms/step - loss: 1.0987 - acc: 0.3338 - val_loss: 1.0987 - val_acc: 0.3325\n",
      "Test score: 1.0987519562244414\n",
      "Test accuracy: 0.3323190789473684\n",
      "Stapelgroesse (batchSize): 8\n",
      "Anzahl der Faltungsschichten: 5Layer\n",
      "Aktivierungsfunktion: elu\n",
      "Optimierungsfunktion: Adam\n",
      "Dropout-Rate Faltungsschicht 1: 0.21113199297716778\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 64\n",
      "Dropout-Rate Faltungsschicht 2: 0.6081229344054953\n",
      "Anzahl der Faltungsschichten: 5Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 128\n",
      "Dropout-Rate Faltungsschicht 3: 0.5069408523662012\n",
      "Anzahl der Filter-Maps Faltungsschicht 4: 128\n",
      "Dropout-Rate Faltungsschicht 4: 0.6087434813719508\n",
      "Anzahl der Filter-Maps Faltungsschicht 5: 128\n",
      "Dropout-Rate Faltungsschicht 5: 0.6174763542737483\n",
      "Anzahl der Neuronen des Fully Connected Layer: 64\n",
      "Dropout-Rate Fully Connected Layer: 0.013953199559087713\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "6084/6084 [==============================] - 173s 28ms/step - loss: 1.1179 - acc: 0.3381 - val_loss: 1.1023 - val_acc: 0.3323\n",
      "Epoch 2/10\n",
      "6084/6084 [==============================] - 167s 27ms/step - loss: 1.1123 - acc: 0.3269 - val_loss: 1.1028 - val_acc: 0.3323\n",
      "Epoch 3/10\n",
      "6084/6084 [==============================] - 167s 27ms/step - loss: 1.1101 - acc: 0.3287 - val_loss: 1.1037 - val_acc: 0.3323\n",
      "Epoch 4/10\n",
      "6084/6084 [==============================] - 167s 27ms/step - loss: 1.1093 - acc: 0.3274 - val_loss: 1.1042 - val_acc: 0.3323\n",
      "Epoch 5/10\n",
      "6084/6084 [==============================] - 170s 28ms/step - loss: 1.1092 - acc: 0.3284 - val_loss: 1.1051 - val_acc: 0.3323\n",
      "Epoch 6/10\n",
      "6084/6084 [==============================] - 217s 36ms/step - loss: 1.1083 - acc: 0.3281 - val_loss: 1.1052 - val_acc: 0.3323\n",
      "Epoch 7/10\n",
      "6084/6084 [==============================] - 295s 48ms/step - loss: 1.1078 - acc: 0.3278 - val_loss: 1.1052 - val_acc: 0.3323\n",
      "Epoch 8/10\n",
      "6084/6084 [==============================] - 208s 34ms/step - loss: 1.1076 - acc: 0.3274 - val_loss: 1.1054 - val_acc: 0.3323\n",
      "Epoch 9/10\n",
      "6084/6084 [==============================] - 168s 28ms/step - loss: 1.1071 - acc: 0.3284 - val_loss: 1.1054 - val_acc: 0.3322\n",
      "Epoch 10/10\n",
      "6084/6084 [==============================] - 167s 27ms/step - loss: 1.1069 - acc: 0.3293 - val_loss: 1.1046 - val_acc: 0.3324\n",
      "Test score: 1.1046811865788397\n",
      "Test accuracy: 0.33226495726495725\n",
      "Stapelgroesse (batchSize): 16\n",
      "Anzahl der Faltungsschichten: 3Layer\n",
      "Aktivierungsfunktion: elu\n",
      "Optimierungsfunktion: RMSprop\n",
      "Dropout-Rate Faltungsschicht 1: 0.3829922841779661\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 32\n",
      "Dropout-Rate Faltungsschicht 2: 0.49783688567670176\n",
      "Anzahl der Faltungsschichten: 3Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 64\n",
      "Dropout-Rate Faltungsschicht 3: 0.022374882170599006\n",
      "Anzahl der Neuronen des Fully Connected Layer: 64\n",
      "Dropout-Rate Fully Connected Layer: 0.0505294469449968\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "3042/3042 [==============================] - 107s 35ms/step - loss: 10.7367 - acc: 0.3334 - val_loss: 10.7419 - val_acc: 0.3336\n",
      "Epoch 2/10\n",
      "3042/3042 [==============================] - 101s 33ms/step - loss: 10.7498 - acc: 0.3331 - val_loss: 10.7432 - val_acc: 0.3335\n",
      "Epoch 3/10\n",
      "3042/3042 [==============================] - 102s 33ms/step - loss: 10.7498 - acc: 0.3331 - val_loss: 10.7432 - val_acc: 0.3335\n",
      "Epoch 4/10\n",
      "3042/3042 [==============================] - 101s 33ms/step - loss: 10.7498 - acc: 0.3331 - val_loss: 10.7432 - val_acc: 0.3335\n",
      "Epoch 5/10\n",
      "3042/3042 [==============================] - 101s 33ms/step - loss: 10.7492 - acc: 0.3331 - val_loss: 10.7432 - val_acc: 0.3335\n",
      "Epoch 6/10\n",
      "3042/3042 [==============================] - 101s 33ms/step - loss: 10.7498 - acc: 0.3331 - val_loss: 10.7432 - val_acc: 0.3335\n",
      "Epoch 7/10\n",
      "3042/3042 [==============================] - 101s 33ms/step - loss: 10.7485 - acc: 0.3331 - val_loss: 10.7432 - val_acc: 0.3335\n",
      "Epoch 8/10\n",
      "3042/3042 [==============================] - 101s 33ms/step - loss: 10.7498 - acc: 0.3331 - val_loss: 10.7432 - val_acc: 0.3335\n",
      "Epoch 9/10\n",
      "3042/3042 [==============================] - 102s 33ms/step - loss: 10.7502 - acc: 0.3330 - val_loss: 10.7432 - val_acc: 0.3335\n",
      "Epoch 10/10\n",
      "3042/3042 [==============================] - 101s 33ms/step - loss: 10.7495 - acc: 0.3331 - val_loss: 10.7432 - val_acc: 0.3335\n",
      "Test score: 10.741862261295319\n",
      "Test accuracy: 0.3335526315789474\n",
      "Stapelgroesse (batchSize): 16\n",
      "Anzahl der Faltungsschichten: 5Layer\n",
      "Aktivierungsfunktion: elu\n",
      "Optimierungsfunktion: Adam\n",
      "Dropout-Rate Faltungsschicht 1: 0.22962711046270404\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 32\n",
      "Dropout-Rate Faltungsschicht 2: 0.5245955294997489\n",
      "Anzahl der Faltungsschichten: 5Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 128\n",
      "Dropout-Rate Faltungsschicht 3: 0.6720840701148673\n",
      "Anzahl der Filter-Maps Faltungsschicht 4: 64\n",
      "Dropout-Rate Faltungsschicht 4: 0.42616405256026657\n",
      "Anzahl der Filter-Maps Faltungsschicht 5: 128\n",
      "Dropout-Rate Faltungsschicht 5: 0.2782009184195105\n",
      "Anzahl der Neuronen des Fully Connected Layer: 64\n",
      "Dropout-Rate Fully Connected Layer: 0.3792584060542794\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3042/3042 [==============================] - 120s 39ms/step - loss: 1.0582 - acc: 0.4335 - val_loss: 0.9967 - val_acc: 0.4745\n",
      "Epoch 2/10\n",
      "3042/3042 [==============================] - 114s 37ms/step - loss: 1.0199 - acc: 0.4590 - val_loss: 0.9937 - val_acc: 0.4308\n",
      "Epoch 3/10\n",
      "3042/3042 [==============================] - 114s 37ms/step - loss: 1.0158 - acc: 0.4647 - val_loss: 0.9728 - val_acc: 0.4967\n",
      "Epoch 4/10\n",
      "3042/3042 [==============================] - 114s 37ms/step - loss: 0.9945 - acc: 0.4840 - val_loss: 0.9878 - val_acc: 0.4353\n",
      "Epoch 5/10\n",
      "3042/3042 [==============================] - 114s 37ms/step - loss: 0.9746 - acc: 0.5013 - val_loss: 0.9592 - val_acc: 0.5092\n",
      "Epoch 6/10\n",
      "3042/3042 [==============================] - 114s 37ms/step - loss: 0.9545 - acc: 0.5160 - val_loss: 0.9034 - val_acc: 0.5360\n",
      "Epoch 7/10\n",
      "3042/3042 [==============================] - 114s 37ms/step - loss: 0.9459 - acc: 0.5216 - val_loss: 0.9413 - val_acc: 0.5002\n",
      "Epoch 8/10\n",
      "3042/3042 [==============================] - 114s 37ms/step - loss: 0.9368 - acc: 0.5236 - val_loss: 0.8938 - val_acc: 0.5376\n",
      "Epoch 9/10\n",
      "3042/3042 [==============================] - 114s 37ms/step - loss: 0.9315 - acc: 0.5301 - val_loss: 0.8843 - val_acc: 0.5608\n",
      "Epoch 10/10\n",
      "3042/3042 [==============================] - 114s 37ms/step - loss: 0.9239 - acc: 0.5402 - val_loss: 0.8819 - val_acc: 0.5641\n",
      "Test score: 0.8821613910951113\n",
      "Test accuracy: 0.5640625\n",
      "Stapelgroesse (batchSize): 16\n",
      "Anzahl der Faltungsschichten: 3Layer\n",
      "Aktivierungsfunktion: elu\n",
      "Optimierungsfunktion: Adam\n",
      "Dropout-Rate Faltungsschicht 1: 0.15370988223089438\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 64\n",
      "Dropout-Rate Faltungsschicht 2: 0.6827003437767334\n",
      "Anzahl der Faltungsschichten: 3Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 64\n",
      "Dropout-Rate Faltungsschicht 3: 0.05832367795781866\n",
      "Anzahl der Neuronen des Fully Connected Layer: 128\n",
      "Dropout-Rate Fully Connected Layer: 0.014396755468086919\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "3042/3042 [==============================] - 126s 42ms/step - loss: 1.1318 - acc: 0.3383 - val_loss: 1.1140 - val_acc: 0.3323\n",
      "Epoch 2/10\n",
      "3042/3042 [==============================] - 120s 40ms/step - loss: 1.1155 - acc: 0.3316 - val_loss: 1.1089 - val_acc: 0.3325\n",
      "Epoch 3/10\n",
      "3042/3042 [==============================] - 120s 40ms/step - loss: 1.1142 - acc: 0.3324 - val_loss: 1.1090 - val_acc: 0.3325\n",
      "Epoch 4/10\n",
      "2363/3042 [======================>.......] - ETA: 23s - loss: 1.1141 - acc: 0.3294"
     ]
    }
   ],
   "source": [
    "# Die Hyperas Methode optim sucht im Suchraum die Parameter \n",
    "# Bei einer Änderung des Methodenrumpf der Methode model() muss der Notebook Kernel neu gestartet werden\n",
    "bestRun, bestModel = optim.minimize(model=model,               \n",
    "                                          data=data,\n",
    "                                          algo=rand.suggest,   # Algorithmus: Random Search\n",
    "                                          max_evals=100,          \n",
    "                                          trials=Trials(),      # eine Liste von Verzeichnissen, die alles über die Suche enthalten.\n",
    "                                          notebook_name='CNN_experiment5')  # Der Name des Notebooks sollte als String angegeben werden\n",
    "print(bestRun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Anzahl der durchläufe bisher: 77\n",
    "Bestes Ergebnis bisherisher: 0.6269"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "102px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
