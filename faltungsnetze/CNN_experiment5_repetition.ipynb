{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Faltungsnetz Versuch 5: Route mit Wiederholung\n",
    "\n",
    "\n",
    "## Einleitung\n",
    "\n",
    "Bei diesem Versuch wurde wiederholt die gleiche Route mit einem Fahrrad befahren siehe. Ziel war es durch die Wiederholung der befahrenen Radwege die Anzahl der unterschiedlichen Oberflächen Typen zu reduzieren und somit eine bessere Klassifizierung zu ermöglichen. \n",
    "\n",
    "## Hypothese\n",
    "\n",
    "Durch Wiederholung bei der Datenerfassung sollte sich eine höhere accuracy erzielen lassen als im Faltungsnetz Versuch 4. Auf der befahrenen Route gibt es mehr Unebenheiten im Vergleich zu Datensatz 37 bis 42. Dadurch waren mehr Daten zum Trainieren verfügbar, weil mehr Daten mit hoher Erschütterung zur Verfügung standen.  \n",
    "\n",
    "## Versuchsaufbau\n",
    "\n",
    "Eine Route im Naturschutzgebiet Höltigbaum wurde 14 Mal befahren siehe Abb. 1. Dabei setzen sich die Trainingsdaten aus den Datensätzen Nummer 43 und 45 bis 51 zusammen.\n",
    "\n",
    "<img src=\"../daten/abbildungen/karteDatensatz43_45_bis_51.png\" />\n",
    "Abbildung 1: Höltigbaum Route 14 Mal wiederholt befahren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Versuch 5.1: 3 Fahrqualitätsklassen\n",
    "\n",
    "#### Versuchsaufbau\n",
    "\n",
    "Die Daten in diesem Versuch wurden in Fuzzy Logik Versuch 6 klassifiziert. Hierbei wurde unterschieden zwischen den 3 Klassen \"gut\", \"mittel\" und \"schlecht\" nach der Fahrqualität. Dieser Versuch ist eine Wiederholung des Faltungsnetz Versuchs 4.7 mit dem Unterschied, dass wie beschrieben andere Datensätze verwendet wurden. Es wurden 20 Epochen durchgeführt.  \n",
    "\n",
    "<img src=\"../daten/abbildungen/karteFuzzyVersuch6_datensatz43_45_bis_51.png\"  alt=\"Karte mit labeln aus Fuzzy Versuch 6\" />\n",
    "Abbildung 2: Längengrad und Breitengrad mit labeln aus Fuzzy Logik Versuch 6\n",
    "\n",
    "#### Ergebnis\n",
    "\n",
    "Wie auf Abb. 4 zu sehen sieht die Konfusionsmatrix der Testdaten ähnlich wie in Versuch 4.7 aus. Die Test Accuracy siehe Tab. 5.2 ist, um $2,6\\%$ höher. Eigentlich wurde eine deutlich höhere accuracy erwartet. Auf Abb. 3 ist die Trainingshistorie abgebildet. Durch erhöhen der Anzahl der Epochen könnte sich auch das Ergebnis verbessern.   \n",
    "  \n",
    "| | \n",
    " --- | --- |\n",
    "<img src=\"../daten/abbildungen/trainingshistorieAccuracyVersuch5_1.png\" alt=\"Trainingshistorie Accuracy von Versuch 5.1\" /> | <img src=\"../daten/abbildungen/trainingshistorieLossVersuch5_1.png\" alt=\"Trainingshistory Loss von Versuch 5.1\" /> \n",
    "Abbildung 3: Accuracy l. S. und Loss r. S. des Versuchs 5.1\n",
    "\n",
    "<img src=\"../daten/abbildungen/konfmatrixVersuch5_1.png\" alt=\"Konfusionmatrix von Versuch 5.1\" /> \n",
    "Abbildung 4: Konfusionmatrix von Versuch 5.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Versuch 5.2: Binäre Anzahl an Klassen\n",
    "\n",
    "### Versuchsaufbau\n",
    "\n",
    "In diesem Versuch wurde der Versuch 4.8 wiederholt nur mit den Datensätzen 43 und 45 bis 51. Auch hier wurde erwartet das die Test accuracy höher ist als im Versuch 4.8. Bei den Versuch gab es zwei Klassen \"gute\" oder \"schlechte\" Fahrqualität siehe Abb. 5.\n",
    "\n",
    "<img src=\"../daten/abbildungen/karteFuzzyVersuch6_2Klassen_datensatz43_45_bis_51.png\" alt=\"Karte mit 2 Klassen\" />\n",
    "Abbildung 5: Längengrad und Breitengrad Karte mit 2 Klassen\n",
    "\n",
    "### Ergebnis\n",
    "\n",
    "Auf Tab. 2 ist zu sehen, dass die Test Accuracy mit $87,5\\%$ deutlich höher ist als im Versuch 4.8 mit $78,7\\%$. Dies sollte daran liegen, dass die Anzahl der Trainingsdaten höher ist und eine Route wiederholt befahren  wurde. \n",
    "\n",
    "<img src=\"../daten/abbildungen/konfmatrixVersuch5_2.png\" alt=\"Konfusionmatrix von Versuch 5.2\" /> \n",
    "Abbildung 6: Konfusionsmatrix von Versuch 5.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Versuch 5.3: Hypertuning mit 3 Klassen\n",
    "\n",
    "#### Hypothese\n",
    "\n",
    "Die Hypothese war, dass durch anpassen der Parameter eine höhere Test accuracy erreicht werden kann als in Versuch 5.2 (siehe Tab. 2). \n",
    "\n",
    "#### Versuchsaufbau\n",
    "\n",
    "In diesem Versuch wurde nochmal nach bessere Parametern für das Modell aus Versuch 4.4 gesucht mit Hyperas. Die Daten wurden in Fuzzy Logik Versuch 6 gelabelt. Dabei wurde wie in Versuch 4.3 die Dropout Rate, die Anzahl der Faltungsschichten bei diesem Versuch von 3 bis 5 und die Optimierungsfunktion sowie weitere Parameter mit Hyperas gesucht. Die Anzahl an Durchläufe mit Hyperopt waren 77.\n",
    "\n",
    "#### Ergebnis\n",
    "\n",
    "Mit den gefundenen Parametern siehe Tab. 1 konnte die Test accuracy, um $2.92\\%$ optimiert werden. \n",
    "\n",
    "Schicht |             Ausgangsformat          | Anzahl der Parameter  |  \n",
    "       --    |          ---                   |          ---          |\n",
    "Conv2D                       | (368, 70, 32)  |          896          |       \n",
    "Activation(ELU)              | (368, 70, 32)  |          0            |   \n",
    "MaxPooling(2 * 2)            | (184, 35, 32)  |          0            |   \n",
    "Dropout (0.16)               | (184, 35, 32)  |          0            |   \n",
    "Conv2D                       | (184, 35, 32)  |         9248          |\n",
    "Activation(ELU)              | (184, 35, 32)  |          0            |\n",
    "MaxPooling                   | (92, 17, 32)   |          0            |\n",
    "Dropout(0.21)                | (92, 17, 32)   |          0            |\n",
    "Conv2D                       | (92, 17, 64)   |          18496        |    \n",
    "Activation(ELU)              | (92, 17, 64)   |          0            | \n",
    "MaxPooling                   | (46, 8, 64)    |          0            | \n",
    "Dropout(0.44)                | (46, 8, 64)    |          0            | \n",
    "Conv2D                       | (46, 8, 64)    |       36928           |\n",
    "Activation(ELU)              | (46, 8, 64)    |          0            |\n",
    "MaxPooling                   | (23, 4, 64)    |          0            |\n",
    "Dropout(0.13)                | (23, 4, 64)    |          0            | \n",
    "Conv2D                       | (23, 4, 64)    |       36928           |\n",
    "Activation(ELU)              | (23, 4, 64)    |          0            |\n",
    "MaxPooling                   | (11, 2, 64)    |          0            |\n",
    "Dropout(0.40)                | (11, 2, 64)    |          0            | \n",
    "Flatten                      | (1408)         |          0            | \n",
    "Dense                        | (64)           |         90176         |   \n",
    "Activation(ELU)              | (64)           |          0            |   \n",
    "Dropout(0.07)                | (64)           |          0            |         \n",
    "Dense                        | (3)            |          520          |       \n",
    "Activation(softmax)          | (3)            |          0            |\n",
    "Tabelle 1: Parameter des gefundenen Modells aus Versuch 5.3 mit der Optimierungsfunktion Adam und einer batch size von 32 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Versuch 5.4: Parameter aus 5.3 mit 100 Epochen \n",
    "\n",
    "#### Hypothese\n",
    "\n",
    "In Versuch 5.3 wurden jeweils nur 10 Epochen genutzt, um ein Faltungsnetz zu trainieren. Deshalb wurde untersucht ob sich die Test Accuracy erhöht indem die Anzahl der Epochen erhöht wurde.\n",
    "\n",
    "#### Versuchsaufbau\n",
    "\n",
    "Bei diesem Versuch wurde ein Faltungsnetz mit den Parametern aus Versuch 5.3 und den Klassen aus Fuzzy Logik Versuch 6 trainiert. Dabei wurden 100 Epochen ausgeführt.  \n",
    "\n",
    "#### Ergebnis\n",
    "\n",
    "In Tab. 2 ist zu sehen, dass mit Erhöhung der Anzahl der Epochen auf 100 mit dem besten Model die Test accuracy, um $1.3\\%$ angehoben werden konnte. Auf Abb. 7 l. S. ist zu erkennen, dass ab Epoche das Model überangepasst wurde. Wie auf Abb. 8 zu sehen wurden vor allen die Klassen \"gut\" und \"schlecht\" vorhergesagt.\n",
    "\n",
    "| | \n",
    " --- | --- |\n",
    "<img src=\"../daten/abbildungen/trainingshistorieAccuracyVersuch5_4.png\" alt=\"Trainingshistorie Accuracy von Versuch 5.4\" /> | <img src=\"../daten/abbildungen/trainingshistorieLossVersuch5_4.png\" alt=\"Trainingshistorie Loss von Versuch 5.4\" /> \n",
    "Abbildung 7: Accuracy l. S. und Loss r. S. des Versuchs 5.4\n",
    "\n",
    "<img src=\"../daten/abbildungen/konfmatrixVersuch5_4.png\" alt=\"Konfusionmatrix von Versuch 5.4\" /> \n",
    "Abbildung 8: Konfusionmatrix von Versuch 5.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Versuch 5.5: Test mit Daten aus Datensatz 37-42 \n",
    "\n",
    "#### Hypothese\n",
    "\n",
    "Mit den gefundenen Parametern aus Versuch 5.3 sollte sich auch die Test accuracy mit den Daten aus Datensatz 37 bis 42 steigern lassen. Die Annahme war, dass die Test accuracy höher sein wird als in Versuch 4.7 ($56.79\\%$). \n",
    "\n",
    "#### Versuchsaufbau\n",
    "\n",
    "In diesem Versuch wurde der Versuch 5.4 wiederholt mit den Datensätzen 37 bis 42. Hierbei wurden die Daten gelabelt in Fuzzy Logik Versuch 4.   \n",
    "\n",
    "#### Ergebnis\n",
    "\n",
    "Die Test Accuracy betrug $57.96\\%$ siehe Tab.2. Damit war diese nur $1.17\\%$ höher als im Versuch 4.7. \n",
    "Ein Blick auf die Trainingshistorie siehe Abb. 9 zeigt, dass das Faltungsnetz überangepasst war.\n",
    "\n",
    "| | \n",
    " --- | --- |\n",
    "<img src=\"../daten/abbildungen/trainingshistorieAccuracyVersuch5_5.png\" alt=\"Trainingshistorie Accuracy von Versuch 5.5\" /> | <img src=\"../daten/abbildungen/trainingshistorieLossVersuch5_5.png\" alt=\"Trainingshistorie Loss von Versuch 5.5\" /> \n",
    "Abbildung 9: Accuracy l. S. und Loss r. S. des Versuchs 5.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Versuch 5.6: Unterteilung der Geschwindigkeit \n",
    "\n",
    "#### Hypothese\n",
    "\n",
    "Dieser Versuch ist motiviert aus dem Ergebnis von Versuch 4.5. Ziel war es zwei Faltungsnetze zu trainieren für den Geschwindigkeitsbereich \"niedrig\" und \"hoch\". Die Erwartung war, dass die Test Accuracy und Loss Werte gering besser ausfallen als im Versuch 5.4.\n",
    "\n",
    "#### Versuchsaufbau\n",
    "\n",
    "Als Parameter für das Faltungsnetz wurden die aus Versuch 5.3 gewählt. Die Daten aus Datensatz 43, 45 bis 51 wurden in Fuzzy Versuch 7 gelabelt. Dabei wurden die Fahrqualitäten zusätzlich in \"niedriger\" und \"hoher\" Geschwindigkeit unterschieden. In Versuch 5.6.1 wurde ein Faltungsnetz für die Geschwindigkeit \"niedrig\" trainiert. Dann wurde in Versuch 5.6.2 ein Modell trainiert für den Geschwindigkeitsbereich \"hoch\".\n",
    " \n",
    "#### Ergebnis\n",
    "\n",
    "Wie auf Abb. 10 und 12 zu sehen wurde das Faltungsnetz überangepasst an den Trainingsdaten. In Versuch 5.6.1 konnte eine bessere Test Accuracy erreicht werden vergleiche mit Tab. 2 als im Versuch 5.4. Allerdings war der Loss der Kostenfunktion, um $0.1198$ höher. Die Testergebnisse aus Versuch 5.6.2 waren schlechter als in Versuch 5.4. Wahrscheinlich war die Anzahl der Trainingsdaten jeder Klasse zu gering durch die Unterteilung der Geschwindigkeit. \n",
    "\n",
    "| | \n",
    " --- | --- |\n",
    "<img src=\"../daten/abbildungen/trainingshistorieAccuracyVersuch5_6_1.png\" alt=\"Trainingshistorie Accuracy von Versuch 5.6.1\" /> | <img src=\"../daten/abbildungen/trainingshistorieLossVersuch5_6_1.png\" alt=\"Trainingshistorie Loss von Versuch 5.6.1\" /> \n",
    "Abbildung 10: Accuracy l. S. und Loss r. S. des Versuchs 5.6.1\n",
    "\n",
    "| | \n",
    " --- | --- |\n",
    "<img src=\"../daten/abbildungen/trainingshistorieAccuracyVersuch5_6_2.png\" alt=\"Trainingshistorie Accuracy von Versuch 5.6.2\" /> | <img src=\"../daten/abbildungen/trainingshistorieLossVersuch5_6_2.png\" alt=\"Trainingshistorie Loss von Versuch 5.6.2\" /> \n",
    "Abbildung 11: Accuracy l. S. und Loss r. S. des Versuchs 5.6.2\n",
    "\n",
    "| | \n",
    " --- | --- |\n",
    "<img src=\"../daten/abbildungen/konfmatrixVersuch5_6_1.png\" alt=\"Trainingshistorie Accuracy von Versuch 5.6.2\" /> | <img src=\"../daten/abbildungen/konfmatrixVersuch5_6_2.png\" alt=\"Trainingshistorie Loss von Versuch 5.6.2\" /> \n",
    "Abbildung 12: Konfusionmatrix der Versuche 5.6.1 und 5.6.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Versuch 5.7: one-versus-the-rest (eine gegen den Rest)\n",
    "\n",
    "#### Hypothese\n",
    "\n",
    "In diesem Versuch wurde untersucht, welches Ergebnis nach der Metrik Test accuracy und Loss erzielt werden kann, bei der Verwendung von der Methode one-versus-the-rest. \n",
    "\n",
    "#### Versuchsaufbau\n",
    "\n",
    "Die Methode one-versus-the-rest (Geron, Hands-On Machine Learning with Scikit-Learn & TensorFlow, O'Reilly, S. 94) also eine Klasse gegen die restlichen Klassen funktioniert nach dem Prinzip mehrere binäre Klassifikatoren zu trainieren. Für jede Klasse soll dabei ein Faltungsnetz trainiert werden. Das fertige System wird so genutzt, dass die Vorhersage jedes Modell verglichen wird und die Klasse vorhergesagt wird mit der höchsten Wahrscheinlichkeit nach der Softmax Funktion. In Versuch 5.7.1 wurde ein Modell trainiert, welches spezialisiert war auf die Klasse \"gut\". Darauf folgend wurde ein Faltungsnetz trainiert für die Klasse \"mittel\" und eines für die Klasse \"schlech\" in Versuch 5.7.2 sowie 5.7.3. Die Daten kamen aus dem Datensatz 43 und 45 bis 51. In Fuzzy Logik Versuch 6 wurden die Daten gelabelt. Den Testdaten wurden $20\\%$ der gleichen Daten für jeden Versuch zugewiesen. Getestet wurde dann mit allen Modellen.\n",
    "\n",
    "#### Ergebnis\n",
    "\n",
    "In Tab. 2 ist zu sehen, dass die Test Accuracy mit $63.09\\%$ nicht höher war als in Versuch 5.4 mit $63.61\\%$. Die Abb. 13 zeigt das vor allem die Klasse \"mittel\" falsch vorhergesagt wurde von den 3 Faltungsnetzen. Mit der gegebenen Zuordnung der Fahrqualität ist es für diese Aufgabe also nicht sinnvoll die Methode one-versus-the-rest zu verwenden.\n",
    "\n",
    "<img src=\"../daten/abbildungen/konfmatrixVersuch5_7.png\" alt=\"Konfusionsmatrix der Testdaten aus Versuch 5.7\" />\n",
    "Abbildung 13: Konfusionsmatrix der Testdaten aus Versuch 5.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Versuch 5.8: Anpassung der Erschütterungszuordnung nach der Analyse der Testdaten\n",
    "\n",
    "#### Einleitung \n",
    "\n",
    "Bei der Darstellung der Testergebnisse aus Datensatz 52, welche gelabelt wurden in Fuzzy Logik Versuch 6 traf die Erwartung der Oberflächen Gleichmäßigkeit für mehrere Wegabschnitte nicht zu. Hierzu siehe auch die Dokumentation zu Fuzzy Versuch 8. Aus diesem Grund wurde der Grenzwert angenommen für die Zuordnung der leichten Erschütterung.\n",
    "\n",
    "#### Hypothese\n",
    "\n",
    "Durch anpassen der Gleichmäßigkeit bzw. Fahrqualitätsklassen sollte sich die Test accuracy im Vergleich zu Versuch 5.4 höher sein.\n",
    "\n",
    "#### Versuchsaufbau\n",
    "\n",
    "Gelabelt wurden die Datensätze 43, 45 bis 51 in Fuzzy Logik Versuch 5. Für das Faltungsnetz \n",
    "wurde die Architektur aus Versuch 5.3 ausgewählt. Die Anzahl der Epochen betrug 100.\n",
    "\n",
    "#### Ergebnis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ergebnisse\n",
    "\n",
    "Versuch Nr. | Trainings accuracy | Trainings loss | Validierungs accuracy | Validierungs loss | Test accuracy | Test loss \n",
    "--- | --- | --- | --- | --- \n",
    "5.1   | $55.24\\%$ | $0.9188$ | $60.30\\%$ | $0.8369$ | $59.39\\%$ | $0.8477$\n",
    "5.2   | $84.26\\%$ | $0.3926$ | $87.66\\%$ | $0.3584$ | $87.5\\%$  | $0.3552$\n",
    "5.3   | $60.82\\%$ | $0.8173$ | $62.69\\%$ | $0.7961$ | $62.31\\%$ | $0.7944$\n",
    "5.4   | $62.99\\%$ | $0.7739$ | $65.36\\%$ | $0.7547$ | $63.61\\%$ | $0.7725$\n",
    "5.5   | $58.80\\%$ | $0.8370$ | $57.61\\%$ | $0.9308$ | $57.96\\%$ | $0.8937$ \n",
    "5.6.1 | $71.01\\%$ | $0.6366$ | $65.56\\%$ | $0.8501$ | $64.71\\%$ | $0.8923$\n",
    "5.6.2 | $63.86\\%$ | $0.7406$ | $60.90\\%$ | $0.8270$ | $60.53\\%$ | $0.8249$\n",
    "5.7.1 | $82.57\\%$ | $0.3961$ | $81.78\\%$ | $0.4652$ | $63.09\\%$\n",
    "5.7.2 | $51\\%$    | $0.6955$ | $51.66\\%$ | $0.6920$ | $63.09\\%$\n",
    "5.7.3 | $78.95\\%$ | $0.4456$ | $77.44\\%$ | $0.4459$ | $63.09\\%$\n",
    "5.8.1 | $60.36\\%$ | $0.8348$ | $61.77\\%$ | $0.9206$ | $61.64\\%$ | $0.8144$\n",
    "5.8.2 | $60.99\\%$ | $0.8250$ | $61.18\\%$ | $0.8365$ | $60.09\\%$ | $0.8319$ \n",
    "Tabelle 2: Ergebnisse ver Versuche  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zusammenfassung\n",
    "\n",
    "In diesem Versuch wurde die Hypothese untersucht, ob durch wiederholtes befahren einer Route eine bessere Klassifizierung möglich ist als in Versuch 4 siehe Kapitel 5. In ersten Teilversuch 5.1 wurde der Versuch 4.7 wiederholt nur mit den Daten aus Datensatz 43, 45 bis 51. Dabei wurde eine $2,6\\%$ höhere accuracy erreicht.\n",
    "Als nächstes wurde der Versuch 4.8 wiederholt siehe Versuch 5.2. Bei welchem nur die Klassen gute und schlechte Fahrqualität existierten. Mit $87,5\\%$ war die Test accuracy deutlich höher als im Versuch 4.8 mit $78,7\\%$ siehe Tab. 2. In Versuch 5.3 wurde mit Hypertuning nach besseren Parametern für das Faltungsnetz gesucht. Ein Faltungsnetzwerk wurde dann in versuch 5.4 mit 100 Epochen trainiert, welches 90 Epochen mehr als in Versuch 5.3 entsprechen. Dieses hatte dann eine Test accuracy von $63.61\\%$\n",
    "Mit den gefundenen Parametern aus Versuch 5.4 wurde ein Model trainiert in Versuch 5.5 mit den Daten aus Datensatz 37 bis 42. Die Erwartung, dass die Test accuracy höher als in Versuch 4.7 sein wird traf zu.\n",
    "In Versuch 5.6 wurden die Daten nochmal in zwei Geschwindigkeitsbereiche unterteilt mit nur mäßigen Ergebnis siehe Tab. 2. Schließlich wurde in Versuch 5.7 mit der Technik one-versus-the-rest experimentiert. Mit dieser konnte eine Test accuracy von $63.09\\%$ erzielt werden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fazit\n",
    "\n",
    "Mit den Versuchen konnte eine bessere Klassifizierung ermöglicht werden. Vergleiche hierzu Versuch 5.4 mit $63.61\\%$ Test accuracy mit Versuch 4.7 ($56.79\\%$). Eine im Vergleich zu den anderen Versuchen hohe Test accuracy konnte mit $87\\%$ in Versuch 5.2 erreicht werden. In dem nur zwischen den Klassen gute und schlechte Fahrqualität unterschieden wurde. Ein Nachteil gegenüber den Versuchen aus Kapitel 6 (Faltungsnetzwerk Versuch 4) ist das diese vermutlich nicht gut generalisieren können bei der Klassifizierung von unbekannten Radwegen.     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-12T15:54:35.865019Z",
     "start_time": "2018-07-12T15:54:29.327920Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from hyperopt import Trials, STATUS_OK, rand\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import uniform, choice\n",
    "import pandas\n",
    "import collections\n",
    "import seaborn as sns\n",
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as mlines\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-12T15:54:36.030826Z",
     "start_time": "2018-07-12T15:54:35.880583Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras import initializers\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.models import load_model\n",
    "import keras.callbacks as cb\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-12T15:54:51.298700Z",
     "start_time": "2018-07-12T15:54:49.143653Z"
    }
   },
   "outputs": [],
   "source": [
    "featuresDf = pandas.read_csv(filepath_or_buffer=\"../daten/merkmale_datensatz_43_45_bis_51/merkmaleMitLabelnFuzzyVersuch8.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-12T15:54:59.926926Z",
     "start_time": "2018-07-12T15:54:59.922929Z"
    }
   },
   "outputs": [],
   "source": [
    "# Nummer des aktuellen Versuchs\n",
    "experimentNumber = \"8_1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-12T15:55:29.202067Z",
     "start_time": "2018-07-12T15:55:29.198070Z"
    }
   },
   "outputs": [],
   "source": [
    "# Hier können die Datensätze ausgewählt werden\n",
    "datasets = ['43','45','46','47','48','49','50','51']\n",
    "# Die Pfade zu den Ordnern in welchem sich die Bilder befinden\n",
    "paths = []\n",
    "# Liste mit Pfaden zu den Bildern\n",
    "imagePaths = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-12T15:56:51.680205Z",
     "start_time": "2018-07-12T15:56:42.466464Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:/bachelor/daten/43/zugeschnitten/\n",
      "Bilder aus folgenden Ordnern werden geladen: ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10']\n",
      "Ordner der geladen wird: 1\n",
      "Ordner der geladen wird: 2\n",
      "Ordner der geladen wird: 3\n",
      "Ordner der geladen wird: 4\n",
      "Ordner der geladen wird: 5\n",
      "Ordner der geladen wird: 6\n",
      "Ordner der geladen wird: 7\n",
      "Ordner der geladen wird: 8\n",
      "Ordner der geladen wird: 9\n",
      "Ordner der geladen wird: 10\n",
      "E:/bachelor/daten/45/zugeschnitten/\n",
      "Bilder aus folgenden Ordnern werden geladen: ['1', '2', '3', '4', '5']\n",
      "Ordner der geladen wird: 1\n",
      "Ordner der geladen wird: 2\n",
      "Ordner der geladen wird: 3\n",
      "Ordner der geladen wird: 4\n",
      "Ordner der geladen wird: 5\n",
      "E:/bachelor/daten/46/zugeschnitten/\n",
      "Bilder aus folgenden Ordnern werden geladen: ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11']\n",
      "Ordner der geladen wird: 1\n",
      "Ordner der geladen wird: 2\n",
      "Ordner der geladen wird: 3\n",
      "Ordner der geladen wird: 4\n",
      "Ordner der geladen wird: 5\n",
      "Ordner der geladen wird: 6\n",
      "Ordner der geladen wird: 7\n",
      "Ordner der geladen wird: 8\n",
      "Ordner der geladen wird: 9\n",
      "Ordner der geladen wird: 10\n",
      "Ordner der geladen wird: 11\n",
      "E:/bachelor/daten/47/zugeschnitten/\n",
      "Bilder aus folgenden Ordnern werden geladen: ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10']\n",
      "Ordner der geladen wird: 1\n",
      "Ordner der geladen wird: 2\n",
      "Ordner der geladen wird: 3\n",
      "Ordner der geladen wird: 4\n",
      "Ordner der geladen wird: 5\n",
      "Ordner der geladen wird: 6\n",
      "Ordner der geladen wird: 7\n",
      "Ordner der geladen wird: 8\n",
      "Ordner der geladen wird: 9\n",
      "Ordner der geladen wird: 10\n",
      "E:/bachelor/daten/48/zugeschnitten/\n",
      "Bilder aus folgenden Ordnern werden geladen: ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10']\n",
      "Ordner der geladen wird: 1\n",
      "Ordner der geladen wird: 2\n",
      "Ordner der geladen wird: 3\n",
      "Ordner der geladen wird: 4\n",
      "Ordner der geladen wird: 5\n",
      "Ordner der geladen wird: 6\n",
      "Ordner der geladen wird: 7\n",
      "Ordner der geladen wird: 8\n",
      "Ordner der geladen wird: 9\n",
      "Ordner der geladen wird: 10\n",
      "E:/bachelor/daten/49/zugeschnitten/\n",
      "Bilder aus folgenden Ordnern werden geladen: ['1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
      "Ordner der geladen wird: 1\n",
      "Ordner der geladen wird: 2\n",
      "Ordner der geladen wird: 3\n",
      "Ordner der geladen wird: 4\n",
      "Ordner der geladen wird: 5\n",
      "Ordner der geladen wird: 6\n",
      "Ordner der geladen wird: 7\n",
      "Ordner der geladen wird: 8\n",
      "Ordner der geladen wird: 9\n",
      "E:/bachelor/daten/50/zugeschnitten/\n",
      "Bilder aus folgenden Ordnern werden geladen: ['1', '2', '3', '4', '5']\n",
      "Ordner der geladen wird: 1\n",
      "Ordner der geladen wird: 2\n",
      "Ordner der geladen wird: 3\n",
      "Ordner der geladen wird: 4\n",
      "Ordner der geladen wird: 5\n",
      "E:/bachelor/daten/51/zugeschnitten/\n",
      "Bilder aus folgenden Ordnern werden geladen: ['1', '2', '3', '4', '5']\n",
      "Ordner der geladen wird: 1\n",
      "Ordner der geladen wird: 2\n",
      "Ordner der geladen wird: 3\n",
      "Ordner der geladen wird: 4\n",
      "Ordner der geladen wird: 5\n"
     ]
    }
   ],
   "source": [
    "for dataset in datasets: # Für jeden Datensatz merke Pfad\n",
    "    paths.append(\"E:/bachelor/daten/\" + dataset + \"/zugeschnitten/\")\n",
    "for path in paths: # Für jeden Pfad hole die Namen der Ordner\n",
    "    folders = os.listdir(path)\n",
    "    folders = sorted(folders, key=int) #sortiert die Reihenfolge de Ordner aufsteifend\n",
    "    print(path)\n",
    "    print(\"Bilder aus folgenden Ordnern werden geladen: \" + str(folders))\n",
    "    for folder in folders: # Aus der Liste der Ordner wird ein Ordner ausgewählt\n",
    "        filesPath = path + folder + \"/\"\n",
    "        files = os.listdir(filesPath)\n",
    "        print(\"Ordner der geladen wird: \" + str(folder))\n",
    "        for name in files: # Ein Dateiname aus diesem Ordner\n",
    "            if \"jpg\" not in name:\n",
    "                continue\n",
    "            imagePaths.append(filesPath + name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-12T15:57:05.779608Z",
     "start_time": "2018-07-12T15:57:05.775610Z"
    }
   },
   "outputs": [],
   "source": [
    "classNames = ['gut','mittel','schlecht'] # Namen der Klassen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-12T15:58:28.574606Z",
     "start_time": "2018-07-12T15:58:28.566609Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    49073\n",
       "0    44335\n",
       "2    26785\n",
       "Name: Klasse, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featuresDf['Klasse'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresDf = featuresDf[featuresDf['Klasse'] != 1]\n",
    "featuresDf[featuresDf['Klasse'] == 2] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-12T15:57:39.772752Z",
     "start_time": "2018-07-12T15:57:39.746945Z"
    }
   },
   "outputs": [],
   "source": [
    "featuresDf = featuresDf[featuresDf['Klasse'] > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Klassen auf die Zahlen 0-2 normieren \n",
    "df = featuresDf['Klasse'] - 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-12T15:57:45.263887Z",
     "start_time": "2018-07-12T15:57:45.251570Z"
    }
   },
   "outputs": [],
   "source": [
    "yLabels = np_utils.to_categorical(featuresDf['Klasse'], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-12T15:57:47.412169Z",
     "start_time": "2018-07-12T15:57:47.389924Z"
    }
   },
   "outputs": [],
   "source": [
    "# Setzten des RandomState um reproduzierbare Ergebnisse zu erzielen.\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-12T15:57:47.677709Z",
     "start_time": "2018-07-12T15:57:47.657458Z"
    }
   },
   "outputs": [],
   "source": [
    "# Mischen der Trainingsdaten\n",
    "xShuffle, yShuffle = shuffle(imagePaths,yLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-12T15:58:20.912001Z",
     "start_time": "2018-07-12T15:57:51.392338Z"
    }
   },
   "outputs": [],
   "source": [
    "# Die Zelle Normiert die Anzahl der Repräsentanten pro Klasse\n",
    "class1Number = 0\n",
    "class2Number = 0 \n",
    "class3Number = 0\n",
    "maxClasses = featuresDf[\"Klasse\"].value_counts().min()\n",
    "indexToDelete = [] \n",
    "i = -1\n",
    "for label in yShuffle:\n",
    "    i = i + 1\n",
    "    labelNumber = np.argmax(label,axis=0)\n",
    "    if labelNumber == 0 and class1Number < maxClasses:\n",
    "        class1Number = class1Number + 1\n",
    "        continue\n",
    "    elif labelNumber == 0:\n",
    "        indexToDelete.append(i)\n",
    "    if labelNumber == 1 and class2Number < maxClasses:\n",
    "        class2Number = class2Number + 1\n",
    "        continue\n",
    "    elif labelNumber == 1:\n",
    "        indexToDelete.append(i)\n",
    "    if labelNumber == 2 and class3Number < maxClasses:\n",
    "        class3Number = class3Number + 1\n",
    "        continue\n",
    "    elif labelNumber == 2:\n",
    "        indexToDelete.append(i)\n",
    "    if labelNumber == 3 and class4Number < maxClasses:\n",
    "        class4Number = class4Number + 1\n",
    "        continue        \n",
    "xShuffle = [i for j, i in enumerate(xShuffle) if j not in indexToDelete]\n",
    "yShuffle = [i for j, i in enumerate(yShuffle) if j not in indexToDelete]\n",
    "yShuffle = np.asarray(yShuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-12T15:58:37.215744Z",
     "start_time": "2018-07-12T15:58:37.199738Z"
    }
   },
   "outputs": [],
   "source": [
    "# Mischen der Trainingsdaten\n",
    "xShuffle, yShuffle = shuffle(xShuffle,yShuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-12T15:58:37.721254Z",
     "start_time": "2018-07-12T15:58:37.705259Z"
    }
   },
   "outputs": [],
   "source": [
    "# Aufteilung in Trainings, Validation und Testdaten\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(xShuffle, yShuffle, test_size=0.10)\n",
    "xTrain, xVal, yTrain, yVal = train_test_split(xTrain, yTrain, test_size=0.20) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-12T15:58:38.216951Z",
     "start_time": "2018-07-12T15:58:38.204957Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 26785, 1: 26785, 2: 26785})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collections.Counter(np.argmax(yShuffle, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-12T15:58:41.625200Z",
     "start_time": "2018-07-12T15:58:41.607665Z"
    }
   },
   "outputs": [],
   "source": [
    "# Diese Funktion läd Bilder in den Hauptspeicher\n",
    "# imagesPaths: Liste mit Pfaden zu den Bildern != null\n",
    "def imageLoader(imagePaths):\n",
    "    images = []\n",
    "    for path in imagePaths:\n",
    "        images.append(cv2.cvtColor(cv2.imread(path),cv2.COLOR_BGR2RGB))\n",
    "    imagesNp = np.array(images)\n",
    "    imagesNp = imagesNp.astype('float32')\n",
    "    # Transfomierung der Bildpunkte auf den Wetebereich von 0 bis 1\n",
    "    imagesNp /= 255\n",
    "    return imagesNp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-12T15:58:41.845940Z",
     "start_time": "2018-07-12T15:58:41.827191Z"
    }
   },
   "outputs": [],
   "source": [
    "# Läd Trainingsdaten in batches\n",
    "def dataLoader(imagePaths, features, batchSize):\n",
    "    imagesCount = len(imagePaths)  \n",
    "    while True:\n",
    "        batchStart = 0\n",
    "        batchEnd = batchSize\n",
    "        while batchStart < imagesCount:\n",
    "            limit = min(batchEnd, imagesCount)\n",
    "            x = imageLoader(imagePaths[batchStart:limit])\n",
    "            y = features[batchStart:limit]\n",
    "            yield (x,y) \n",
    "            batchStart += batchSize   \n",
    "            batchEnd += batchSize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-12T15:58:44.621274Z",
     "start_time": "2018-07-12T15:58:44.606168Z"
    }
   },
   "outputs": [],
   "source": [
    "# Parameter für das CNN\n",
    "inputShape     = (368, 70, 3)   # Eingangs Array-Form \n",
    "numNeuronsC1   = 32                # Anzahl der Filter / 1 Faltungsschicht\n",
    "numNeuronsC2   = 32                # Anzahl der Filter / 2 Faltungsschicht\n",
    "numNeuronsC3   = 64                # Anzahl der Filter / 3 Faltungsschicht\n",
    "numNeuronsC4   = 64\n",
    "numNeuronsC5   = 64\n",
    "numNeuronsD1   = 64                # Anzahl der Neuronen des Fully connected layer - vollverbundene Schicht\n",
    "poolSize       = 2                 # Größe der Pooling-Layer\n",
    "convKernelSize = 3                 # Größe des Faltungskern n*n\n",
    "batchSize      = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-12T17:05:13.375094Z",
     "start_time": "2018-07-12T15:58:54.678705Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1078/1078 [==============================] - 790s 733ms/step - loss: 1.0633 - acc: 0.4198 - val_loss: 1.0434 - val_acc: 0.4336\n",
      "Epoch 2/100\n",
      "1078/1078 [==============================] - 123s 114ms/step - loss: 1.0252 - acc: 0.4589 - val_loss: 1.0901 - val_acc: 0.4291\n",
      "Epoch 3/100\n",
      "1078/1078 [==============================] - 122s 113ms/step - loss: 0.9991 - acc: 0.4810 - val_loss: 0.9515 - val_acc: 0.5159\n",
      "Epoch 4/100\n",
      "1078/1078 [==============================] - 124s 115ms/step - loss: 0.9485 - acc: 0.5222 - val_loss: 0.9203 - val_acc: 0.5585\n",
      "Epoch 5/100\n",
      "1078/1078 [==============================] - 122s 114ms/step - loss: 0.9170 - acc: 0.5475 - val_loss: 0.9059 - val_acc: 0.5700\n",
      "Epoch 6/100\n",
      "1078/1078 [==============================] - 122s 113ms/step - loss: 0.9073 - acc: 0.5553 - val_loss: 0.8988 - val_acc: 0.5704\n",
      "Epoch 7/100\n",
      "1078/1078 [==============================] - 122s 113ms/step - loss: 0.8939 - acc: 0.5686 - val_loss: 0.8983 - val_acc: 0.5718\n",
      "Epoch 8/100\n",
      "1078/1078 [==============================] - 123s 114ms/step - loss: 0.8882 - acc: 0.5744 - val_loss: 0.9061 - val_acc: 0.5678o - ETA: 2s - loss\n",
      "Epoch 9/100\n",
      "1078/1078 [==============================] - 125s 116ms/step - loss: 0.8796 - acc: 0.5789 - val_loss: 0.8764 - val_acc: 0.5807\n",
      "Epoch 10/100\n",
      "1078/1078 [==============================] - 126s 117ms/step - loss: 0.8738 - acc: 0.5843 - val_loss: 0.8577 - val_acc: 0.5961\n",
      "Epoch 11/100\n",
      "1078/1078 [==============================] - 122s 113ms/step - loss: 0.8674 - acc: 0.5877 - val_loss: 0.8834 - val_acc: 0.5923\n",
      "Epoch 12/100\n",
      "1078/1078 [==============================] - 121s 112ms/step - loss: 0.8631 - acc: 0.5899 - val_loss: 0.8647 - val_acc: 0.5948\n",
      "Epoch 13/100\n",
      "1078/1078 [==============================] - 122s 113ms/step - loss: 0.8590 - acc: 0.5881 - val_loss: 0.8628 - val_acc: 0.5972\n",
      "Epoch 14/100\n",
      "1078/1078 [==============================] - 122s 113ms/step - loss: 0.8574 - acc: 0.5937 - val_loss: 0.8691 - val_acc: 0.5925\n",
      "Epoch 15/100\n",
      "1078/1078 [==============================] - 122s 113ms/step - loss: 0.8563 - acc: 0.5917 - val_loss: 0.8724 - val_acc: 0.5856\n",
      "Epoch 16/100\n",
      "1078/1078 [==============================] - 122s 113ms/step - loss: 0.8505 - acc: 0.5968 - val_loss: 0.9137 - val_acc: 0.5825\n",
      "Epoch 17/100\n",
      "1078/1078 [==============================] - 122s 113ms/step - loss: 0.8478 - acc: 0.5978 - val_loss: 0.8550 - val_acc: 0.5979\n",
      "Epoch 18/100\n",
      "1078/1078 [==============================] - 121s 113ms/step - loss: 0.8451 - acc: 0.6019 - val_loss: 0.8795 - val_acc: 0.5850\n",
      "Epoch 19/100\n",
      "1078/1078 [==============================] - 121s 112ms/step - loss: 0.8448 - acc: 0.5979 - val_loss: 0.8637 - val_acc: 0.6030\n",
      "Epoch 20/100\n",
      "1078/1078 [==============================] - 122s 113ms/step - loss: 0.8417 - acc: 0.6003 - val_loss: 0.8889 - val_acc: 0.5771\n",
      "Epoch 21/100\n",
      "1078/1078 [==============================] - 122s 113ms/step - loss: 0.8392 - acc: 0.6035 - val_loss: 0.8747 - val_acc: 0.5920 - loss: 0.8388 - acc: 0.6 - ETA: 2s - loss\n",
      "Epoch 22/100\n",
      "1078/1078 [==============================] - 122s 113ms/step - loss: 0.8384 - acc: 0.6031 - val_loss: 0.8625 - val_acc: 0.5990\n",
      "Epoch 23/100\n",
      "1078/1078 [==============================] - 124s 115ms/step - loss: 0.8383 - acc: 0.6029 - val_loss: 0.9077 - val_acc: 0.5720\n",
      "Epoch 24/100\n",
      "1078/1078 [==============================] - 125s 116ms/step - loss: 0.8376 - acc: 0.6031 - val_loss: 0.8949 - val_acc: 0.5668\n",
      "Epoch 25/100\n",
      "1078/1078 [==============================] - 123s 114ms/step - loss: 0.8360 - acc: 0.6027 - val_loss: 0.8700 - val_acc: 0.5811\n",
      "Epoch 26/100\n",
      "1078/1078 [==============================] - 124s 115ms/step - loss: 0.8377 - acc: 0.6023 - val_loss: 0.9313 - val_acc: 0.5438\n",
      "Epoch 27/100\n",
      "1078/1078 [==============================] - 126s 117ms/step - loss: 0.8352 - acc: 0.6077 - val_loss: 0.8919 - val_acc: 0.57580s - loss: 0.8357 - acc: \n",
      "Epoch 28/100\n",
      "1078/1078 [==============================] - 127s 118ms/step - loss: 0.8337 - acc: 0.6050 - val_loss: 0.8894 - val_acc: 0.5616\n",
      "Epoch 29/100\n",
      "1078/1078 [==============================] - 127s 118ms/step - loss: 0.8372 - acc: 0.6045 - val_loss: 0.8712 - val_acc: 0.5764\n",
      "Epoch 30/100\n",
      "1078/1078 [==============================] - 126s 117ms/step - loss: 0.8367 - acc: 0.5980 - val_loss: 0.8682 - val_acc: 0.5864\n",
      "Epoch 31/100\n",
      "1078/1078 [==============================] - 123s 115ms/step - loss: 0.8318 - acc: 0.6064 - val_loss: 0.8640 - val_acc: 0.5740\n",
      "Epoch 32/100\n",
      "1078/1078 [==============================] - 124s 115ms/step - loss: 0.8292 - acc: 0.6067 - val_loss: 0.8697 - val_acc: 0.5782 loss:\n",
      "Epoch 33/100\n",
      "1078/1078 [==============================] - 123s 115ms/step - loss: 0.8284 - acc: 0.6076 - val_loss: 0.8680 - val_acc: 0.5918\n",
      "Epoch 34/100\n",
      "1078/1078 [==============================] - 124s 115ms/step - loss: 0.8265 - acc: 0.6083 - val_loss: 0.8706 - val_acc: 0.5927cc: 0.\n",
      "Epoch 35/100\n",
      "1078/1078 [==============================] - 123s 115ms/step - loss: 0.8315 - acc: 0.6057 - val_loss: 0.9149 - val_acc: 0.5564\n",
      "Epoch 36/100\n",
      "1078/1078 [==============================] - 123s 114ms/step - loss: 0.8312 - acc: 0.6040 - val_loss: 0.8698 - val_acc: 0.5891\n",
      "Epoch 37/100\n",
      "1078/1078 [==============================] - 124s 115ms/step - loss: 0.8308 - acc: 0.6087 - val_loss: 0.8753 - val_acc: 0.5918 - loss: 0.8309 - acc: - ETA\n",
      "Epoch 38/100\n",
      "1078/1078 [==============================] - 124s 115ms/step - loss: 0.8327 - acc: 0.6024 - val_loss: 0.8865 - val_acc: 0.5883\n",
      "Epoch 39/100\n",
      "1078/1078 [==============================] - 124s 115ms/step - loss: 0.8267 - acc: 0.6076 - val_loss: 0.8913 - val_acc: 0.58410.8267 - acc: 0 - ETA: 2s - lo\n",
      "Epoch 40/100\n",
      "1078/1078 [==============================] - 124s 115ms/step - loss: 0.8286 - acc: 0.6072 - val_loss: 0.8772 - val_acc: 0.5962\n",
      "Epoch 41/100\n",
      "1078/1078 [==============================] - 124s 115ms/step - loss: 0.8332 - acc: 0.6048 - val_loss: 0.8885 - val_acc: 0.5915\n",
      "Epoch 42/100\n",
      "1078/1078 [==============================] - 124s 115ms/step - loss: 0.8323 - acc: 0.6052 - val_loss: 0.8972 - val_acc: 0.5830\n",
      "Epoch 43/100\n",
      "1078/1078 [==============================] - 124s 115ms/step - loss: 0.8269 - acc: 0.6066 - val_loss: 0.8820 - val_acc: 0.5794\n",
      "Epoch 44/100\n",
      "1078/1078 [==============================] - 124s 115ms/step - loss: 0.8289 - acc: 0.6058 - val_loss: 0.9026 - val_acc: 0.58030.6\n",
      "Epoch 45/100\n",
      "1078/1078 [==============================] - 127s 118ms/step - loss: 0.8276 - acc: 0.6056 - val_loss: 0.8665 - val_acc: 0.5982\n",
      "Epoch 46/100\n",
      "1078/1078 [==============================] - 126s 117ms/step - loss: 0.8299 - acc: 0.6078 - val_loss: 0.8814 - val_acc: 0.5948\n",
      "Epoch 47/100\n",
      "1078/1078 [==============================] - 126s 117ms/step - loss: 0.8295 - acc: 0.6082 - val_loss: 0.8682 - val_acc: 0.5826\n",
      "Epoch 48/100\n",
      "1078/1078 [==============================] - 125s 116ms/step - loss: 0.8279 - acc: 0.6080 - val_loss: 0.8670 - val_acc: 0.6013\n",
      "Epoch 49/100\n",
      "1078/1078 [==============================] - 126s 117ms/step - loss: 0.8289 - acc: 0.6044 - val_loss: 0.8619 - val_acc: 0.6019\n",
      "Epoch 50/100\n",
      "1078/1078 [==============================] - 126s 117ms/step - loss: 0.8306 - acc: 0.6078 - val_loss: 0.9226 - val_acc: 0.5820\n",
      "Epoch 51/100\n",
      "1078/1078 [==============================] - 126s 117ms/step - loss: 0.8245 - acc: 0.6111 - val_loss: 0.8470 - val_acc: 0.6068\n",
      "Epoch 52/100\n",
      "1078/1078 [==============================] - 126s 117ms/step - loss: 0.8251 - acc: 0.6081 - val_loss: 0.8590 - val_acc: 0.6032\n",
      "Epoch 53/100\n",
      "1078/1078 [==============================] - 129s 119ms/step - loss: 0.8228 - acc: 0.6071 - val_loss: 0.8743 - val_acc: 0.5850\n",
      "Epoch 54/100\n",
      "1078/1078 [==============================] - 125s 116ms/step - loss: 0.8248 - acc: 0.6088 - val_loss: 0.9200 - val_acc: 0.5855\n",
      "Epoch 55/100\n",
      "1078/1078 [==============================] - 125s 116ms/step - loss: 0.8258 - acc: 0.6073 - val_loss: 0.8587 - val_acc: 0.5969\n",
      "Epoch 56/100\n",
      "1078/1078 [==============================] - 125s 116ms/step - loss: 0.8297 - acc: 0.6056 - val_loss: 0.8644 - val_acc: 0.5847: 7s - l - ETA: 4s - loss: 0.8289 - a\n",
      "Epoch 57/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1078/1078 [==============================] - 125s 116ms/step - loss: 0.8274 - acc: 0.6069 - val_loss: 0.9084 - val_acc: 0.5739\n",
      "Epoch 58/100\n",
      "1078/1078 [==============================] - 124s 115ms/step - loss: 0.8295 - acc: 0.6017 - val_loss: 0.8734 - val_acc: 0.59378285 - ac - ETA: 1s - loss: 0.8293 -\n",
      "Epoch 59/100\n",
      "1078/1078 [==============================] - 124s 115ms/step - loss: 0.8289 - acc: 0.6061 - val_loss: 0.8878 - val_acc: 0.5735\n",
      "Epoch 60/100\n",
      "1078/1078 [==============================] - 125s 116ms/step - loss: 0.8232 - acc: 0.6075 - val_loss: 0.8661 - val_acc: 0.58701s - loss: 0.8225 - acc: 0.608 - ETA: 1s - loss: 0.82\n",
      "Epoch 61/100\n",
      "1078/1078 [==============================] - 124s 115ms/step - loss: 0.8242 - acc: 0.6093 - val_loss: 0.8764 - val_acc: 0.5736\n",
      "Epoch 62/100\n",
      "1078/1078 [==============================] - 124s 115ms/step - loss: 0.8262 - acc: 0.6073 - val_loss: 0.8457 - val_acc: 0.6026\n",
      "Epoch 63/100\n",
      "1078/1078 [==============================] - 125s 116ms/step - loss: 0.8269 - acc: 0.6082 - val_loss: 0.8682 - val_acc: 0.5955\n",
      "Epoch 64/100\n",
      "1078/1078 [==============================] - 125s 116ms/step - loss: 0.8275 - acc: 0.6047 - val_loss: 0.8515 - val_acc: 0.5956\n",
      "Epoch 65/100\n",
      "1078/1078 [==============================] - 125s 116ms/step - loss: 0.8272 - acc: 0.6066 - val_loss: 0.8598 - val_acc: 0.5827\n",
      "Epoch 66/100\n",
      "1078/1078 [==============================] - 125s 116ms/step - loss: 0.8309 - acc: 0.6037 - val_loss: 0.8554 - val_acc: 0.5910\n",
      "Epoch 67/100\n",
      "1078/1078 [==============================] - 125s 116ms/step - loss: 0.8272 - acc: 0.6045 - val_loss: 0.8570 - val_acc: 0.5912\n",
      "Epoch 68/100\n",
      "1078/1078 [==============================] - 125s 116ms/step - loss: 0.8272 - acc: 0.6068 - val_loss: 0.8451 - val_acc: 0.5978oss: 0.8273 - acc: 0.60 - ETA: 7s -  - ETA: 4s - loss: 0.8271 - acc -\n",
      "Epoch 69/100\n",
      "1078/1078 [==============================] - 125s 116ms/step - loss: 0.8260 - acc: 0.6062 - val_loss: 0.8505 - val_acc: 0.5983\n",
      "Epoch 70/100\n",
      "1078/1078 [==============================] - 125s 116ms/step - loss: 0.8303 - acc: 0.6048 - val_loss: 0.8641 - val_acc: 0.5963\n",
      "Epoch 71/100\n",
      "1078/1078 [==============================] - 125s 116ms/step - loss: 0.8232 - acc: 0.6074 - val_loss: 0.8413 - val_acc: 0.6078\n",
      "Epoch 72/100\n",
      "1078/1078 [==============================] - 125s 116ms/step - loss: 0.8260 - acc: 0.6067 - val_loss: 0.8727 - val_acc: 0.5879\n",
      "Epoch 73/100\n",
      "1078/1078 [==============================] - 125s 116ms/step - loss: 0.8287 - acc: 0.6074 - val_loss: 0.8538 - val_acc: 0.5987\n",
      "Epoch 74/100\n",
      "1078/1078 [==============================] - 125s 116ms/step - loss: 0.8281 - acc: 0.6069 - val_loss: 0.8693 - val_acc: 0.5897oss: 0.8278 - acc: \n",
      "Epoch 75/100\n",
      "1078/1078 [==============================] - 125s 116ms/step - loss: 0.8258 - acc: 0.6081 - val_loss: 0.8717 - val_acc: 0.5916\n",
      "Epoch 76/100\n",
      "1078/1078 [==============================] - 125s 116ms/step - loss: 0.8260 - acc: 0.6066 - val_loss: 0.8857 - val_acc: 0.5849\n",
      "Epoch 77/100\n",
      "1078/1078 [==============================] - 125s 116ms/step - loss: 0.8274 - acc: 0.6072 - val_loss: 0.8536 - val_acc: 0.60155s - loss: 0.8263 - acc: 0.60 - ETA: 5s - loss: - ETA\n",
      "Epoch 78/100\n",
      "1078/1078 [==============================] - 125s 116ms/step - loss: 0.8247 - acc: 0.6077 - val_loss: 0.8517 - val_acc: 0.6013\n",
      "Epoch 79/100\n",
      "1078/1078 [==============================] - 125s 116ms/step - loss: 0.8257 - acc: 0.6074 - val_loss: 0.8599 - val_acc: 0.6057\n",
      "Epoch 80/100\n",
      "1078/1078 [==============================] - 125s 116ms/step - loss: 0.8303 - acc: 0.6019 - val_loss: 0.8493 - val_acc: 0.5996\n",
      "Epoch 81/100\n",
      "1078/1078 [==============================] - 125s 116ms/step - loss: 0.8258 - acc: 0.6062 - val_loss: 0.8882 - val_acc: 0.5801\n",
      "Epoch 82/100\n",
      "1078/1078 [==============================] - 125s 116ms/step - loss: 0.8264 - acc: 0.6052 - val_loss: 0.8718 - val_acc: 0.5857\n",
      "Epoch 83/100\n",
      "1078/1078 [==============================] - 125s 116ms/step - loss: 0.8271 - acc: 0.6060 - val_loss: 0.8710 - val_acc: 0.6017oss: 0.8270 - acc:\n",
      "Epoch 84/100\n",
      "1078/1078 [==============================] - 125s 116ms/step - loss: 0.8260 - acc: 0.6071 - val_loss: 0.8577 - val_acc: 0.6091\n",
      "Epoch 85/100\n",
      "1078/1078 [==============================] - 126s 116ms/step - loss: 0.8230 - acc: 0.6026 - val_loss: 0.8576 - val_acc: 0.5965 - lo\n",
      "Epoch 86/100\n",
      "1078/1078 [==============================] - 125s 116ms/step - loss: 0.8243 - acc: 0.6086 - val_loss: 0.9362 - val_acc: 0.5927\n",
      "Epoch 87/100\n",
      "1078/1078 [==============================] - 126s 117ms/step - loss: 0.8242 - acc: 0.6066 - val_loss: 0.8970 - val_acc: 0.59425s - - ETA: 2s \n",
      "Epoch 88/100\n",
      "1078/1078 [==============================] - 126s 117ms/step - loss: 0.8269 - acc: 0.6043 - val_loss: 0.8869 - val_acc: 0.5970\n",
      "Epoch 89/100\n",
      "1078/1078 [==============================] - 126s 117ms/step - loss: 0.8268 - acc: 0.6051 - val_loss: 0.8807 - val_acc: 0.5969\n",
      "Epoch 90/100\n",
      "1078/1078 [==============================] - 126s 117ms/step - loss: 0.8249 - acc: 0.6051 - val_loss: 0.8823 - val_acc: 0.5955s:  - ETA: 3s - loss: 0.8243 - acc: 0 - ETA: 2s - l\n",
      "Epoch 91/100\n",
      "1078/1078 [==============================] - 126s 117ms/step - loss: 0.8276 - acc: 0.6063 - val_loss: 0.8634 - val_acc: 0.6025- l\n",
      "Epoch 92/100\n",
      "1078/1078 [==============================] - 126s 117ms/step - loss: 0.8271 - acc: 0.6019 - val_loss: 0.8393 - val_acc: 0.6060.8269  - ETA: 5s - loss: \n",
      "Epoch 93/100\n",
      "1078/1078 [==============================] - 126s 117ms/step - loss: 0.8318 - acc: 0.6021 - val_loss: 0.8637 - val_acc: 0.6051- loss: 0 - ETA: 4s - los - ETA: 1s - loss: 0.8314 - acc: 0. - ETA: 1s - loss: 0.8316 - acc: - ETA: 0s - loss: 0.8320 - acc\n",
      "Epoch 94/100\n",
      "1078/1078 [==============================] - 126s 117ms/step - loss: 0.8300 - acc: 0.6056 - val_loss: 0.8449 - val_acc: 0.6113\n",
      "Epoch 95/100\n",
      "1078/1078 [==============================] - 129s 119ms/step - loss: 0.8250 - acc: 0.6099 - val_loss: 0.8365 - val_acc: 0.6118\n",
      "Epoch 96/100\n",
      "1078/1078 [==============================] - 129s 120ms/step - loss: 0.8250 - acc: 0.6065 - val_loss: 0.8539 - val_acc: 0.6036\n",
      "Epoch 97/100\n",
      "1078/1078 [==============================] - 126s 117ms/step - loss: 0.8233 - acc: 0.6046 - val_loss: 0.8380 - val_acc: 0.6104\n",
      "Epoch 98/100\n",
      "1078/1078 [==============================] - 127s 118ms/step - loss: 0.8249 - acc: 0.6066 - val_loss: 0.8515 - val_acc: 0.5910\n",
      "Epoch 99/100\n",
      "1078/1078 [==============================] - 129s 119ms/step - loss: 0.9098 - acc: 0.5550 - val_loss: 1.0432 - val_acc: 0.5180\n",
      "Epoch 100/100\n",
      "1078/1078 [==============================] - 130s 121ms/step - loss: 0.8674 - acc: 0.5828 - val_loss: 0.8677 - val_acc: 0.5979\n",
      "Test score: 0.86533254745\n",
      "Test accuracy: 0.592491610738\n"
     ]
    }
   ],
   "source": [
    "modelCNN = Sequential()\n",
    "modelCNN.add(Conv2D(numNeuronsC1, (convKernelSize, convKernelSize), padding='same', input_shape=inputShape,kernel_initializer=initializers.glorot_uniform(seed=42)))\n",
    "modelCNN.add(Activation(\"elu\"))\n",
    "modelCNN.add(MaxPooling2D(pool_size=(poolSize, poolSize)))\n",
    "modelCNN.add(Dropout(0.16))\n",
    "modelCNN.add(Conv2D(numNeuronsC2, (convKernelSize, convKernelSize), padding='same', kernel_initializer=initializers.glorot_uniform(seed=42)))\n",
    "modelCNN.add(Activation(\"elu\"))\n",
    "modelCNN.add(MaxPooling2D(pool_size=(poolSize, poolSize)))\n",
    "modelCNN.add(Dropout(0.21))\n",
    "modelCNN.add(Conv2D(numNeuronsC3, (convKernelSize, convKernelSize), padding='same', kernel_initializer=initializers.glorot_uniform(seed=42)))\n",
    "modelCNN.add(Activation(\"elu\"))\n",
    "modelCNN.add(MaxPooling2D(pool_size=(poolSize, poolSize)))       \n",
    "modelCNN.add(Dropout(0.44))\n",
    "modelCNN.add(Conv2D(numNeuronsC4, (convKernelSize, convKernelSize), padding='same', kernel_initializer=initializers.glorot_uniform(seed=42)))\n",
    "modelCNN.add(Activation(\"elu\"))\n",
    "modelCNN.add(MaxPooling2D(pool_size=(poolSize, poolSize)))       \n",
    "modelCNN.add(Dropout(0.13))\n",
    "modelCNN.add(Conv2D(numNeuronsC5, (convKernelSize, convKernelSize), padding='same', kernel_initializer=initializers.glorot_uniform(seed=42)))\n",
    "modelCNN.add(Activation(\"elu\"))\n",
    "modelCNN.add(MaxPooling2D(pool_size=(poolSize, poolSize)))       \n",
    "modelCNN.add(Dropout(0.40))\n",
    "modelCNN.add(Flatten())\n",
    "modelCNN.add(Dense(numNeuronsD1, kernel_initializer=initializers.glorot_uniform(seed=42)))\n",
    "modelCNN.add(Activation(\"elu\"))\n",
    "modelCNN.add(Dropout(0.07)) \n",
    "modelCNN.add(Dense(3, kernel_initializer=initializers.glorot_uniform(seed=42)))\n",
    "modelCNN.add(Activation('softmax'))\n",
    "            \n",
    "modelCNN.compile(loss='categorical_crossentropy', optimizer=\"Adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "earlyStopping  = cb.EarlyStopping(monitor='val_acc', patience=100, verbose=1, mode='max')\n",
    "checkpointSafe = cb.ModelCheckpoint('ergebnisse_versuch5/modell_versuch5_' + experimentNumber, monitor='val_acc', save_best_only=True)   \n",
    "hist = modelCNN.fit_generator(dataLoader(xTrain, yTrain, batchSize), epochs=100, steps_per_epoch=(int(len(xTrain)/batchSize)),\n",
    "              validation_data=dataLoader(xVal, yVal, batchSize), validation_steps=(int(len(xVal)/batchSize)), callbacks=[earlyStopping,checkpointSafe])\n",
    "\n",
    "score, acc = modelCNN.evaluate_generator( dataLoader(xTest, yTest, batchSize), steps=(int(len(xTest)/batchSize)))\n",
    "print('Test score: '    +  str(score))\n",
    "print('Test accuracy: ' +  str(acc)  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-12T17:06:08.247266Z",
     "start_time": "2018-07-12T17:06:07.849408Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot Modell Historie Loss und Accuracy\n",
    "plt.figure(1)\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.title('Trainingshistorie: Loss')\n",
    "plt.xlabel('Epoche')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(hist.history['loss'])\n",
    "plt.plot(hist.history['val_loss'])\n",
    "plt.legend(['Training', 'Validation'])\n",
    "plt.xlim(0,99)\n",
    "plt.grid(True)\n",
    "plt.savefig(\"trainingshistorieLossVersuch5_\" + experimentNumber + \".png\")\n",
    "plt.xticks(np.arange(0, 99.1, step=10))\n",
    "plt.figure(2)\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.title('Trainingshistorie: Accuracy')\n",
    "plt.xticks()\n",
    "plt.xlabel('Epoche')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(hist.history['acc'])\n",
    "plt.plot(hist.history['val_acc'])\n",
    "plt.legend(['Training', 'Validation'], loc='right')\n",
    "plt.xlim(0,99)\n",
    "plt.xticks(np.arange(0, 99.1, step=10))\n",
    "plt.grid(True)\n",
    "plt.savefig(\"trainingshistorieAccuracyVersuch5_\" + experimentNumber + \".png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-12T17:06:23.424764Z",
     "start_time": "2018-07-12T17:06:22.198802Z"
    }
   },
   "outputs": [],
   "source": [
    "# Läd Modell\n",
    "modell1 = load_model('ergebnisse_versuch5/modell_versuch5_8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-12T17:06:28.340317Z",
     "start_time": "2018-07-12T17:06:25.122587Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.81449210240071512, 0.61640936254980083]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modell1.evaluate_generator( dataLoader(xTest, yTest, 32), steps=int(len(xTest)/32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-12T14:21:06.429982Z",
     "start_time": "2018-07-12T14:21:06.405756Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_6 (Conv2D)            (None, 368, 70, 32)       896       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 368, 70, 32)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 184, 35, 32)       0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 184, 35, 32)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 184, 35, 32)       9248      \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 184, 35, 32)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 92, 17, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 92, 17, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 92, 17, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 92, 17, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 46, 8, 64)         0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 46, 8, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 46, 8, 64)         36928     \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 46, 8, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 23, 4, 64)         0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 23, 4, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 23, 4, 64)         36928     \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 23, 4, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 11, 2, 64)         0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 11, 2, 64)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 1408)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                90176     \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 3)                 195       \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 192,867\n",
      "Trainable params: 192,867\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "modell1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-12T17:06:46.463353Z",
     "start_time": "2018-07-12T17:06:35.812805Z"
    }
   },
   "outputs": [],
   "source": [
    "# Bei der Verwendung von predict_generator werden die daten gemischt \n",
    "validPreds = []\n",
    "imageList = []\n",
    "for path in xTest:\n",
    "    imageList = []   \n",
    "    img = cv2.cvtColor(cv2.imread(path),cv2.COLOR_BGR2RGB)\n",
    "    img = np.array(img)\n",
    "    img = img.astype('float32')\n",
    "    img /= 255\n",
    "    imageList.append(img)\n",
    "    validPreds.append(modell1.predict(np.asarray(imageList)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-12T17:06:51.578876Z",
     "start_time": "2018-07-12T17:06:51.389883Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1249  348   53]\n",
      " [ 423  725  461]\n",
      " [ 132  455  949]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Konfusionsmatrix\n",
    "validPredArray = np.argmax(np.vstack(validPreds), axis=1)\n",
    "yTestMax = np.argmax(yTest,axis=1)\n",
    "cnfMatrix = confusion_matrix(yTestMax, validPredArray)\n",
    "print(cnfMatrix)\n",
    "fig, ax = plt.subplots(figsize=(6,6)) \n",
    "ax = sns.heatmap(cnfMatrix, fmt=\"d\", cmap=plt.cm.Blues, ax=ax , annot=True)\n",
    "ax.set_xticklabels(classNames)\n",
    "ax.set_yticklabels(classNames)\n",
    "plt.title('Konfusionsmatrix Versuch 5.8.2')\n",
    "plt.ylabel('Wahre Klasse')\n",
    "plt.xlabel('Vorhergesagte Klasse')\n",
    "plt.savefig('konfmatrixVersuch5_8_2.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-10T14:38:30.844033Z",
     "start_time": "2018-07-10T14:38:30.808651Z"
    }
   },
   "outputs": [],
   "source": [
    "# Eine Funktion die das zu optimierende Keras-Modell beschreibt\n",
    "# Die vorgehensweise mit einer Funktion ist nach der Dokumentation von Hyperas vorgegeben \n",
    "# siehe https://github.com/maxpumperla/hyperas\n",
    "def model(xTrain, xVal, yTrain, yVal):\n",
    "    # Parameter für das CNN\n",
    "    inputShape     = (368, 70, 3)   # Eingangs Array-Form \n",
    "    numNeuronsC1   = 32                # Anzahl der Filter / 1 Faltungsschicht\n",
    "    poolSize       = 2                 # Größe der Pooling-Layer\n",
    "    convKernelSize = 3                 # Größe des Faltungskern n*n\n",
    "    batchSize      = {{choice([8, 16, 32])}}\n",
    "    print(\"Stapelgroesse (batchSize): \" + str(batchSize))\n",
    "    \n",
    "    model = Sequential()\n",
    "    layerCountTuning = {{choice(['3Layer','4Layer','5Layer'])}}\n",
    "    print(\"Anzahl der Faltungsschichten: \" + layerCountTuning)\n",
    "    af = {{choice(['relu', 'elu'])}}\n",
    "    print(\"Aktivierungsfunktion: \" + af)\n",
    "    optf = {{choice(['RMSprop','Adam'])}}\n",
    "    print(\"Optimierungsfunktion: \" + optf)\n",
    "    model.add(Conv2D(numNeuronsC1, (convKernelSize, convKernelSize), padding='same', input_shape=inputShape,kernel_initializer=initializers.glorot_uniform(seed=42)))\n",
    "    model.add(Activation(af))\n",
    "    model.add(MaxPooling2D(pool_size=(poolSize, poolSize)))\n",
    "    dropoutrate1 = {{uniform(0, 0.70)}}\n",
    "    print(\"Dropout-Rate Faltungsschicht 1: \" + str(dropoutrate1))\n",
    "    model.add(Dropout(dropoutrate1))\n",
    "    \n",
    "    filterCount2 = {{choice([32, 64])}}\n",
    "    print(\"Anzahl der Filter-Maps Faltungsschicht 2: \" + str(filterCount2))\n",
    "    model.add(Conv2D(filterCount2, (convKernelSize, convKernelSize), padding='same', kernel_initializer=initializers.glorot_uniform(seed=42)))\n",
    "    model.add(Activation(af))\n",
    "    model.add(MaxPooling2D(pool_size=(poolSize, poolSize)))\n",
    "    dropoutrate2 = {{uniform(0, 0.70)}}        \n",
    "    print(\"Dropout-Rate Faltungsschicht 2: \" + str(dropoutrate2))\n",
    "    model.add(Dropout(dropoutrate2))\n",
    "    \n",
    "    if layerCountTuning == '3Layer' or layerCountTuning == '4Layer' or layerCountTuning == '5Layer':\n",
    "        filterCount3 = {{choice([64, 128])}}\n",
    "        print(\"Anzahl der Filter-Maps Faltungsschicht 3: \" + str(filterCount3))\n",
    "        model.add(Conv2D(filterCount3, (convKernelSize, convKernelSize), padding='same', kernel_initializer=initializers.glorot_uniform(seed=42)))\n",
    "        model.add(Activation(af))\n",
    "        model.add(MaxPooling2D(pool_size=(poolSize, poolSize)))\n",
    "        dropoutrate3 = {{uniform(0, 0.70)}}        \n",
    "        print(\"Dropout-Rate Faltungsschicht 3: \" + str(dropoutrate3))\n",
    "        model.add(Dropout(dropoutrate3))\n",
    "\n",
    "    if layerCountTuning == '4Layer' or layerCountTuning == '5Layer':\n",
    "        filterCount4 = {{choice([64, 128])}}\n",
    "        print(\"Anzahl der Filter-Maps Faltungsschicht 4: \" + str(filterCount4))\n",
    "        model.add(Conv2D(filterCount4, (convKernelSize, convKernelSize), padding='same', kernel_initializer=initializers.glorot_uniform(seed=42)))\n",
    "        model.add(Activation(af))\n",
    "        model.add(MaxPooling2D(pool_size=(poolSize, poolSize)))\n",
    "        dropoutrate4 = {{uniform(0, 0.70)}}        \n",
    "        print(\"Dropout-Rate Faltungsschicht 4: \" + str(dropoutrate4))\n",
    "        model.add(Dropout(dropoutrate4)) \n",
    "    \n",
    "    if layerCountTuning == '5Layer':\n",
    "        filterCount5 = {{choice([64, 128])}}\n",
    "        print(\"Anzahl der Filter-Maps Faltungsschicht 5: \" + str(filterCount5))\n",
    "        model.add(Conv2D(filterCount5, (convKernelSize, convKernelSize), padding='same', kernel_initializer=initializers.glorot_uniform(seed=42)))\n",
    "        model.add(Activation(af))\n",
    "        model.add(MaxPooling2D(pool_size=(poolSize, poolSize)))\n",
    "        dropoutrate5 = {{uniform(0, 0.70)}}        \n",
    "        print(\"Dropout-Rate Faltungsschicht 5: \" + str(dropoutrate5))\n",
    "        model.add(Dropout(dropoutrate5)) \n",
    "\n",
    "    model.add(Flatten())\n",
    "    dims4 = {{choice([64, 128])}}\n",
    "    print(\"Anzahl der Neuronen des Fully Connected Layer: \" + str(dims4))    \n",
    "    model.add(Dense(dims4, kernel_initializer=initializers.glorot_uniform(seed=42)))\n",
    "    model.add(Activation(af))\n",
    "    dropoutFull = {{uniform(0, 0.70)}}  \n",
    "    print(\"Dropout-Rate Fully Connected Layer: \" + str(dropoutFull))\n",
    "    model.add(Dropout(dropoutFull)) \n",
    "    model.add(Dense(3, kernel_initializer=initializers.glorot_uniform(seed=42)))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    # Diese Funktion läd Bilder in den Hauptspeicher\n",
    "    # imagesPaths: Liste mit Pfaden zu den Bildern != null\n",
    "    def imageLoader(imagePaths):\n",
    "        images = []\n",
    "        for path in imagePaths:\n",
    "            images.append(cv2.cvtColor(cv2.imread(path),cv2.COLOR_BGR2RGB))    \n",
    "        imagesNp = np.array(images)\n",
    "        imagesNp = imagesNp.astype('float32')\n",
    "        imagesNp /= 255\n",
    "        return imagesNp\n",
    "\n",
    "    # Läd Trainingsdaten in batches\n",
    "    def dataLoader(imagePaths, features, batchSize):\n",
    "        imagesCount= len(imagePaths)  \n",
    "        while True:\n",
    "            batchStart = 0\n",
    "            batchEnd = batchSize\n",
    "            while batchStart < imagesCount:\n",
    "                limit = min(batchEnd, imagesCount)\n",
    "                x = imageLoader(imagePaths[batchStart:limit])\n",
    "                y = features[batchStart:limit]\n",
    "                yield (x,y) \n",
    "                batchStart += batchSize   \n",
    "                batchEnd += batchSize\n",
    "                \n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optf, metrics=[\"accuracy\"])\n",
    "    print('Faltungsnetz wird trainiert...')\n",
    "    # Early Stopping unterbricht das Training, wenn nach 10 Epochen die Kostenfunktion nicht weiter minimiert werden konnte \n",
    "    earlyStopping = cb.EarlyStopping(monitor='val_acc', patience=10, verbose=1, mode='max')\n",
    "    checkpointSafe = cb.ModelCheckpoint('ergebnisse_versuch5/modell_versuch5_3', monitor='val_acc', save_best_only=True)   \n",
    "    model.fit_generator(dataLoader(xTrain, yTrain, batchSize), epochs=10, steps_per_epoch=(int(len(xTrain)/batchSize)),\n",
    "              validation_data=dataLoader(xVal, yVal, batchSize), validation_steps=(int(len(xVal)/batchSize)), callbacks=[earlyStopping,checkpointSafe])\n",
    "    score, acc = model.evaluate_generator( dataLoader(xVal, yVal, batchSize), steps=(int(len(xVal)/batchSize)))\n",
    "    print('Test score: ' + str(score))\n",
    "    print('Test accuracy: ' +  str(acc))\n",
    "    # Die Rückgabewerte werden verarbeitet von Hyperas\n",
    "    # loss ist die Kostenfunktion welche minimiert werden soll mit Hyperas\n",
    "    # status (STATUS_OK) gibt an das, dass Modell erfolgreich ausgeführt wurde\n",
    "    return {'loss': -acc, 'status': STATUS_OK, 'model': model}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-10T14:38:31.644985Z",
     "start_time": "2018-07-10T14:38:31.632990Z"
    }
   },
   "outputs": [],
   "source": [
    "def data():\n",
    "    # Hier können die Datensätze ausgewählt werden\n",
    "    datasets = ['43','45','46','47','48','49','50','51']\n",
    "    # Die Pfade zu den Ordnern in welchem sich die Bilder befinden\n",
    "    paths = []\n",
    "    # Liste mit Pfaden zu den Bildern\n",
    "    imagePaths = []\n",
    "    for dataset in datasets: # Für jeden Datensatz merke Pfad\n",
    "        paths.append(\"C:/Users/morro/Documents/datenRoh/\" + dataset + \"/zugeschnitten/\")\n",
    "    for path in paths: # Für jeden Pfad hole die Namen der Ordner\n",
    "        folders = os.listdir(path)\n",
    "        folders = sorted(folders, key=int) #sortiert die Reihenfolge de Ordner aufsteifend\n",
    "        print(path)\n",
    "        print(\"Bilder aus folgenden Ordnern werden geladen: \" + str(folders))\n",
    "        for folder in folders: # Aus der Liste der Ordner wird ein Ordner ausgewählt\n",
    "            filesPath = path + folder + \"/\"\n",
    "            files = os.listdir(filesPath)\n",
    "            print(\"Ordner der geladen wird: \" + str(folder))\n",
    "            for name in files: # Ein Dateiname aus diesem Ordner\n",
    "                if \"jpg\" not in name:\n",
    "                    continue\n",
    "                imagePaths.append(filesPath + name)\n",
    "    # Y Klassen Labels zuweisen\n",
    "    featuresDf = pandas.read_csv(filepath_or_buffer=\"../daten/merkmale_datensatz_43_45_bis_51/merkmaleMitLabelnFuzzyVersuch6.csv\")\n",
    "    yLabels = np_utils.to_categorical(featuresDf['Klasse'], 0)\n",
    "    # Setzten des RandomState um reproduzierbare Ergebnisse zu erzielen.\n",
    "    np.random.seed(42)\n",
    "    # Mischen der Trainingsdaten\n",
    "    xShuffle, yShuffle = shuffle(imagePaths,yLabels)\n",
    "    class1Number = 0\n",
    "    class2Number = 0 \n",
    "    class3Number = 0\n",
    "    class4Number = 0\n",
    "    class5Number = 0\n",
    "    class6Number = 0\n",
    "    class7Number = 0\n",
    "    class8Number = 0\n",
    "    maxClasses = featuresDf[\"Klasse\"].value_counts().min()\n",
    "    indexToDelete = [] \n",
    "    i = -1\n",
    "    for label in yShuffle:\n",
    "        i = i + 1\n",
    "        labelNumber = np.argmax(label,axis=0)\n",
    "        if labelNumber == 0 and class1Number < maxClasses:\n",
    "            class1Number = class1Number + 1\n",
    "            continue\n",
    "        elif labelNumber == 0:\n",
    "            indexToDelete.append(i)\n",
    "        if labelNumber == 1 and class2Number < maxClasses:\n",
    "            class2Number = class2Number + 1\n",
    "            continue\n",
    "        elif labelNumber == 1:\n",
    "            indexToDelete.append(i)\n",
    "        if labelNumber == 2 and class3Number < maxClasses:\n",
    "            class3Number = class3Number + 1\n",
    "            continue\n",
    "        elif labelNumber == 2:\n",
    "            indexToDelete.append(i)\n",
    "        if labelNumber == 3 and class4Number < maxClasses:\n",
    "            class4Number = class4Number + 1\n",
    "            continue        \n",
    "        elif labelNumber == 3:\n",
    "            indexToDelete.append(i)\n",
    "        if labelNumber == 4 and class5Number < maxClasses:\n",
    "            class5Number = class5Number + 1\n",
    "            continue\n",
    "        elif labelNumber == 4:\n",
    "            indexToDelete.append(i)\n",
    "        if labelNumber == 5 and class6Number < maxClasses:\n",
    "            class6Number = class6Number + 1\n",
    "            continue\n",
    "        elif labelNumber == 5:\n",
    "            indexToDelete.append(i)\n",
    "        if labelNumber == 6 and class7Number < maxClasses:\n",
    "            class7Number = class7Number + 1\n",
    "            continue\n",
    "        elif labelNumber == 6:\n",
    "            indexToDelete.append(i)\n",
    "        if labelNumber == 7 and class8Number < maxClasses:\n",
    "            class8Number = class8Number + 1\n",
    "            continue\n",
    "        elif labelNumber == 7:\n",
    "            indexToDelete.append(i)\n",
    "\n",
    "    xShuffle = [i for j, i in enumerate(xShuffle) if j not in indexToDelete]\n",
    "    yShuffle = [i for j, i in enumerate(yShuffle) if j not in indexToDelete]\n",
    "    yShuffle = np.asarray(yShuffle)\n",
    "    xShuffle, yShuffle = shuffle(xShuffle,yShuffle)\n",
    "    # Aufteilung in Trainings und Testdaten\n",
    "    xTrain, xTest, yTrain, yTest = train_test_split(xShuffle, yShuffle, test_size=0.1)\n",
    "    xTrain, xVal, yTrain, yVal = train_test_split(xTrain, yTrain, test_size=0.2)\n",
    "    return xTrain, xVal, yTrain, yVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-07-10T14:38:32.507Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Imports:\n",
      "#coding=utf-8\n",
      "\n",
      "from __future__ import print_function\n",
      "\n",
      "try:\n",
      "    from hyperopt import Trials, STATUS_OK, rand\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas import optim\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas.distributions import uniform, choice\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import seaborn as sns\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import matplotlib.pyplot as plt\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import matplotlib.lines as mlines\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from mpl_toolkits.axes_grid1 import ImageGrid\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import os\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import cv2\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.utils import np_utils\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras import initializers\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.optimizers import RMSprop\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import load_model\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import keras.callbacks as cb\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.utils import shuffle\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.metrics import confusion_matrix\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import collections\n",
      "except:\n",
      "    pass\n",
      "\n",
      ">>> Hyperas search space:\n",
      "\n",
      "def get_space():\n",
      "    return {\n",
      "        'batchSize': hp.choice('batchSize', [8, 16, 32]),\n",
      "        'layerCountTuning': hp.choice('layerCountTuning', ['3Layer','4Layer','5Layer']),\n",
      "        'af': hp.choice('af', ['relu', 'elu']),\n",
      "        'optf': hp.choice('optf', ['RMSprop','Adam']),\n",
      "        'dropoutrate1': hp.uniform('dropoutrate1', 0, 0.70),\n",
      "        'filterCount2': hp.choice('filterCount2', [32, 64]),\n",
      "        'dropoutrate1_1': hp.uniform('dropoutrate1_1', 0, 0.70),\n",
      "        'filterCount3': hp.choice('filterCount3', [64, 128]),\n",
      "        'dropoutrate1_2': hp.uniform('dropoutrate1_2', 0, 0.70),\n",
      "        'filterCount3_1': hp.choice('filterCount3_1', [64, 128]),\n",
      "        'dropoutrate1_3': hp.uniform('dropoutrate1_3', 0, 0.70),\n",
      "        'filterCount3_2': hp.choice('filterCount3_2', [64, 128]),\n",
      "        'dropoutrate1_4': hp.uniform('dropoutrate1_4', 0, 0.70),\n",
      "        'filterCount3_3': hp.choice('filterCount3_3', [64, 128]),\n",
      "        'dropoutrate1_5': hp.uniform('dropoutrate1_5', 0, 0.70),\n",
      "    }\n",
      "\n",
      ">>> Data\n",
      "   1: \n",
      "   2: # Hier können die Datensätze ausgewählt werden\n",
      "   3: datasets = ['43','45','46','47','48','49','50','51']\n",
      "   4: # Die Pfade zu den Ordnern in welchem sich die Bilder befinden\n",
      "   5: paths = []\n",
      "   6: # Liste mit Pfaden zu den Bildern\n",
      "   7: imagePaths = []\n",
      "   8: for dataset in datasets: # Für jeden Datensatz merke Pfad\n",
      "   9:     paths.append(\"C:/Users/morro/Documents/datenRoh/\" + dataset + \"/zugeschnitten/\")\n",
      "  10: for path in paths: # Für jeden Pfad hole die Namen der Ordner\n",
      "  11:     folders = os.listdir(path)\n",
      "  12:     folders = sorted(folders, key=int) #sortiert die Reihenfolge de Ordner aufsteifend\n",
      "  13:     print(path)\n",
      "  14:     print(\"Bilder aus folgenden Ordnern werden geladen: \" + str(folders))\n",
      "  15:     for folder in folders: # Aus der Liste der Ordner wird ein Ordner ausgewählt\n",
      "  16:         filesPath = path + folder + \"/\"\n",
      "  17:         files = os.listdir(filesPath)\n",
      "  18:         print(\"Ordner der geladen wird: \" + str(folder))\n",
      "  19:         for name in files: # Ein Dateiname aus diesem Ordner\n",
      "  20:             if \"jpg\" not in name:\n",
      "  21:                 continue\n",
      "  22:             imagePaths.append(filesPath + name)\n",
      "  23: # Y Klassen Labels zuweisen\n",
      "  24: featuresDf = pandas.read_csv(filepath_or_buffer=\"../daten/merkmale_datensatz_43_45_bis_51/merkmaleMitLabelnFuzzyVersuch6.csv\")\n",
      "  25: yLabels = np_utils.to_categorical(featuresDf['Klasse'], 0)\n",
      "  26: # Setzten des RandomState um reproduzierbare Ergebnisse zu erzielen.\n",
      "  27: np.random.seed(42)\n",
      "  28: # Mischen der Trainingsdaten\n",
      "  29: xShuffle, yShuffle = shuffle(imagePaths,yLabels)\n",
      "  30: class1Number = 0\n",
      "  31: class2Number = 0 \n",
      "  32: class3Number = 0\n",
      "  33: class4Number = 0\n",
      "  34: class5Number = 0\n",
      "  35: class6Number = 0\n",
      "  36: class7Number = 0\n",
      "  37: class8Number = 0\n",
      "  38: maxClasses = featuresDf[\"Klasse\"].value_counts().min()\n",
      "  39: indexToDelete = [] \n",
      "  40: i = -1\n",
      "  41: for label in yShuffle:\n",
      "  42:     i = i + 1\n",
      "  43:     labelNumber = np.argmax(label,axis=0)\n",
      "  44:     if labelNumber == 0 and class1Number < maxClasses:\n",
      "  45:         class1Number = class1Number + 1\n",
      "  46:         continue\n",
      "  47:     elif labelNumber == 0:\n",
      "  48:         indexToDelete.append(i)\n",
      "  49:     if labelNumber == 1 and class2Number < maxClasses:\n",
      "  50:         class2Number = class2Number + 1\n",
      "  51:         continue\n",
      "  52:     elif labelNumber == 1:\n",
      "  53:         indexToDelete.append(i)\n",
      "  54:     if labelNumber == 2 and class3Number < maxClasses:\n",
      "  55:         class3Number = class3Number + 1\n",
      "  56:         continue\n",
      "  57:     elif labelNumber == 2:\n",
      "  58:         indexToDelete.append(i)\n",
      "  59:     if labelNumber == 3 and class4Number < maxClasses:\n",
      "  60:         class4Number = class4Number + 1\n",
      "  61:         continue        \n",
      "  62:     elif labelNumber == 3:\n",
      "  63:         indexToDelete.append(i)\n",
      "  64:     if labelNumber == 4 and class5Number < maxClasses:\n",
      "  65:         class5Number = class5Number + 1\n",
      "  66:         continue\n",
      "  67:     elif labelNumber == 4:\n",
      "  68:         indexToDelete.append(i)\n",
      "  69:     if labelNumber == 5 and class6Number < maxClasses:\n",
      "  70:         class6Number = class6Number + 1\n",
      "  71:         continue\n",
      "  72:     elif labelNumber == 5:\n",
      "  73:         indexToDelete.append(i)\n",
      "  74:     if labelNumber == 6 and class7Number < maxClasses:\n",
      "  75:         class7Number = class7Number + 1\n",
      "  76:         continue\n",
      "  77:     elif labelNumber == 6:\n",
      "  78:         indexToDelete.append(i)\n",
      "  79:     if labelNumber == 7 and class8Number < maxClasses:\n",
      "  80:         class8Number = class8Number + 1\n",
      "  81:         continue\n",
      "  82:     elif labelNumber == 7:\n",
      "  83:         indexToDelete.append(i)\n",
      "  84: \n",
      "  85: xShuffle = [i for j, i in enumerate(xShuffle) if j not in indexToDelete]\n",
      "  86: yShuffle = [i for j, i in enumerate(yShuffle) if j not in indexToDelete]\n",
      "  87: yShuffle = np.asarray(yShuffle)\n",
      "  88: xShuffle, yShuffle = shuffle(xShuffle,yShuffle)\n",
      "  89: # Aufteilung in Trainings und Testdaten\n",
      "  90: xTrain, xTest, yTrain, yTest = train_test_split(xShuffle, yShuffle, test_size=0.1)\n",
      "  91: xTrain, xVal, yTrain, yVal = train_test_split(xTrain, yTrain, test_size=0.2)\n",
      "  92: \n",
      "  93: \n",
      "  94: \n",
      ">>> Resulting replaced keras model:\n",
      "\n",
      "   1: def keras_fmin_fnct(space):\n",
      "   2: \n",
      "   3:     # Parameter für das CNN\n",
      "   4:     inputShape     = (368, 70, 3)   # Eingangs Array-Form \n",
      "   5:     numNeuronsC1   = 32                # Anzahl der Filter / 1 Faltungsschicht\n",
      "   6:     poolSize       = 2                 # Größe der Pooling-Layer\n",
      "   7:     convKernelSize = 3                 # Größe des Faltungskern n*n\n",
      "   8:     batchSize      = space['batchSize']\n",
      "   9:     print(\"Stapelgroesse (batchSize): \" + str(batchSize))\n",
      "  10:     \n",
      "  11:     model = Sequential()\n",
      "  12:     layerCountTuning = space['layerCountTuning']\n",
      "  13:     print(\"Anzahl der Faltungsschichten: \" + layerCountTuning)\n",
      "  14:     af = space['af']\n",
      "  15:     print(\"Aktivierungsfunktion: \" + af)\n",
      "  16:     optf = space['optf']\n",
      "  17:     print(\"Optimierungsfunktion: \" + optf)\n",
      "  18:     model.add(Conv2D(numNeuronsC1, (convKernelSize, convKernelSize), padding='same', input_shape=inputShape,kernel_initializer=initializers.glorot_uniform(seed=42)))\n",
      "  19:     model.add(Activation(af))\n",
      "  20:     model.add(MaxPooling2D(pool_size=(poolSize, poolSize)))\n",
      "  21:     dropoutrate1 = space['dropoutrate1']\n",
      "  22:     print(\"Dropout-Rate Faltungsschicht 1: \" + str(dropoutrate1))\n",
      "  23:     model.add(Dropout(dropoutrate1))\n",
      "  24:     \n",
      "  25:     filterCount2 = space['filterCount2']\n",
      "  26:     print(\"Anzahl der Filter-Maps Faltungsschicht 2: \" + str(filterCount2))\n",
      "  27:     model.add(Conv2D(filterCount2, (convKernelSize, convKernelSize), padding='same', kernel_initializer=initializers.glorot_uniform(seed=42)))\n",
      "  28:     model.add(Activation(af))\n",
      "  29:     model.add(MaxPooling2D(pool_size=(poolSize, poolSize)))\n",
      "  30:     dropoutrate2 = space['dropoutrate1_1']        \n",
      "  31:     print(\"Dropout-Rate Faltungsschicht 2: \" + str(dropoutrate2))\n",
      "  32:     model.add(Dropout(dropoutrate2))\n",
      "  33:     \n",
      "  34:     if layerCountTuning == '3Layer' or layerCountTuning == '4Layer' or layerCountTuning == '5Layer':\n",
      "  35:         print(\"Anzahl der Faltungsschichten: \" + layerCountTuning)\n",
      "  36:         filterCount3 = space['filterCount3']\n",
      "  37:         print(\"Anzahl der Filter-Maps Faltungsschicht 3: \" + str(filterCount3))\n",
      "  38:         model.add(Conv2D(filterCount3, (convKernelSize, convKernelSize), padding='same', kernel_initializer=initializers.glorot_uniform(seed=42)))\n",
      "  39:         model.add(Activation(af))\n",
      "  40:         model.add(MaxPooling2D(pool_size=(poolSize, poolSize)))\n",
      "  41:         dropoutrate3 = space['dropoutrate1_2']        \n",
      "  42:         print(\"Dropout-Rate Faltungsschicht 3: \" + str(dropoutrate3))\n",
      "  43:         model.add(Dropout(dropoutrate3))\n",
      "  44: \n",
      "  45:     if layerCountTuning == '4Layer' or layerCountTuning == '5Layer':\n",
      "  46:         filterCount4 = space['filterCount3_1']\n",
      "  47:         print(\"Anzahl der Filter-Maps Faltungsschicht 4: \" + str(filterCount4))\n",
      "  48:         model.add(Conv2D(filterCount4, (convKernelSize, convKernelSize), padding='same', kernel_initializer=initializers.glorot_uniform(seed=42)))\n",
      "  49:         model.add(Activation(af))\n",
      "  50:         model.add(MaxPooling2D(pool_size=(poolSize, poolSize)))\n",
      "  51:         dropoutrate4 = space['dropoutrate1_3']        \n",
      "  52:         print(\"Dropout-Rate Faltungsschicht 4: \" + str(dropoutrate4))\n",
      "  53:         model.add(Dropout(dropoutrate4)) \n",
      "  54:     \n",
      "  55:     if layerCountTuning == '5Layer':\n",
      "  56:         filterCount5 = space['filterCount3_2']\n",
      "  57:         print(\"Anzahl der Filter-Maps Faltungsschicht 5: \" + str(filterCount5))\n",
      "  58:         model.add(Conv2D(filterCount5, (convKernelSize, convKernelSize), padding='same', kernel_initializer=initializers.glorot_uniform(seed=42)))\n",
      "  59:         model.add(Activation(af))\n",
      "  60:         model.add(MaxPooling2D(pool_size=(poolSize, poolSize)))\n",
      "  61:         dropoutrate5 = space['dropoutrate1_4']        \n",
      "  62:         print(\"Dropout-Rate Faltungsschicht 5: \" + str(dropoutrate5))\n",
      "  63:         model.add(Dropout(dropoutrate5)) \n",
      "  64: \n",
      "  65:     model.add(Flatten())\n",
      "  66:     dims4 = space['filterCount3_3']\n",
      "  67:     print(\"Anzahl der Neuronen des Fully Connected Layer: \" + str(dims4))    \n",
      "  68:     model.add(Dense(dims4, kernel_initializer=initializers.glorot_uniform(seed=42)))\n",
      "  69:     model.add(Activation(af))\n",
      "  70:     dropoutFull = space['dropoutrate1_5']  \n",
      "  71:     print(\"Dropout-Rate Fully Connected Layer: \" + str(dropoutFull))\n",
      "  72:     model.add(Dropout(dropoutFull)) \n",
      "  73:     model.add(Dense(3, kernel_initializer=initializers.glorot_uniform(seed=42)))\n",
      "  74:     model.add(Activation('softmax'))\n",
      "  75:     \n",
      "  76:     # Diese Funktion läd Bilder in den Hauptspeicher\n",
      "  77:     # imagesPaths: Liste mit Pfaden zu den Bildern != null\n",
      "  78:     def imageLoader(imagePaths):\n",
      "  79:         images = []\n",
      "  80:         for path in imagePaths:\n",
      "  81:             images.append(cv2.cvtColor(cv2.imread(path),cv2.COLOR_BGR2RGB))    \n",
      "  82:         imagesNp = np.array(images)\n",
      "  83:         imagesNp = imagesNp.astype('float32')\n",
      "  84:         imagesNp /= 255\n",
      "  85:         return imagesNp\n",
      "  86: \n",
      "  87:     # Läd Trainingsdaten in batches\n",
      "  88:     def dataLoader(imagePaths, features, batchSize):\n",
      "  89:         imagesCount= len(imagePaths)  \n",
      "  90:         while True:\n",
      "  91:             batchStart = 0\n",
      "  92:             batchEnd = batchSize\n",
      "  93:             while batchStart < imagesCount:\n",
      "  94:                 limit = min(batchEnd, imagesCount)\n",
      "  95:                 x = imageLoader(imagePaths[batchStart:limit])\n",
      "  96:                 y = features[batchStart:limit]\n",
      "  97:                 yield (x,y) \n",
      "  98:                 batchStart += batchSize   \n",
      "  99:                 batchEnd += batchSize\n",
      " 100:                 \n",
      " 101:     model.compile(loss='categorical_crossentropy', optimizer=optf, metrics=[\"accuracy\"])\n",
      " 102:     print('Faltungsnetz wird trainiert...')\n",
      " 103:     # Early Stopping unterbricht das Training, wenn nach 10 Epochen die Kostenfunktion nicht weiter minimiert werden konnte \n",
      " 104:     earlyStopping = cb.EarlyStopping(monitor='val_acc', patience=10, verbose=1, mode='max')\n",
      " 105:     checkpointSafe = cb.ModelCheckpoint('ergebnisse_versuch5/modell_versuch5_3', monitor='val_acc', save_best_only=True)   \n",
      " 106:     model.fit_generator(dataLoader(xTrain, yTrain, batchSize), epochs=10, steps_per_epoch=(int(len(xTrain)/batchSize)),\n",
      " 107:               validation_data=dataLoader(xVal, yVal, batchSize), validation_steps=(int(len(xVal)/batchSize)), callbacks=[earlyStopping,checkpointSafe])\n",
      " 108:     score, acc = model.evaluate_generator( dataLoader(xVal, yVal, batchSize), steps=(int(len(xVal)/batchSize)))\n",
      " 109:     print('Test score: ' + str(score))\n",
      " 110:     print('Test accuracy: ' +  str(acc))\n",
      " 111:     # Die Rückgabewerte werden verarbeitet von Hyperas\n",
      " 112:     # loss ist die Kostenfunktion welche minimiert werden soll mit Hyperas\n",
      " 113:     # status (STATUS_OK) gibt an das, dass Modell erfolgreich ausgeführt wurde\n",
      " 114:     return {'loss': -acc, 'status': STATUS_OK, 'model': model}\n",
      " 115: \n",
      "C:/Users/morro/Documents/datenRoh/43/zugeschnitten/\n",
      "Bilder aus folgenden Ordnern werden geladen: ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10']\n",
      "Ordner der geladen wird: 1\n",
      "Ordner der geladen wird: 2\n",
      "Ordner der geladen wird: 3\n",
      "Ordner der geladen wird: 4\n",
      "Ordner der geladen wird: 5\n",
      "Ordner der geladen wird: 6\n",
      "Ordner der geladen wird: 7\n",
      "Ordner der geladen wird: 8\n",
      "Ordner der geladen wird: 9\n",
      "Ordner der geladen wird: 10\n",
      "C:/Users/morro/Documents/datenRoh/45/zugeschnitten/\n",
      "Bilder aus folgenden Ordnern werden geladen: ['1', '2', '3', '4', '5']\n",
      "Ordner der geladen wird: 1\n",
      "Ordner der geladen wird: 2\n",
      "Ordner der geladen wird: 3\n",
      "Ordner der geladen wird: 4\n",
      "Ordner der geladen wird: 5\n",
      "C:/Users/morro/Documents/datenRoh/46/zugeschnitten/\n",
      "Bilder aus folgenden Ordnern werden geladen: ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11']\n",
      "Ordner der geladen wird: 1\n",
      "Ordner der geladen wird: 2\n",
      "Ordner der geladen wird: 3\n",
      "Ordner der geladen wird: 4\n",
      "Ordner der geladen wird: 5\n",
      "Ordner der geladen wird: 6\n",
      "Ordner der geladen wird: 7\n",
      "Ordner der geladen wird: 8\n",
      "Ordner der geladen wird: 9\n",
      "Ordner der geladen wird: 10\n",
      "Ordner der geladen wird: 11\n",
      "C:/Users/morro/Documents/datenRoh/47/zugeschnitten/\n",
      "Bilder aus folgenden Ordnern werden geladen: ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10']\n",
      "Ordner der geladen wird: 1\n",
      "Ordner der geladen wird: 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ordner der geladen wird: 3\n",
      "Ordner der geladen wird: 4\n",
      "Ordner der geladen wird: 5\n",
      "Ordner der geladen wird: 6\n",
      "Ordner der geladen wird: 7\n",
      "Ordner der geladen wird: 8\n",
      "Ordner der geladen wird: 9\n",
      "Ordner der geladen wird: 10\n",
      "C:/Users/morro/Documents/datenRoh/48/zugeschnitten/\n",
      "Bilder aus folgenden Ordnern werden geladen: ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10']\n",
      "Ordner der geladen wird: 1\n",
      "Ordner der geladen wird: 2\n",
      "Ordner der geladen wird: 3\n",
      "Ordner der geladen wird: 4\n",
      "Ordner der geladen wird: 5\n",
      "Ordner der geladen wird: 6\n",
      "Ordner der geladen wird: 7\n",
      "Ordner der geladen wird: 8\n",
      "Ordner der geladen wird: 9\n",
      "Ordner der geladen wird: 10\n",
      "C:/Users/morro/Documents/datenRoh/49/zugeschnitten/\n",
      "Bilder aus folgenden Ordnern werden geladen: ['1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
      "Ordner der geladen wird: 1\n",
      "Ordner der geladen wird: 2\n",
      "Ordner der geladen wird: 3\n",
      "Ordner der geladen wird: 4\n",
      "Ordner der geladen wird: 5\n",
      "Ordner der geladen wird: 6\n",
      "Ordner der geladen wird: 7\n",
      "Ordner der geladen wird: 8\n",
      "Ordner der geladen wird: 9\n",
      "C:/Users/morro/Documents/datenRoh/50/zugeschnitten/\n",
      "Bilder aus folgenden Ordnern werden geladen: ['1', '2', '3', '4', '5']\n",
      "Ordner der geladen wird: 1\n",
      "Ordner der geladen wird: 2\n",
      "Ordner der geladen wird: 3\n",
      "Ordner der geladen wird: 4\n",
      "Ordner der geladen wird: 5\n",
      "C:/Users/morro/Documents/datenRoh/51/zugeschnitten/\n",
      "Bilder aus folgenden Ordnern werden geladen: ['1', '2', '3', '4', '5']\n",
      "Ordner der geladen wird: 1\n",
      "Ordner der geladen wird: 2\n",
      "Ordner der geladen wird: 3\n",
      "Ordner der geladen wird: 4\n",
      "Ordner der geladen wird: 5\n",
      "Stapelgroesse (batchSize): 32\n",
      "Anzahl der Faltungsschichten: 4Layer\n",
      "Aktivierungsfunktion: elu\n",
      "Optimierungsfunktion: RMSprop\n",
      "Dropout-Rate Faltungsschicht 1: 0.36650111313577477\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 64\n",
      "Dropout-Rate Faltungsschicht 2: 0.5910892819828057\n",
      "Anzahl der Faltungsschichten: 4Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 128\n",
      "Dropout-Rate Faltungsschicht 3: 0.31073308941714595\n",
      "Anzahl der Filter-Maps Faltungsschicht 4: 128\n",
      "Dropout-Rate Faltungsschicht 4: 0.02787756283401593\n",
      "Anzahl der Neuronen des Fully Connected Layer: 64\n",
      "Dropout-Rate Fully Connected Layer: 0.22452694320321762\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "1521/1521 [==============================] - 108s 71ms/step - loss: 1.2174 - acc: 0.3969 - val_loss: 1.0893 - val_acc: 0.4145\n",
      "Epoch 2/10\n",
      "1521/1521 [==============================] - 143s 94ms/step - loss: 1.0000 - acc: 0.4847 - val_loss: 0.9058 - val_acc: 0.5399\n",
      "Epoch 3/10\n",
      "1521/1521 [==============================] - 173s 114ms/step - loss: 0.9269 - acc: 0.5401 - val_loss: 1.1243 - val_acc: 0.3870\n",
      "Epoch 4/10\n",
      "1521/1521 [==============================] - 105s 69ms/step - loss: 0.9000 - acc: 0.5576 - val_loss: 0.8534 - val_acc: 0.5890\n",
      "Epoch 5/10\n",
      "1521/1521 [==============================] - 105s 69ms/step - loss: 0.8841 - acc: 0.5681 - val_loss: 0.8903 - val_acc: 0.5624\n",
      "Epoch 6/10\n",
      "1521/1521 [==============================] - 105s 69ms/step - loss: 0.8758 - acc: 0.5797 - val_loss: 1.1957 - val_acc: 0.4783\n",
      "Epoch 7/10\n",
      "1521/1521 [==============================] - 105s 69ms/step - loss: 0.8672 - acc: 0.5807 - val_loss: 0.9035 - val_acc: 0.6017\n",
      "Epoch 8/10\n",
      "1521/1521 [==============================] - 105s 69ms/step - loss: 0.8626 - acc: 0.5843 - val_loss: 1.0176 - val_acc: 0.5696\n",
      "Epoch 9/10\n",
      "1521/1521 [==============================] - 105s 69ms/step - loss: 0.8636 - acc: 0.5834 - val_loss: 1.1003 - val_acc: 0.5476\n",
      "Epoch 10/10\n",
      "1521/1521 [==============================] - 105s 69ms/step - loss: 0.8587 - acc: 0.5880 - val_loss: 0.9441 - val_acc: 0.6005\n",
      "Test score: 0.9450120011442587\n",
      "Test accuracy: 0.6002467105263158\n",
      "Stapelgroesse (batchSize): 8\n",
      "Anzahl der Faltungsschichten: 5Layer\n",
      "Aktivierungsfunktion: relu\n",
      "Optimierungsfunktion: RMSprop\n",
      "Dropout-Rate Faltungsschicht 1: 0.3823049904758277\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 32\n",
      "Dropout-Rate Faltungsschicht 2: 0.04027144697613922\n",
      "Anzahl der Faltungsschichten: 5Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 128\n",
      "Dropout-Rate Faltungsschicht 3: 0.11001387944772216\n",
      "Anzahl der Filter-Maps Faltungsschicht 4: 64\n",
      "Dropout-Rate Faltungsschicht 4: 0.3809694643867013\n",
      "Anzahl der Filter-Maps Faltungsschicht 5: 64\n",
      "Dropout-Rate Faltungsschicht 5: 0.3038288829951559\n",
      "Anzahl der Neuronen des Fully Connected Layer: 128\n",
      "Dropout-Rate Fully Connected Layer: 0.48825528701971843\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "6084/6084 [==============================] - 139s 23ms/step - loss: 1.0654 - acc: 0.4259 - val_loss: 0.9995 - val_acc: 0.4756\n",
      "Epoch 2/10\n",
      "6084/6084 [==============================] - 138s 23ms/step - loss: 1.0450 - acc: 0.4697 - val_loss: 1.0295 - val_acc: 0.4346\n",
      "Epoch 3/10\n",
      "6084/6084 [==============================] - 136s 22ms/step - loss: 1.0672 - acc: 0.4602 - val_loss: 1.1404 - val_acc: 0.3376\n",
      "Epoch 4/10\n",
      "6084/6084 [==============================] - 136s 22ms/step - loss: 1.0892 - acc: 0.4414 - val_loss: 1.1144 - val_acc: 0.3393\n",
      "Epoch 5/10\n",
      "6084/6084 [==============================] - 135s 22ms/step - loss: 1.0950 - acc: 0.4430 - val_loss: 1.1337 - val_acc: 0.3336\n",
      "Epoch 6/10\n",
      "6084/6084 [==============================] - 135s 22ms/step - loss: 1.0928 - acc: 0.4549 - val_loss: 1.1301 - val_acc: 0.3496\n",
      "Epoch 7/10\n",
      "6084/6084 [==============================] - 137s 23ms/step - loss: 1.1431 - acc: 0.4543 - val_loss: 1.0707 - val_acc: 0.4120\n",
      "Epoch 8/10\n",
      "6084/6084 [==============================] - 137s 23ms/step - loss: 2.5590 - acc: 0.4345 - val_loss: 10.7321 - val_acc: 0.3342\n",
      "Epoch 9/10\n",
      "6084/6084 [==============================] - 136s 22ms/step - loss: 10.7494 - acc: 0.3331 - val_loss: 10.7335 - val_acc: 0.3341\n",
      "Epoch 10/10\n",
      "6084/6084 [==============================] - 136s 22ms/step - loss: 10.7457 - acc: 0.3333 - val_loss: 10.7321 - val_acc: 0.3342\n",
      "Test score: 10.732150633973092\n",
      "Test accuracy: 0.334155161078238\n",
      "Stapelgroesse (batchSize): 16\n",
      "Anzahl der Faltungsschichten: 5Layer\n",
      "Aktivierungsfunktion: elu\n",
      "Optimierungsfunktion: Adam\n",
      "Dropout-Rate Faltungsschicht 1: 0.4930316398594563\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 64\n",
      "Dropout-Rate Faltungsschicht 2: 0.5337907116491443\n",
      "Anzahl der Faltungsschichten: 5Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 128\n",
      "Dropout-Rate Faltungsschicht 3: 0.48407428062707253\n",
      "Anzahl der Filter-Maps Faltungsschicht 4: 128\n",
      "Dropout-Rate Faltungsschicht 4: 0.6068703632453584\n",
      "Anzahl der Filter-Maps Faltungsschicht 5: 64\n",
      "Dropout-Rate Faltungsschicht 5: 0.6803014606088199\n",
      "Anzahl der Neuronen des Fully Connected Layer: 64\n",
      "Dropout-Rate Fully Connected Layer: 0.10160055304815652\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "3042/3042 [==============================] - 125s 41ms/step - loss: 1.0797 - acc: 0.4143 - val_loss: 1.0112 - val_acc: 0.4655\n",
      "Epoch 2/10\n",
      "3042/3042 [==============================] - 124s 41ms/step - loss: 1.0341 - acc: 0.4486 - val_loss: 1.0249 - val_acc: 0.4699\n",
      "Epoch 3/10\n",
      "3042/3042 [==============================] - 128s 42ms/step - loss: 1.0109 - acc: 0.4744 - val_loss: 1.1377 - val_acc: 0.3840\n",
      "Epoch 4/10\n",
      "3042/3042 [==============================] - 129s 42ms/step - loss: 0.9997 - acc: 0.4947 - val_loss: 1.0241 - val_acc: 0.4103\n",
      "Epoch 5/10\n",
      "3042/3042 [==============================] - 127s 42ms/step - loss: 0.9893 - acc: 0.5016 - val_loss: 0.9399 - val_acc: 0.5420\n",
      "Epoch 6/10\n",
      "3042/3042 [==============================] - 124s 41ms/step - loss: 0.9811 - acc: 0.5047 - val_loss: 0.9524 - val_acc: 0.4707\n",
      "Epoch 7/10\n",
      "3042/3042 [==============================] - 124s 41ms/step - loss: 0.9680 - acc: 0.5132 - val_loss: 0.9800 - val_acc: 0.5176\n",
      "Epoch 8/10\n",
      "3042/3042 [==============================] - 124s 41ms/step - loss: 0.9557 - acc: 0.5227 - val_loss: 0.9143 - val_acc: 0.5691\n",
      "Epoch 9/10\n",
      "3042/3042 [==============================] - 124s 41ms/step - loss: 0.9535 - acc: 0.5233 - val_loss: 0.9815 - val_acc: 0.5237\n",
      "Epoch 10/10\n",
      "3042/3042 [==============================] - 124s 41ms/step - loss: 0.9457 - acc: 0.5276 - val_loss: 0.9239 - val_acc: 0.5506\n",
      "Test score: 0.9238097608873719\n",
      "Test accuracy: 0.5508223684210526\n",
      "Stapelgroesse (batchSize): 16\n",
      "Anzahl der Faltungsschichten: 4Layer\n",
      "Aktivierungsfunktion: relu\n",
      "Optimierungsfunktion: Adam\n",
      "Dropout-Rate Faltungsschicht 1: 0.5510035245500235\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 64\n",
      "Dropout-Rate Faltungsschicht 2: 0.1532124532686945\n",
      "Anzahl der Faltungsschichten: 4Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 64\n",
      "Dropout-Rate Faltungsschicht 3: 0.17944072462772032\n",
      "Anzahl der Filter-Maps Faltungsschicht 4: 128\n",
      "Dropout-Rate Faltungsschicht 4: 0.08445402539120138\n",
      "Anzahl der Neuronen des Fully Connected Layer: 128\n",
      "Dropout-Rate Fully Connected Layer: 0.08210828672230966\n",
      "Faltungsnetz wird trainiert...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3042/3042 [==============================] - 115s 38ms/step - loss: 1.0787 - acc: 0.3852 - val_loss: 1.0160 - val_acc: 0.4714\n",
      "Epoch 2/10\n",
      "3042/3042 [==============================] - 115s 38ms/step - loss: 1.0064 - acc: 0.4737 - val_loss: 0.9918 - val_acc: 0.4863\n",
      "Epoch 3/10\n",
      "3042/3042 [==============================] - 114s 38ms/step - loss: 0.9796 - acc: 0.4936 - val_loss: 1.0061 - val_acc: 0.4633\n",
      "Epoch 4/10\n",
      "3042/3042 [==============================] - 114s 37ms/step - loss: 0.9616 - acc: 0.5083 - val_loss: 0.9940 - val_acc: 0.4930\n",
      "Epoch 5/10\n",
      "3042/3042 [==============================] - 113s 37ms/step - loss: 0.9510 - acc: 0.5154 - val_loss: 1.0409 - val_acc: 0.4408\n",
      "Epoch 6/10\n",
      "3042/3042 [==============================] - 113s 37ms/step - loss: 0.9280 - acc: 0.5363 - val_loss: 1.1132 - val_acc: 0.3883\n",
      "Epoch 7/10\n",
      "3042/3042 [==============================] - 114s 37ms/step - loss: 0.9119 - acc: 0.5474 - val_loss: 1.1050 - val_acc: 0.4045\n",
      "Epoch 8/10\n",
      "3042/3042 [==============================] - 114s 37ms/step - loss: 0.8943 - acc: 0.5599 - val_loss: 1.1845 - val_acc: 0.3817\n",
      "Epoch 9/10\n",
      "3042/3042 [==============================] - 114s 37ms/step - loss: 0.8801 - acc: 0.5712 - val_loss: 1.1841 - val_acc: 0.3589\n",
      "Epoch 10/10\n",
      "3042/3042 [==============================] - 113s 37ms/step - loss: 0.8690 - acc: 0.5763 - val_loss: 1.2188 - val_acc: 0.3736\n",
      "Test score: 1.2191633364871928\n",
      "Test accuracy: 0.37351973684210527\n",
      "Stapelgroesse (batchSize): 32\n",
      "Anzahl der Faltungsschichten: 3Layer\n",
      "Aktivierungsfunktion: elu\n",
      "Optimierungsfunktion: Adam\n",
      "Dropout-Rate Faltungsschicht 1: 0.26045732045458037\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 32\n",
      "Dropout-Rate Faltungsschicht 2: 0.5553204526749792\n",
      "Anzahl der Faltungsschichten: 3Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 128\n",
      "Dropout-Rate Faltungsschicht 3: 0.3761602688998059\n",
      "Anzahl der Neuronen des Fully Connected Layer: 128\n",
      "Dropout-Rate Fully Connected Layer: 0.023263294966867562\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "1521/1521 [==============================] - 96s 63ms/step - loss: 1.1056 - acc: 0.4323 - val_loss: 2.3848 - val_acc: 0.4178\n",
      "Epoch 2/10\n",
      "1521/1521 [==============================] - 94s 62ms/step - loss: 1.0081 - acc: 0.4726 - val_loss: 1.9240 - val_acc: 0.3503\n",
      "Epoch 3/10\n",
      "1521/1521 [==============================] - 95s 62ms/step - loss: 0.9892 - acc: 0.4841 - val_loss: 1.7529 - val_acc: 0.4871\n",
      "Epoch 4/10\n",
      "1521/1521 [==============================] - 95s 62ms/step - loss: 0.9783 - acc: 0.4915 - val_loss: 1.7475 - val_acc: 0.3976\n",
      "Epoch 5/10\n",
      "1521/1521 [==============================] - 94s 62ms/step - loss: 0.9961 - acc: 0.4619 - val_loss: 0.9394 - val_acc: 0.5386\n",
      "Epoch 6/10\n",
      "1521/1521 [==============================] - 94s 62ms/step - loss: 0.9046 - acc: 0.5462 - val_loss: 0.9279 - val_acc: 0.5616\n",
      "Epoch 7/10\n",
      "1521/1521 [==============================] - 95s 62ms/step - loss: 0.8805 - acc: 0.5635 - val_loss: 0.8440 - val_acc: 0.5937\n",
      "Epoch 8/10\n",
      "1521/1521 [==============================] - 94s 62ms/step - loss: 0.8744 - acc: 0.5680 - val_loss: 0.8664 - val_acc: 0.5972\n",
      "Epoch 9/10\n",
      "1521/1521 [==============================] - 95s 62ms/step - loss: 0.8553 - acc: 0.5804 - val_loss: 0.8428 - val_acc: 0.6017\n",
      "Epoch 10/10\n",
      "1521/1521 [==============================] - 95s 62ms/step - loss: 0.8477 - acc: 0.5876 - val_loss: 0.8555 - val_acc: 0.5975\n",
      "Test score: 0.8553494884779579\n",
      "Test accuracy: 0.5974506578947368\n",
      "Stapelgroesse (batchSize): 8\n",
      "Anzahl der Faltungsschichten: 5Layer\n",
      "Aktivierungsfunktion: elu\n",
      "Optimierungsfunktion: RMSprop\n",
      "Dropout-Rate Faltungsschicht 1: 0.6861024010168819\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 64\n",
      "Dropout-Rate Faltungsschicht 2: 0.649927584574656\n",
      "Anzahl der Faltungsschichten: 5Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 64\n",
      "Dropout-Rate Faltungsschicht 3: 0.399162292849581\n",
      "Anzahl der Filter-Maps Faltungsschicht 4: 128\n",
      "Dropout-Rate Faltungsschicht 4: 0.4385232152116758\n",
      "Anzahl der Filter-Maps Faltungsschicht 5: 64\n",
      "Dropout-Rate Faltungsschicht 5: 0.13328636172166075\n",
      "Anzahl der Neuronen des Fully Connected Layer: 64\n",
      "Dropout-Rate Fully Connected Layer: 0.05644691958057099\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "6084/6084 [==============================] - 148s 24ms/step - loss: 1.1037 - acc: 0.4086 - val_loss: 1.1131 - val_acc: 0.4117\n",
      "Epoch 2/10\n",
      "6084/6084 [==============================] - 152s 25ms/step - loss: 1.0367 - acc: 0.4574 - val_loss: 0.9948 - val_acc: 0.4933\n",
      "Epoch 3/10\n",
      "6084/6084 [==============================] - 153s 25ms/step - loss: 1.0219 - acc: 0.4752 - val_loss: 0.9996 - val_acc: 0.5344\n",
      "Epoch 4/10\n",
      "6084/6084 [==============================] - 152s 25ms/step - loss: 1.0222 - acc: 0.4762 - val_loss: 1.3516 - val_acc: 0.3851\n",
      "Epoch 5/10\n",
      "6084/6084 [==============================] - 153s 25ms/step - loss: 1.0314 - acc: 0.4709 - val_loss: 1.1399 - val_acc: 0.3868\n",
      "Epoch 6/10\n",
      "6084/6084 [==============================] - 155s 26ms/step - loss: 1.0241 - acc: 0.4815 - val_loss: 1.1868 - val_acc: 0.3906\n",
      "Epoch 7/10\n",
      "6084/6084 [==============================] - 147s 24ms/step - loss: 1.0276 - acc: 0.4776 - val_loss: 4.4679 - val_acc: 0.4458\n",
      "Epoch 8/10\n",
      "6084/6084 [==============================] - 149s 24ms/step - loss: 1.0386 - acc: 0.4746 - val_loss: 2.7179 - val_acc: 0.4393\n",
      "Epoch 9/10\n",
      "6084/6084 [==============================] - 148s 24ms/step - loss: 1.0412 - acc: 0.4756 - val_loss: 3.8056 - val_acc: 0.4218\n",
      "Epoch 10/10\n",
      "6084/6084 [==============================] - 201s 33ms/step - loss: 1.0376 - acc: 0.4755 - val_loss: 7.1466 - val_acc: 0.3428\n",
      "Test score: 7.145932047168022\n",
      "Test accuracy: 0.3428665351742275\n",
      "Stapelgroesse (batchSize): 16\n",
      "Anzahl der Faltungsschichten: 3Layer\n",
      "Aktivierungsfunktion: elu\n",
      "Optimierungsfunktion: Adam\n",
      "Dropout-Rate Faltungsschicht 1: 0.3355825176936589\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 64\n",
      "Dropout-Rate Faltungsschicht 2: 0.2666132242226218\n",
      "Anzahl der Faltungsschichten: 3Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 64\n",
      "Dropout-Rate Faltungsschicht 3: 0.2965316398645662\n",
      "Anzahl der Neuronen des Fully Connected Layer: 128\n",
      "Dropout-Rate Fully Connected Layer: 0.15986691790317023\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "3042/3042 [==============================] - 166s 55ms/step - loss: 1.0726 - acc: 0.4375 - val_loss: 1.0835 - val_acc: 0.3900\n",
      "Epoch 2/10\n",
      "3042/3042 [==============================] - 115s 38ms/step - loss: 0.9759 - acc: 0.4934 - val_loss: 0.9151 - val_acc: 0.5355\n",
      "Epoch 3/10\n",
      "3042/3042 [==============================] - 115s 38ms/step - loss: 0.9158 - acc: 0.5424 - val_loss: 0.8812 - val_acc: 0.5500\n",
      "Epoch 4/10\n",
      "3042/3042 [==============================] - 115s 38ms/step - loss: 0.8866 - acc: 0.5615 - val_loss: 0.8708 - val_acc: 0.5476\n",
      "Epoch 5/10\n",
      "3042/3042 [==============================] - 116s 38ms/step - loss: 0.8665 - acc: 0.5734 - val_loss: 1.2010 - val_acc: 0.4517\n",
      "Epoch 6/10\n",
      "3042/3042 [==============================] - 116s 38ms/step - loss: 0.8489 - acc: 0.5836 - val_loss: 0.8936 - val_acc: 0.5722\n",
      "Epoch 7/10\n",
      "3042/3042 [==============================] - 115s 38ms/step - loss: 0.8303 - acc: 0.5897 - val_loss: 0.9416 - val_acc: 0.5667\n",
      "Epoch 8/10\n",
      "3042/3042 [==============================] - 115s 38ms/step - loss: 0.8153 - acc: 0.5991 - val_loss: 0.9194 - val_acc: 0.5602\n",
      "Epoch 9/10\n",
      "3042/3042 [==============================] - 116s 38ms/step - loss: 0.7988 - acc: 0.6069 - val_loss: 1.1296 - val_acc: 0.5299\n",
      "Epoch 10/10\n",
      "3042/3042 [==============================] - 119s 39ms/step - loss: 0.7823 - acc: 0.6177 - val_loss: 0.9882 - val_acc: 0.5555\n",
      "Test score: 0.988474285837851\n",
      "Test accuracy: 0.5553453947368421\n",
      "Stapelgroesse (batchSize): 32\n",
      "Anzahl der Faltungsschichten: 4Layer\n",
      "Aktivierungsfunktion: relu\n",
      "Optimierungsfunktion: RMSprop\n",
      "Dropout-Rate Faltungsschicht 1: 0.5248675502930554\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 32\n",
      "Dropout-Rate Faltungsschicht 2: 0.6521929535480597\n",
      "Anzahl der Faltungsschichten: 4Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 64\n",
      "Dropout-Rate Faltungsschicht 3: 0.26105331432376633\n",
      "Anzahl der Filter-Maps Faltungsschicht 4: 64\n",
      "Dropout-Rate Faltungsschicht 4: 0.6957910099955584\n",
      "Anzahl der Neuronen des Fully Connected Layer: 128\n",
      "Dropout-Rate Fully Connected Layer: 0.21781501997478092\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1521/1521 [==============================] - 85s 56ms/step - loss: 1.0669 - acc: 0.4209 - val_loss: 1.0403 - val_acc: 0.4282\n",
      "Epoch 2/10\n",
      "1521/1521 [==============================] - 84s 55ms/step - loss: 1.0219 - acc: 0.4654 - val_loss: 1.0378 - val_acc: 0.4424\n",
      "Epoch 3/10\n",
      "1521/1521 [==============================] - 82s 54ms/step - loss: 1.0058 - acc: 0.4817 - val_loss: 1.0554 - val_acc: 0.4093\n",
      "Epoch 4/10\n",
      "1521/1521 [==============================] - 81s 53ms/step - loss: 0.9927 - acc: 0.4914 - val_loss: 1.0571 - val_acc: 0.4268\n",
      "Epoch 5/10\n",
      "1521/1521 [==============================] - 81s 53ms/step - loss: 0.9836 - acc: 0.5029 - val_loss: 1.0247 - val_acc: 0.4509\n",
      "Epoch 6/10\n",
      "1521/1521 [==============================] - 81s 53ms/step - loss: 0.9732 - acc: 0.5121 - val_loss: 1.0696 - val_acc: 0.3978\n",
      "Epoch 7/10\n",
      "1521/1521 [==============================] - 81s 53ms/step - loss: 0.9717 - acc: 0.5151 - val_loss: 1.0801 - val_acc: 0.3997\n",
      "Epoch 8/10\n",
      "1521/1521 [==============================] - 81s 53ms/step - loss: 0.9607 - acc: 0.5216 - val_loss: 1.1391 - val_acc: 0.3509\n",
      "Epoch 9/10\n",
      "1521/1521 [==============================] - 81s 53ms/step - loss: 0.9576 - acc: 0.5229 - val_loss: 1.0624 - val_acc: 0.4076\n",
      "Epoch 10/10\n",
      "1521/1521 [==============================] - 81s 53ms/step - loss: 1.1343 - acc: 0.5213 - val_loss: 1.0462 - val_acc: 0.4253\n",
      "Test score: 1.046307374929127\n",
      "Test accuracy: 0.4252467105263158\n",
      "Stapelgroesse (batchSize): 16\n",
      "Anzahl der Faltungsschichten: 5Layer\n",
      "Aktivierungsfunktion: relu\n",
      "Optimierungsfunktion: RMSprop\n",
      "Dropout-Rate Faltungsschicht 1: 0.2888234898361773\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 32\n",
      "Dropout-Rate Faltungsschicht 2: 0.23421018558252513\n",
      "Anzahl der Faltungsschichten: 5Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 128\n",
      "Dropout-Rate Faltungsschicht 3: 0.6236168941633805\n",
      "Anzahl der Filter-Maps Faltungsschicht 4: 128\n",
      "Dropout-Rate Faltungsschicht 4: 0.45319110742946134\n",
      "Anzahl der Filter-Maps Faltungsschicht 5: 128\n",
      "Dropout-Rate Faltungsschicht 5: 0.5711831948196173\n",
      "Anzahl der Neuronen des Fully Connected Layer: 64\n",
      "Dropout-Rate Fully Connected Layer: 0.5509464626270063\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "3042/3042 [==============================] - 109s 36ms/step - loss: 1.0861 - acc: 0.3775 - val_loss: 1.0567 - val_acc: 0.4133\n",
      "Epoch 2/10\n",
      "3042/3042 [==============================] - 107s 35ms/step - loss: 1.0414 - acc: 0.4534 - val_loss: 1.0377 - val_acc: 0.4620\n",
      "Epoch 3/10\n",
      "3042/3042 [==============================] - 107s 35ms/step - loss: 1.0306 - acc: 0.4731 - val_loss: 1.1059 - val_acc: 0.3921\n",
      "Epoch 4/10\n",
      "3042/3042 [==============================] - 107s 35ms/step - loss: 1.0210 - acc: 0.4900 - val_loss: 1.0836 - val_acc: 0.4339\n",
      "Epoch 5/10\n",
      "3042/3042 [==============================] - 107s 35ms/step - loss: 1.0152 - acc: 0.4942 - val_loss: 1.0947 - val_acc: 0.4219\n",
      "Epoch 6/10\n",
      "3042/3042 [==============================] - 107s 35ms/step - loss: 1.0335 - acc: 0.4954 - val_loss: 1.0540 - val_acc: 0.4485\n",
      "Epoch 7/10\n",
      "3042/3042 [==============================] - 107s 35ms/step - loss: 1.0296 - acc: 0.4960 - val_loss: 1.0560 - val_acc: 0.3617\n",
      "Epoch 8/10\n",
      "3042/3042 [==============================] - 107s 35ms/step - loss: 1.0571 - acc: 0.5004 - val_loss: 1.1364 - val_acc: 0.3774\n",
      "Epoch 9/10\n",
      "3042/3042 [==============================] - 107s 35ms/step - loss: 1.0278 - acc: 0.4973 - val_loss: 1.1721 - val_acc: 0.3455\n",
      "Epoch 10/10\n",
      "3042/3042 [==============================] - 107s 35ms/step - loss: 1.0172 - acc: 0.4964 - val_loss: 1.1942 - val_acc: 0.3645\n",
      "Test score: 1.1940337749688248\n",
      "Test accuracy: 0.36463815789473686\n",
      "Stapelgroesse (batchSize): 32\n",
      "Anzahl der Faltungsschichten: 3Layer\n",
      "Aktivierungsfunktion: elu\n",
      "Optimierungsfunktion: Adam\n",
      "Dropout-Rate Faltungsschicht 1: 0.1411873881737138\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 32\n",
      "Dropout-Rate Faltungsschicht 2: 0.43940119562733904\n",
      "Anzahl der Faltungsschichten: 3Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 64\n",
      "Dropout-Rate Faltungsschicht 3: 0.6827140018946355\n",
      "Anzahl der Neuronen des Fully Connected Layer: 128\n",
      "Dropout-Rate Fully Connected Layer: 0.5212403354707789\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "1521/1521 [==============================] - 86s 57ms/step - loss: 1.0631 - acc: 0.4466 - val_loss: 1.0066 - val_acc: 0.4583: 5s - loss: 1\n",
      "Epoch 2/10\n",
      "1521/1521 [==============================] - 405s 266ms/step - loss: 0.9914 - acc: 0.4871 - val_loss: 0.9325 - val_acc: 0.5371\n",
      "Epoch 3/10\n",
      "1521/1521 [==============================] - 141s 93ms/step - loss: 0.9486 - acc: 0.5180 - val_loss: 0.8693 - val_acc: 0.5791\n",
      "Epoch 4/10\n",
      "1521/1521 [==============================] - 84s 55ms/step - loss: 0.9146 - acc: 0.5455 - val_loss: 0.8649 - val_acc: 0.5821\n",
      "Epoch 5/10\n",
      "1521/1521 [==============================] - 84s 55ms/step - loss: 0.8935 - acc: 0.5576 - val_loss: 0.8608 - val_acc: 0.5867\n",
      "Epoch 6/10\n",
      "1521/1521 [==============================] - 84s 55ms/step - loss: 0.8842 - acc: 0.5644 - val_loss: 0.8476 - val_acc: 0.5914\n",
      "Epoch 7/10\n",
      "1521/1521 [==============================] - 84s 55ms/step - loss: 0.8766 - acc: 0.5695 - val_loss: 0.8311 - val_acc: 0.6072\n",
      "Epoch 8/10\n",
      "1521/1521 [==============================] - 84s 55ms/step - loss: 0.8749 - acc: 0.5694 - val_loss: 0.8381 - val_acc: 0.6030\n",
      "Epoch 9/10\n",
      "1521/1521 [==============================] - 84s 55ms/step - loss: 0.8713 - acc: 0.5737 - val_loss: 0.8228 - val_acc: 0.6064\n",
      "Epoch 10/10\n",
      "1521/1521 [==============================] - 84s 55ms/step - loss: 0.8710 - acc: 0.5755 - val_loss: 0.8329 - val_acc: 0.5973\n",
      "Test score: 0.8331705294157329\n",
      "Test accuracy: 0.5971217105263158\n",
      "Stapelgroesse (batchSize): 32\n",
      "Anzahl der Faltungsschichten: 4Layer\n",
      "Aktivierungsfunktion: elu\n",
      "Optimierungsfunktion: Adam\n",
      "Dropout-Rate Faltungsschicht 1: 0.5618022521290307\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 32\n",
      "Dropout-Rate Faltungsschicht 2: 0.43180700701104185\n",
      "Anzahl der Faltungsschichten: 4Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 64\n",
      "Dropout-Rate Faltungsschicht 3: 0.6300135909702449\n",
      "Anzahl der Filter-Maps Faltungsschicht 4: 64\n",
      "Dropout-Rate Faltungsschicht 4: 0.3705274468787073\n",
      "Anzahl der Neuronen des Fully Connected Layer: 128\n",
      "Dropout-Rate Fully Connected Layer: 0.0007036970434920242\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "1521/1521 [==============================] - 85s 56ms/step - loss: 1.0484 - acc: 0.4412 - val_loss: 1.0298 - val_acc: 0.4631\n",
      "Epoch 2/10\n",
      "1521/1521 [==============================] - 84s 55ms/step - loss: 1.0028 - acc: 0.4745 - val_loss: 1.0190 - val_acc: 0.4735\n",
      "Epoch 3/10\n",
      "1521/1521 [==============================] - 84s 55ms/step - loss: 0.9650 - acc: 0.5061 - val_loss: 0.9512 - val_acc: 0.5228\n",
      "Epoch 4/10\n",
      "1521/1521 [==============================] - 84s 55ms/step - loss: 0.9361 - acc: 0.5317 - val_loss: 0.9677 - val_acc: 0.5599\n",
      "Epoch 5/10\n",
      "1521/1521 [==============================] - 83s 55ms/step - loss: 0.9164 - acc: 0.5429 - val_loss: 0.9301 - val_acc: 0.5348\n",
      "Epoch 6/10\n",
      "1521/1521 [==============================] - 84s 55ms/step - loss: 0.8996 - acc: 0.5548 - val_loss: 1.1168 - val_acc: 0.4887\n",
      "Epoch 7/10\n",
      "1521/1521 [==============================] - 84s 55ms/step - loss: 0.8858 - acc: 0.5636 - val_loss: 0.8574 - val_acc: 0.5895\n",
      "Epoch 8/10\n",
      "1521/1521 [==============================] - 84s 55ms/step - loss: 0.8748 - acc: 0.5706 - val_loss: 0.8966 - val_acc: 0.5831\n",
      "Epoch 9/10\n",
      "1521/1521 [==============================] - 83s 55ms/step - loss: 0.8666 - acc: 0.5759 - val_loss: 0.8579 - val_acc: 0.5912\n",
      "Epoch 10/10\n",
      "1521/1521 [==============================] - 84s 55ms/step - loss: 0.8606 - acc: 0.5795 - val_loss: 0.9831 - val_acc: 0.5683\n",
      "Test score: 0.9832243426849968\n",
      "Test accuracy: 0.5679276315789473\n",
      "Stapelgroesse (batchSize): 8\n",
      "Anzahl der Faltungsschichten: 3Layer\n",
      "Aktivierungsfunktion: elu\n",
      "Optimierungsfunktion: Adam\n",
      "Dropout-Rate Faltungsschicht 1: 0.22696402957324682\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 64\n",
      "Dropout-Rate Faltungsschicht 2: 0.15295936298124646\n",
      "Anzahl der Faltungsschichten: 3Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 128\n",
      "Dropout-Rate Faltungsschicht 3: 0.04614877235030587\n",
      "Anzahl der Neuronen des Fully Connected Layer: 64\n",
      "Dropout-Rate Fully Connected Layer: 0.18949638026428298\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "6084/6084 [==============================] - 155s 26ms/step - loss: 10.7428 - acc: 0.3332 - val_loss: 10.7322 - val_acc: 0.3342\n",
      "Epoch 2/10\n",
      "6084/6084 [==============================] - 154s 25ms/step - loss: 10.7490 - acc: 0.3331 - val_loss: 10.7335 - val_acc: 0.3341\n",
      "Epoch 3/10\n",
      "6084/6084 [==============================] - 154s 25ms/step - loss: 10.7494 - acc: 0.3331 - val_loss: 10.7361 - val_acc: 0.3339\n",
      "Epoch 4/10\n",
      "6084/6084 [==============================] - 154s 25ms/step - loss: 10.7497 - acc: 0.3331 - val_loss: 10.7361 - val_acc: 0.3339\n",
      "Epoch 5/10\n",
      "6084/6084 [==============================] - 154s 25ms/step - loss: 10.7500 - acc: 0.3330 - val_loss: 10.7348 - val_acc: 0.3340\n",
      "Epoch 6/10\n",
      "6084/6084 [==============================] - 154s 25ms/step - loss: 10.7494 - acc: 0.3331 - val_loss: 10.7348 - val_acc: 0.3340\n",
      "Epoch 7/10\n",
      "6084/6084 [==============================] - 154s 25ms/step - loss: 10.7494 - acc: 0.3331 - val_loss: 10.7308 - val_acc: 0.3342\n",
      "Epoch 8/10\n",
      "6084/6084 [==============================] - 154s 25ms/step - loss: 10.7497 - acc: 0.3331 - val_loss: 10.7348 - val_acc: 0.3340\n",
      "Epoch 9/10\n",
      "6084/6084 [==============================] - 154s 25ms/step - loss: 10.7497 - acc: 0.3331 - val_loss: 10.7321 - val_acc: 0.3342\n",
      "Epoch 10/10\n",
      "6084/6084 [==============================] - 154s 25ms/step - loss: 10.7497 - acc: 0.3331 - val_loss: 10.7361 - val_acc: 0.3339\n",
      "Test score: 10.732150633973092\n",
      "Test accuracy: 0.334155161078238\n",
      "Stapelgroesse (batchSize): 16\n",
      "Anzahl der Faltungsschichten: 4Layer\n",
      "Aktivierungsfunktion: elu\n",
      "Optimierungsfunktion: Adam\n",
      "Dropout-Rate Faltungsschicht 1: 0.502566005987901\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 64\n",
      "Dropout-Rate Faltungsschicht 2: 0.1998550853505832\n",
      "Anzahl der Faltungsschichten: 4Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 64\n",
      "Dropout-Rate Faltungsschicht 3: 0.3759860620037276\n",
      "Anzahl der Filter-Maps Faltungsschicht 4: 128\n",
      "Dropout-Rate Faltungsschicht 4: 0.5196250585612635\n",
      "Anzahl der Neuronen des Fully Connected Layer: 128\n",
      "Dropout-Rate Fully Connected Layer: 0.008207557723527914\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "3042/3042 [==============================] - 117s 38ms/step - loss: 1.1192 - acc: 0.3521 - val_loss: 1.1120 - val_acc: 0.3323\n",
      "Epoch 2/10\n",
      "3042/3042 [==============================] - 116s 38ms/step - loss: 1.1132 - acc: 0.3300 - val_loss: 1.1105 - val_acc: 0.3325\n",
      "Epoch 3/10\n",
      "3042/3042 [==============================] - 116s 38ms/step - loss: 1.1133 - acc: 0.3291 - val_loss: 1.1119 - val_acc: 0.3325\n",
      "Epoch 4/10\n",
      "3042/3042 [==============================] - 115s 38ms/step - loss: 1.1127 - acc: 0.3284 - val_loss: 1.1081 - val_acc: 0.3325\n",
      "Epoch 5/10\n",
      "3042/3042 [==============================] - 115s 38ms/step - loss: 1.1120 - acc: 0.3296 - val_loss: 1.1049 - val_acc: 0.3323\n",
      "Epoch 6/10\n",
      "3042/3042 [==============================] - 116s 38ms/step - loss: 1.1117 - acc: 0.3304 - val_loss: 1.1019 - val_acc: 0.3323\n",
      "Epoch 7/10\n",
      "3042/3042 [==============================] - 116s 38ms/step - loss: 1.1108 - acc: 0.3323 - val_loss: 1.1007 - val_acc: 0.3323\n",
      "Epoch 8/10\n",
      "3042/3042 [==============================] - 116s 38ms/step - loss: 1.1114 - acc: 0.3331 - val_loss: 1.0998 - val_acc: 0.3323\n",
      "Epoch 9/10\n",
      "3042/3042 [==============================] - 116s 38ms/step - loss: 1.1111 - acc: 0.3290 - val_loss: 1.0987 - val_acc: 0.3325\n",
      "Epoch 10/10\n",
      "3042/3042 [==============================] - 115s 38ms/step - loss: 1.1108 - acc: 0.3318 - val_loss: 1.0989 - val_acc: 0.3337\n",
      "Test score: 1.0989086941668862\n",
      "Test accuracy: 0.3335526315789474\n",
      "Stapelgroesse (batchSize): 16\n",
      "Anzahl der Faltungsschichten: 4Layer\n",
      "Aktivierungsfunktion: elu\n",
      "Optimierungsfunktion: RMSprop\n",
      "Dropout-Rate Faltungsschicht 1: 0.20482279996588446\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 32\n",
      "Dropout-Rate Faltungsschicht 2: 0.5455088012568097\n",
      "Anzahl der Faltungsschichten: 4Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 64\n",
      "Dropout-Rate Faltungsschicht 3: 0.5763085244479565\n",
      "Anzahl der Filter-Maps Faltungsschicht 4: 128\n",
      "Dropout-Rate Faltungsschicht 4: 0.5310129254705536\n",
      "Anzahl der Neuronen des Fully Connected Layer: 64\n",
      "Dropout-Rate Fully Connected Layer: 0.004221254827684406\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "3042/3042 [==============================] - 102s 33ms/step - loss: 1.2013 - acc: 0.4181 - val_loss: 1.0200 - val_acc: 0.4454\n",
      "Epoch 2/10\n",
      "3042/3042 [==============================] - 100s 33ms/step - loss: 1.0288 - acc: 0.4651 - val_loss: 0.9104 - val_acc: 0.5439\n",
      "Epoch 3/10\n",
      "3042/3042 [==============================] - 101s 33ms/step - loss: 0.9536 - acc: 0.5269 - val_loss: 0.9624 - val_acc: 0.4843\n",
      "Epoch 4/10\n",
      "3042/3042 [==============================] - 100s 33ms/step - loss: 0.9220 - acc: 0.5434 - val_loss: 1.1371 - val_acc: 0.4627\n",
      "Epoch 5/10\n",
      "3042/3042 [==============================] - 100s 33ms/step - loss: 0.9079 - acc: 0.5553 - val_loss: 0.9897 - val_acc: 0.4613\n",
      "Epoch 6/10\n",
      "3042/3042 [==============================] - 101s 33ms/step - loss: 0.9044 - acc: 0.5554 - val_loss: 0.8598 - val_acc: 0.5930\n",
      "Epoch 7/10\n",
      "3042/3042 [==============================] - 101s 33ms/step - loss: 0.9007 - acc: 0.5577 - val_loss: 0.9805 - val_acc: 0.5606\n",
      "Epoch 8/10\n",
      "3042/3042 [==============================] - 101s 33ms/step - loss: 0.9028 - acc: 0.5574 - val_loss: 0.9284 - val_acc: 0.5383\n",
      "Epoch 9/10\n",
      "3042/3042 [==============================] - 101s 33ms/step - loss: 0.9044 - acc: 0.5554 - val_loss: 0.9108 - val_acc: 0.5561\n",
      "Epoch 10/10\n",
      "3042/3042 [==============================] - 101s 33ms/step - loss: 0.9034 - acc: 0.5578 - val_loss: 0.8931 - val_acc: 0.5538\n",
      "Test score: 0.8932697055371184\n",
      "Test accuracy: 0.5537006578947369\n",
      "Stapelgroesse (batchSize): 8\n",
      "Anzahl der Faltungsschichten: 3Layer\n",
      "Aktivierungsfunktion: relu\n",
      "Optimierungsfunktion: Adam\n",
      "Dropout-Rate Faltungsschicht 1: 0.6859801490268133\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 32\n",
      "Dropout-Rate Faltungsschicht 2: 0.3567663629232832\n",
      "Anzahl der Faltungsschichten: 3Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 128\n",
      "Dropout-Rate Faltungsschicht 3: 0.6398422280118664\n",
      "Anzahl der Neuronen des Fully Connected Layer: 128\n",
      "Dropout-Rate Fully Connected Layer: 0.3468418591094041\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "6084/6084 [==============================] - 161s 26ms/step - loss: 1.1058 - acc: 0.3347 - val_loss: 1.0988 - val_acc: 0.3323\n",
      "Epoch 2/10\n",
      "6084/6084 [==============================] - 159s 26ms/step - loss: 1.0988 - acc: 0.3330 - val_loss: 1.0988 - val_acc: 0.3325\n",
      "Epoch 3/10\n",
      "6084/6084 [==============================] - 160s 26ms/step - loss: 1.0988 - acc: 0.3330 - val_loss: 1.0988 - val_acc: 0.3323\n",
      "Epoch 4/10\n",
      "6084/6084 [==============================] - 159s 26ms/step - loss: 1.0989 - acc: 0.3319 - val_loss: 1.0989 - val_acc: 0.3323\n",
      "Epoch 5/10\n",
      "6084/6084 [==============================] - 159s 26ms/step - loss: 1.0988 - acc: 0.3321 - val_loss: 1.0988 - val_acc: 0.3323\n",
      "Epoch 6/10\n",
      "6084/6084 [==============================] - 159s 26ms/step - loss: 1.0988 - acc: 0.3321 - val_loss: 1.0988 - val_acc: 0.3322\n",
      "Epoch 7/10\n",
      "6084/6084 [==============================] - 159s 26ms/step - loss: 1.0988 - acc: 0.3321 - val_loss: 1.0988 - val_acc: 0.3324\n",
      "Epoch 8/10\n",
      "6084/6084 [==============================] - 160s 26ms/step - loss: 1.0988 - acc: 0.3321 - val_loss: 1.0988 - val_acc: 0.3324\n",
      "Epoch 9/10\n",
      "6084/6084 [==============================] - 159s 26ms/step - loss: 1.0988 - acc: 0.3321 - val_loss: 1.0988 - val_acc: 0.3323\n",
      "Epoch 10/10\n",
      "6084/6084 [==============================] - 159s 26ms/step - loss: 1.0988 - acc: 0.3321 - val_loss: 1.0988 - val_acc: 0.3324\n",
      "Test score: 1.0988241953727527\n",
      "Test accuracy: 0.33226495726495725\n",
      "Stapelgroesse (batchSize): 16\n",
      "Anzahl der Faltungsschichten: 4Layer\n",
      "Aktivierungsfunktion: relu\n",
      "Optimierungsfunktion: Adam\n",
      "Dropout-Rate Faltungsschicht 1: 0.5307933999683194\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 32\n",
      "Dropout-Rate Faltungsschicht 2: 0.1449955280636636\n",
      "Anzahl der Faltungsschichten: 4Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 128\n",
      "Dropout-Rate Faltungsschicht 3: 0.04730330293000361\n",
      "Anzahl der Filter-Maps Faltungsschicht 4: 64\n",
      "Dropout-Rate Faltungsschicht 4: 0.37732292891116026\n",
      "Anzahl der Neuronen des Fully Connected Layer: 64\n",
      "Dropout-Rate Fully Connected Layer: 0.10028084027027029\n",
      "Faltungsnetz wird trainiert...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3042/3042 [==============================] - 104s 34ms/step - loss: 1.0990 - acc: 0.3322 - val_loss: 1.0988 - val_acc: 0.3323\n",
      "Epoch 2/10\n",
      "3042/3042 [==============================] - 102s 34ms/step - loss: 1.0987 - acc: 0.3334 - val_loss: 1.0988 - val_acc: 0.3325\n",
      "Epoch 3/10\n",
      "3042/3042 [==============================] - 102s 34ms/step - loss: 1.0987 - acc: 0.3339 - val_loss: 1.0988 - val_acc: 0.3323\n",
      "Epoch 4/10\n",
      "3042/3042 [==============================] - 102s 34ms/step - loss: 1.0987 - acc: 0.3337 - val_loss: 1.0988 - val_acc: 0.3323\n",
      "Epoch 5/10\n",
      "3042/3042 [==============================] - 102s 34ms/step - loss: 1.0987 - acc: 0.3337 - val_loss: 1.0988 - val_acc: 0.3323\n",
      "Epoch 6/10\n",
      "3042/3042 [==============================] - 102s 34ms/step - loss: 1.0987 - acc: 0.3337 - val_loss: 1.0988 - val_acc: 0.3323\n",
      "Epoch 7/10\n",
      "3042/3042 [==============================] - 103s 34ms/step - loss: 1.0987 - acc: 0.3337 - val_loss: 1.0988 - val_acc: 0.3323\n",
      "Epoch 8/10\n",
      "3042/3042 [==============================] - 102s 34ms/step - loss: 1.0987 - acc: 0.3338 - val_loss: 1.0988 - val_acc: 0.3323\n",
      "Epoch 9/10\n",
      "3042/3042 [==============================] - 102s 34ms/step - loss: 1.0987 - acc: 0.3337 - val_loss: 1.0988 - val_acc: 0.3323\n",
      "Epoch 10/10\n",
      "3042/3042 [==============================] - 103s 34ms/step - loss: 1.0987 - acc: 0.3338 - val_loss: 1.0988 - val_acc: 0.3323\n",
      "Test score: 1.0987519544990438\n",
      "Test accuracy: 0.3323190789473684\n",
      "Stapelgroesse (batchSize): 16\n",
      "Anzahl der Faltungsschichten: 4Layer\n",
      "Aktivierungsfunktion: elu\n",
      "Optimierungsfunktion: Adam\n",
      "Dropout-Rate Faltungsschicht 1: 0.4831880841923193\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 32\n",
      "Dropout-Rate Faltungsschicht 2: 0.4218928627980383\n",
      "Anzahl der Faltungsschichten: 4Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 64\n",
      "Dropout-Rate Faltungsschicht 3: 0.4360259891153708\n",
      "Anzahl der Filter-Maps Faltungsschicht 4: 128\n",
      "Dropout-Rate Faltungsschicht 4: 0.4074207844976787\n",
      "Anzahl der Neuronen des Fully Connected Layer: 128\n",
      "Dropout-Rate Fully Connected Layer: 0.5411342377700985\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "3042/3042 [==============================] - 105s 34ms/step - loss: 1.0582 - acc: 0.4389 - val_loss: 1.0110 - val_acc: 0.4437\n",
      "Epoch 2/10\n",
      "3042/3042 [==============================] - 104s 34ms/step - loss: 1.0256 - acc: 0.4574 - val_loss: 1.0219 - val_acc: 0.4442\n",
      "Epoch 3/10\n",
      "3042/3042 [==============================] - 104s 34ms/step - loss: 1.0143 - acc: 0.4719 - val_loss: 0.9639 - val_acc: 0.5209\n",
      "Epoch 4/10\n",
      "3042/3042 [==============================] - 103s 34ms/step - loss: 0.9919 - acc: 0.4864 - val_loss: 0.9873 - val_acc: 0.4055\n",
      "Epoch 5/10\n",
      "3042/3042 [==============================] - 104s 34ms/step - loss: 0.9850 - acc: 0.4905 - val_loss: 0.9750 - val_acc: 0.4695\n",
      "Epoch 6/10\n",
      "3042/3042 [==============================] - 104s 34ms/step - loss: 0.9814 - acc: 0.4938 - val_loss: 0.8979 - val_acc: 0.5662\n",
      "Epoch 7/10\n",
      "3042/3042 [==============================] - 104s 34ms/step - loss: 0.9811 - acc: 0.4921 - val_loss: 0.9779 - val_acc: 0.5186\n",
      "Epoch 8/10\n",
      "3042/3042 [==============================] - 104s 34ms/step - loss: 0.9856 - acc: 0.4845 - val_loss: 1.0374 - val_acc: 0.4282\n",
      "Epoch 9/10\n",
      "3042/3042 [==============================] - 104s 34ms/step - loss: 0.9919 - acc: 0.4798 - val_loss: 0.9703 - val_acc: 0.4708\n",
      "Epoch 10/10\n",
      "3042/3042 [==============================] - 104s 34ms/step - loss: 0.9970 - acc: 0.4751 - val_loss: 2.5703 - val_acc: 0.3865\n",
      "Test score: 2.5708897152229357\n",
      "Test accuracy: 0.38626644736842103\n",
      "Stapelgroesse (batchSize): 8\n",
      "Anzahl der Faltungsschichten: 3Layer\n",
      "Aktivierungsfunktion: relu\n",
      "Optimierungsfunktion: Adam\n",
      "Dropout-Rate Faltungsschicht 1: 0.004402839931825553\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 64\n",
      "Dropout-Rate Faltungsschicht 2: 0.0016145402093660887\n",
      "Anzahl der Faltungsschichten: 3Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 64\n",
      "Dropout-Rate Faltungsschicht 3: 0.0908218196803085\n",
      "Anzahl der Neuronen des Fully Connected Layer: 128\n",
      "Dropout-Rate Fully Connected Layer: 0.2833657173581164\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "6084/6084 [==============================] - 148s 24ms/step - loss: 1.0991 - acc: 0.3342 - val_loss: 1.0988 - val_acc: 0.3323\n",
      "Epoch 2/10\n",
      "6084/6084 [==============================] - 147s 24ms/step - loss: 1.0988 - acc: 0.3321 - val_loss: 1.0988 - val_acc: 0.3323\n",
      "Epoch 3/10\n",
      "6084/6084 [==============================] - 147s 24ms/step - loss: 1.0988 - acc: 0.3321 - val_loss: 1.0988 - val_acc: 0.3323\n",
      "Epoch 4/10\n",
      "6084/6084 [==============================] - 146s 24ms/step - loss: 1.0988 - acc: 0.3321 - val_loss: 1.0988 - val_acc: 0.3322\n",
      "Epoch 5/10\n",
      "6084/6084 [==============================] - 146s 24ms/step - loss: 1.0988 - acc: 0.3321 - val_loss: 1.0988 - val_acc: 0.3322\n",
      "Epoch 6/10\n",
      "6084/6084 [==============================] - 147s 24ms/step - loss: 1.0988 - acc: 0.3321 - val_loss: 1.0988 - val_acc: 0.3322\n",
      "Epoch 7/10\n",
      "6084/6084 [==============================] - 147s 24ms/step - loss: 1.0988 - acc: 0.3321 - val_loss: 1.0988 - val_acc: 0.3322\n",
      "Epoch 8/10\n",
      "6084/6084 [==============================] - 147s 24ms/step - loss: 1.0988 - acc: 0.3321 - val_loss: 1.0988 - val_acc: 0.3322\n",
      "Epoch 9/10\n",
      "6084/6084 [==============================] - 146s 24ms/step - loss: 1.0988 - acc: 0.3321 - val_loss: 1.0988 - val_acc: 0.3324\n",
      "Epoch 10/10\n",
      "6084/6084 [==============================] - 147s 24ms/step - loss: 1.0988 - acc: 0.3321 - val_loss: 1.0988 - val_acc: 0.3324\n",
      "Test score: 1.0988241953727527\n",
      "Test accuracy: 0.33226495726495725\n",
      "Stapelgroesse (batchSize): 8\n",
      "Anzahl der Faltungsschichten: 3Layer\n",
      "Aktivierungsfunktion: elu\n",
      "Optimierungsfunktion: RMSprop\n",
      "Dropout-Rate Faltungsschicht 1: 0.16247928184812713\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 64\n",
      "Dropout-Rate Faltungsschicht 2: 0.5862336116278009\n",
      "Anzahl der Faltungsschichten: 3Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 128\n",
      "Dropout-Rate Faltungsschicht 3: 0.3396749535127678\n",
      "Anzahl der Neuronen des Fully Connected Layer: 64\n",
      "Dropout-Rate Fully Connected Layer: 0.271304795333835\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "6084/6084 [==============================] - 153s 25ms/step - loss: 10.7386 - acc: 0.3336 - val_loss: 10.7414 - val_acc: 0.3336\n",
      "Epoch 2/10\n",
      "6084/6084 [==============================] - 152s 25ms/step - loss: 10.7501 - acc: 0.3330 - val_loss: 10.7414 - val_acc: 0.3336\n",
      "Epoch 3/10\n",
      "6084/6084 [==============================] - 152s 25ms/step - loss: 10.7508 - acc: 0.3330 - val_loss: 10.7414 - val_acc: 0.3336\n",
      "Epoch 4/10\n",
      "6084/6084 [==============================] - 152s 25ms/step - loss: 10.7462 - acc: 0.3331 - val_loss: 10.7414 - val_acc: 0.3336\n",
      "Epoch 5/10\n",
      "6084/6084 [==============================] - 152s 25ms/step - loss: 10.7099 - acc: 0.3354 - val_loss: 10.7321 - val_acc: 0.3342\n",
      "Epoch 6/10\n",
      "6084/6084 [==============================] - 152s 25ms/step - loss: 10.7494 - acc: 0.3331 - val_loss: 10.7361 - val_acc: 0.3339\n",
      "Epoch 7/10\n",
      "6084/6084 [==============================] - 152s 25ms/step - loss: 10.7313 - acc: 0.3342 - val_loss: 10.7613 - val_acc: 0.3323\n",
      "Epoch 8/10\n",
      "6084/6084 [==============================] - 152s 25ms/step - loss: 10.7367 - acc: 0.3339 - val_loss: 10.7600 - val_acc: 0.3324\n",
      "Epoch 9/10\n",
      "6084/6084 [==============================] - 152s 25ms/step - loss: 10.7361 - acc: 0.3339 - val_loss: 10.7600 - val_acc: 0.3324\n",
      "Epoch 10/10\n",
      "6084/6084 [==============================] - 152s 25ms/step - loss: 10.7570 - acc: 0.3325 - val_loss: 10.7348 - val_acc: 0.3340\n",
      "Test score: 10.732150633973092\n",
      "Test accuracy: 0.334155161078238\n",
      "Stapelgroesse (batchSize): 32\n",
      "Anzahl der Faltungsschichten: 5Layer\n",
      "Aktivierungsfunktion: elu\n",
      "Optimierungsfunktion: Adam\n",
      "Dropout-Rate Faltungsschicht 1: 0.4610984734470713\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 64\n",
      "Dropout-Rate Faltungsschicht 2: 0.16081779868236015\n",
      "Anzahl der Faltungsschichten: 5Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 64\n",
      "Dropout-Rate Faltungsschicht 3: 0.08805334360109915\n",
      "Anzahl der Filter-Maps Faltungsschicht 4: 128\n",
      "Dropout-Rate Faltungsschicht 4: 0.6708566424053988\n",
      "Anzahl der Filter-Maps Faltungsschicht 5: 64\n",
      "Dropout-Rate Faltungsschicht 5: 0.6710317480025905\n",
      "Anzahl der Neuronen des Fully Connected Layer: 64\n",
      "Dropout-Rate Fully Connected Layer: 0.5758588201404047\n",
      "Faltungsnetz wird trainiert...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1521/1521 [==============================] - 101s 67ms/step - loss: 1.0682 - acc: 0.4294 - val_loss: 1.0246 - val_acc: 0.4584\n",
      "Epoch 2/10\n",
      "1521/1521 [==============================] - 100s 65ms/step - loss: 0.9934 - acc: 0.4879 - val_loss: 0.9054 - val_acc: 0.5486\n",
      "Epoch 3/10\n",
      "1521/1521 [==============================] - 99s 65ms/step - loss: 0.9388 - acc: 0.5286 - val_loss: 0.8705 - val_acc: 0.5648\n",
      "Epoch 4/10\n",
      "1521/1521 [==============================] - 99s 65ms/step - loss: 0.9218 - acc: 0.5430 - val_loss: 0.8608 - val_acc: 0.5792\n",
      "Epoch 5/10\n",
      "1521/1521 [==============================] - 99s 65ms/step - loss: 0.9191 - acc: 0.5406 - val_loss: 0.8794 - val_acc: 0.5704\n",
      "Epoch 6/10\n",
      "1521/1521 [==============================] - 100s 65ms/step - loss: 0.9204 - acc: 0.5420 - val_loss: 0.8384 - val_acc: 0.5832\n",
      "Epoch 7/10\n",
      "1521/1521 [==============================] - 100s 65ms/step - loss: 0.9243 - acc: 0.5415 - val_loss: 0.8425 - val_acc: 0.5799\n",
      "Epoch 8/10\n",
      "1521/1521 [==============================] - 100s 65ms/step - loss: 0.9252 - acc: 0.5410 - val_loss: 0.8397 - val_acc: 0.5955\n",
      "Epoch 9/10\n",
      "1521/1521 [==============================] - 100s 65ms/step - loss: 0.9270 - acc: 0.5388 - val_loss: 0.8675 - val_acc: 0.5935\n",
      "Epoch 10/10\n",
      "1521/1521 [==============================] - 100s 65ms/step - loss: 0.9280 - acc: 0.5405 - val_loss: 0.8334 - val_acc: 0.6059\n",
      "Test score: 0.8335627453891854\n",
      "Test accuracy: 0.6059210526315789\n",
      "Stapelgroesse (batchSize): 32\n",
      "Anzahl der Faltungsschichten: 4Layer\n",
      "Aktivierungsfunktion: elu\n",
      "Optimierungsfunktion: Adam\n",
      "Dropout-Rate Faltungsschicht 1: 0.6427027259508195\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 64\n",
      "Dropout-Rate Faltungsschicht 2: 0.4169443998086015\n",
      "Anzahl der Faltungsschichten: 4Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 64\n",
      "Dropout-Rate Faltungsschicht 3: 0.18710814475088694\n",
      "Anzahl der Filter-Maps Faltungsschicht 4: 64\n",
      "Dropout-Rate Faltungsschicht 4: 0.13477514402321533\n",
      "Anzahl der Neuronen des Fully Connected Layer: 128\n",
      "Dropout-Rate Fully Connected Layer: 0.3423376321763028\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "1521/1521 [==============================] - 98s 64ms/step - loss: 1.0483 - acc: 0.4486 - val_loss: 0.9845 - val_acc: 0.4867\n",
      "Epoch 2/10\n",
      "1521/1521 [==============================] - 96s 63ms/step - loss: 0.9907 - acc: 0.4888 - val_loss: 0.9219 - val_acc: 0.5291\n",
      "Epoch 3/10\n",
      "1521/1521 [==============================] - 96s 63ms/step - loss: 0.9425 - acc: 0.5291 - val_loss: 0.9552 - val_acc: 0.5060\n",
      "Epoch 4/10\n",
      "1521/1521 [==============================] - 96s 63ms/step - loss: 0.9164 - acc: 0.5468 - val_loss: 0.9187 - val_acc: 0.5344\n",
      "Epoch 5/10\n",
      "1521/1521 [==============================] - 96s 63ms/step - loss: 0.8953 - acc: 0.5596 - val_loss: 0.9177 - val_acc: 0.5339\n",
      "Epoch 6/10\n",
      "1521/1521 [==============================] - 96s 63ms/step - loss: 0.8884 - acc: 0.5653 - val_loss: 0.9549 - val_acc: 0.5520\n",
      "Epoch 7/10\n",
      "1521/1521 [==============================] - 96s 63ms/step - loss: 0.8777 - acc: 0.5723 - val_loss: 1.0137 - val_acc: 0.5543\n",
      "Epoch 8/10\n",
      "1521/1521 [==============================] - 96s 63ms/step - loss: 0.8701 - acc: 0.5750 - val_loss: 0.9515 - val_acc: 0.5781\n",
      "Epoch 9/10\n",
      "1521/1521 [==============================] - 96s 63ms/step - loss: 0.8652 - acc: 0.5743 - val_loss: 1.0182 - val_acc: 0.5359\n",
      "Epoch 10/10\n",
      "1521/1521 [==============================] - 96s 63ms/step - loss: 0.8591 - acc: 0.5785 - val_loss: 0.9136 - val_acc: 0.5857\n",
      "Test score: 0.9139016013396414\n",
      "Test accuracy: 0.5855263157894737\n",
      "Stapelgroesse (batchSize): 32\n",
      "Anzahl der Faltungsschichten: 5Layer\n",
      "Aktivierungsfunktion: elu\n",
      "Optimierungsfunktion: Adam\n",
      "Dropout-Rate Faltungsschicht 1: 0.15659987172287998\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 32\n",
      "Dropout-Rate Faltungsschicht 2: 0.2129190258346184\n",
      "Anzahl der Faltungsschichten: 5Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 64\n",
      "Dropout-Rate Faltungsschicht 3: 0.442275237188643\n",
      "Anzahl der Filter-Maps Faltungsschicht 4: 64\n",
      "Dropout-Rate Faltungsschicht 4: 0.13420926237033706\n",
      "Anzahl der Filter-Maps Faltungsschicht 5: 64\n",
      "Dropout-Rate Faltungsschicht 5: 0.40227014875377326\n",
      "Anzahl der Neuronen des Fully Connected Layer: 64\n",
      "Dropout-Rate Fully Connected Layer: 0.07500448226106743\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "1521/1521 [==============================] - 87s 57ms/step - loss: 1.0318 - acc: 0.4482 - val_loss: 0.9615 - val_acc: 0.5008\n",
      "Epoch 2/10\n",
      "1521/1521 [==============================] - 86s 56ms/step - loss: 0.9443 - acc: 0.5232 - val_loss: 0.8557 - val_acc: 0.5830\n",
      "Epoch 3/10\n",
      "1521/1521 [==============================] - 86s 56ms/step - loss: 0.8860 - acc: 0.5623 - val_loss: 0.8573 - val_acc: 0.5744\n",
      "Epoch 4/10\n",
      "1521/1521 [==============================] - 86s 56ms/step - loss: 0.8579 - acc: 0.5833 - val_loss: 0.8502 - val_acc: 0.5737\n",
      "Epoch 5/10\n",
      "1521/1521 [==============================] - 85s 56ms/step - loss: 0.8448 - acc: 0.5920 - val_loss: 0.8667 - val_acc: 0.5947\n",
      "Epoch 6/10\n",
      "1521/1521 [==============================] - 86s 56ms/step - loss: 0.8349 - acc: 0.5995 - val_loss: 0.8991 - val_acc: 0.5808\n",
      "Epoch 7/10\n",
      "1521/1521 [==============================] - 86s 56ms/step - loss: 0.8296 - acc: 0.6030 - val_loss: 0.8624 - val_acc: 0.6032\n",
      "Epoch 8/10\n",
      "1521/1521 [==============================] - 86s 56ms/step - loss: 0.8262 - acc: 0.6042 - val_loss: 0.8031 - val_acc: 0.6228\n",
      "Epoch 9/10\n",
      "1521/1521 [==============================] - 85s 56ms/step - loss: 0.8173 - acc: 0.6082 - val_loss: 0.7961 - val_acc: 0.6269\n",
      "Epoch 10/10\n",
      "1521/1521 [==============================] - 86s 56ms/step - loss: 0.8148 - acc: 0.6086 - val_loss: 0.7944 - val_acc: 0.6231\n",
      "Test score: 0.7944222856509058\n",
      "Test accuracy: 0.6231085526315789\n",
      "Stapelgroesse (batchSize): 32\n",
      "Anzahl der Faltungsschichten: 5Layer\n",
      "Aktivierungsfunktion: relu\n",
      "Optimierungsfunktion: Adam\n",
      "Dropout-Rate Faltungsschicht 1: 0.03784859630175669\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 64\n",
      "Dropout-Rate Faltungsschicht 2: 0.6475624675409367\n",
      "Anzahl der Faltungsschichten: 5Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 64\n",
      "Dropout-Rate Faltungsschicht 3: 0.07605493406267955\n",
      "Anzahl der Filter-Maps Faltungsschicht 4: 64\n",
      "Dropout-Rate Faltungsschicht 4: 0.36656570380794323\n",
      "Anzahl der Filter-Maps Faltungsschicht 5: 64\n",
      "Dropout-Rate Faltungsschicht 5: 0.3645909423573867\n",
      "Anzahl der Neuronen des Fully Connected Layer: 64\n",
      "Dropout-Rate Fully Connected Layer: 0.4688951121094675\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "1521/1521 [==============================] - 98s 64ms/step - loss: 1.0996 - acc: 0.3337 - val_loss: 1.0988 - val_acc: 0.3323\n",
      "Epoch 2/10\n",
      "1521/1521 [==============================] - 96s 63ms/step - loss: 1.0988 - acc: 0.3320 - val_loss: 1.0987 - val_acc: 0.3325\n",
      "Epoch 3/10\n",
      "1521/1521 [==============================] - 95s 63ms/step - loss: 1.0987 - acc: 0.3319 - val_loss: 1.0987 - val_acc: 0.3325\n",
      "Epoch 4/10\n",
      "1521/1521 [==============================] - 95s 63ms/step - loss: 1.0987 - acc: 0.3318 - val_loss: 1.0987 - val_acc: 0.3325\n",
      "Epoch 5/10\n",
      "1521/1521 [==============================] - 95s 63ms/step - loss: 1.0987 - acc: 0.3328 - val_loss: 1.0987 - val_acc: 0.3325\n",
      "Epoch 6/10\n",
      "1521/1521 [==============================] - 95s 63ms/step - loss: 1.0987 - acc: 0.3325 - val_loss: 1.0987 - val_acc: 0.3325\n",
      "Epoch 7/10\n",
      "1521/1521 [==============================] - 95s 63ms/step - loss: 1.0987 - acc: 0.3324 - val_loss: 1.0987 - val_acc: 0.3325\n",
      "Epoch 8/10\n",
      "1521/1521 [==============================] - 95s 63ms/step - loss: 1.0987 - acc: 0.3322 - val_loss: 1.0987 - val_acc: 0.3325\n",
      "Epoch 9/10\n",
      "1521/1521 [==============================] - 95s 63ms/step - loss: 1.0987 - acc: 0.3325 - val_loss: 1.0987 - val_acc: 0.3325\n",
      "Epoch 10/10\n",
      "1521/1521 [==============================] - 95s 63ms/step - loss: 1.0987 - acc: 0.3325 - val_loss: 1.0987 - val_acc: 0.3325\n",
      "Test score: 1.0986950698651765\n",
      "Test accuracy: 0.3323190789473684\n",
      "Stapelgroesse (batchSize): 32\n",
      "Anzahl der Faltungsschichten: 5Layer\n",
      "Aktivierungsfunktion: relu\n",
      "Optimierungsfunktion: RMSprop\n",
      "Dropout-Rate Faltungsschicht 1: 0.4063029446461945\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 64\n",
      "Dropout-Rate Faltungsschicht 2: 0.04103941739720152\n",
      "Anzahl der Faltungsschichten: 5Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 64\n",
      "Dropout-Rate Faltungsschicht 3: 0.5298945883736625\n",
      "Anzahl der Filter-Maps Faltungsschicht 4: 64\n",
      "Dropout-Rate Faltungsschicht 4: 0.1126017636345988\n",
      "Anzahl der Filter-Maps Faltungsschicht 5: 128\n",
      "Dropout-Rate Faltungsschicht 5: 0.1951722566868454\n",
      "Anzahl der Neuronen des Fully Connected Layer: 64\n",
      "Dropout-Rate Fully Connected Layer: 0.6301271821968158\n",
      "Faltungsnetz wird trainiert...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1521/1521 [==============================] - 98s 64ms/step - loss: 1.0762 - acc: 0.4135 - val_loss: 0.9976 - val_acc: 0.4824\n",
      "Epoch 2/10\n",
      "1521/1521 [==============================] - 96s 63ms/step - loss: 1.0234 - acc: 0.4743 - val_loss: 0.9937 - val_acc: 0.4802\n",
      "Epoch 3/10\n",
      "1521/1521 [==============================] - 96s 63ms/step - loss: 0.9810 - acc: 0.5086 - val_loss: 0.9948 - val_acc: 0.5046\n",
      "Epoch 4/10\n",
      "1521/1521 [==============================] - 96s 63ms/step - loss: 0.9649 - acc: 0.5217 - val_loss: 1.0470 - val_acc: 0.4467\n",
      "Epoch 5/10\n",
      "1521/1521 [==============================] - 96s 63ms/step - loss: 0.9588 - acc: 0.5343 - val_loss: 0.9032 - val_acc: 0.5624\n",
      "Epoch 6/10\n",
      "1521/1521 [==============================] - 96s 63ms/step - loss: 0.9391 - acc: 0.5441 - val_loss: 0.9486 - val_acc: 0.5248\n",
      "Epoch 7/10\n",
      "1521/1521 [==============================] - 96s 63ms/step - loss: 0.9338 - acc: 0.5505 - val_loss: 0.9198 - val_acc: 0.5455\n",
      "Epoch 8/10\n",
      "1521/1521 [==============================] - 96s 63ms/step - loss: 0.9378 - acc: 0.5449 - val_loss: 0.9984 - val_acc: 0.4666\n",
      "Epoch 9/10\n",
      "1521/1521 [==============================] - 96s 63ms/step - loss: 0.9445 - acc: 0.5476 - val_loss: 1.1074 - val_acc: 0.3908\n",
      "Epoch 10/10\n",
      "1521/1521 [==============================] - 96s 63ms/step - loss: 0.9398 - acc: 0.5450 - val_loss: 0.9674 - val_acc: 0.5526\n",
      "Test score: 0.9676417040197473\n",
      "Test accuracy: 0.5525493421052632\n",
      "Stapelgroesse (batchSize): 16\n",
      "Anzahl der Faltungsschichten: 4Layer\n",
      "Aktivierungsfunktion: elu\n",
      "Optimierungsfunktion: RMSprop\n",
      "Dropout-Rate Faltungsschicht 1: 0.6117517831509787\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 64\n",
      "Dropout-Rate Faltungsschicht 2: 0.07874046469699175\n",
      "Anzahl der Faltungsschichten: 4Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 128\n",
      "Dropout-Rate Faltungsschicht 3: 0.38879746602236\n",
      "Anzahl der Filter-Maps Faltungsschicht 4: 64\n",
      "Dropout-Rate Faltungsschicht 4: 0.10165264323193196\n",
      "Anzahl der Neuronen des Fully Connected Layer: 64\n",
      "Dropout-Rate Fully Connected Layer: 0.30294519188552504\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "3042/3042 [==============================] - 121s 40ms/step - loss: 1.1448 - acc: 0.4293 - val_loss: 1.0504 - val_acc: 0.4509\n",
      "Epoch 2/10\n",
      "3042/3042 [==============================] - 119s 39ms/step - loss: 0.9728 - acc: 0.5048 - val_loss: 0.9170 - val_acc: 0.5436\n",
      "Epoch 3/10\n",
      "3042/3042 [==============================] - 119s 39ms/step - loss: 0.9453 - acc: 0.5272 - val_loss: 1.1011 - val_acc: 0.4846\n",
      "Epoch 4/10\n",
      "3042/3042 [==============================] - 119s 39ms/step - loss: 0.9416 - acc: 0.5306 - val_loss: 0.9562 - val_acc: 0.5065\n",
      "Epoch 5/10\n",
      "3042/3042 [==============================] - 119s 39ms/step - loss: 0.9381 - acc: 0.5336 - val_loss: 0.9248 - val_acc: 0.5368\n",
      "Epoch 6/10\n",
      "3042/3042 [==============================] - 119s 39ms/step - loss: 0.9408 - acc: 0.5354 - val_loss: 1.1272 - val_acc: 0.5056\n",
      "Epoch 7/10\n",
      "3042/3042 [==============================] - 119s 39ms/step - loss: 0.9475 - acc: 0.5304 - val_loss: 0.8805 - val_acc: 0.5861\n",
      "Epoch 8/10\n",
      "3042/3042 [==============================] - 119s 39ms/step - loss: 0.9557 - acc: 0.5196 - val_loss: 0.9508 - val_acc: 0.4935\n",
      "Epoch 9/10\n",
      "3042/3042 [==============================] - 119s 39ms/step - loss: 0.9521 - acc: 0.5214 - val_loss: 0.9309 - val_acc: 0.5573\n",
      "Epoch 10/10\n",
      "3042/3042 [==============================] - 119s 39ms/step - loss: 0.9602 - acc: 0.5146 - val_loss: 1.1408 - val_acc: 0.4751\n",
      "Test score: 1.140751149309309\n",
      "Test accuracy: 0.4752467105263158\n",
      "Stapelgroesse (batchSize): 32\n",
      "Anzahl der Faltungsschichten: 4Layer\n",
      "Aktivierungsfunktion: relu\n",
      "Optimierungsfunktion: RMSprop\n",
      "Dropout-Rate Faltungsschicht 1: 0.2636404947056047\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 64\n",
      "Dropout-Rate Faltungsschicht 2: 0.6836210943752541\n",
      "Anzahl der Faltungsschichten: 4Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 128\n",
      "Dropout-Rate Faltungsschicht 3: 0.22966076346119307\n",
      "Anzahl der Filter-Maps Faltungsschicht 4: 128\n",
      "Dropout-Rate Faltungsschicht 4: 0.6582916558341014\n",
      "Anzahl der Neuronen des Fully Connected Layer: 128\n",
      "Dropout-Rate Fully Connected Layer: 0.3055128722061833\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "1521/1521 [==============================] - 106s 70ms/step - loss: 1.0632 - acc: 0.4315 - val_loss: 1.0696 - val_acc: 0.3931\n",
      "Epoch 2/10\n",
      "1521/1521 [==============================] - 104s 68ms/step - loss: 1.0264 - acc: 0.4675 - val_loss: 1.0037 - val_acc: 0.4858\n",
      "Epoch 3/10\n",
      "1521/1521 [==============================] - 104s 68ms/step - loss: 0.9995 - acc: 0.4917 - val_loss: 1.0496 - val_acc: 0.4595\n",
      "Epoch 4/10\n",
      "1521/1521 [==============================] - 104s 68ms/step - loss: 0.9841 - acc: 0.5047 - val_loss: 1.0282 - val_acc: 0.4797\n",
      "Epoch 5/10\n",
      "1521/1521 [==============================] - 104s 68ms/step - loss: 0.9694 - acc: 0.5191 - val_loss: 0.9679 - val_acc: 0.5188\n",
      "Epoch 6/10\n",
      "1521/1521 [==============================] - 104s 68ms/step - loss: 0.9621 - acc: 0.5287 - val_loss: 0.9850 - val_acc: 0.5000\n",
      "Epoch 7/10\n",
      "1521/1521 [==============================] - 104s 68ms/step - loss: 0.9484 - acc: 0.5333 - val_loss: 0.9769 - val_acc: 0.4925\n",
      "Epoch 8/10\n",
      "1521/1521 [==============================] - 104s 68ms/step - loss: 0.9500 - acc: 0.5362 - val_loss: 1.0261 - val_acc: 0.4497\n",
      "Epoch 9/10\n",
      "1521/1521 [==============================] - 104s 68ms/step - loss: 0.9523 - acc: 0.5366 - val_loss: 1.0203 - val_acc: 0.4538\n",
      "Epoch 10/10\n",
      "1521/1521 [==============================] - 104s 68ms/step - loss: 0.9466 - acc: 0.5406 - val_loss: 0.9873 - val_acc: 0.5175\n",
      "Test score: 0.9873156003261867\n",
      "Test accuracy: 0.5174342105263158\n",
      "Stapelgroesse (batchSize): 8\n",
      "Anzahl der Faltungsschichten: 4Layer\n",
      "Aktivierungsfunktion: elu\n",
      "Optimierungsfunktion: RMSprop\n",
      "Dropout-Rate Faltungsschicht 1: 0.04094223486538234\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 32\n",
      "Dropout-Rate Faltungsschicht 2: 0.5501173450933593\n",
      "Anzahl der Faltungsschichten: 4Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 128\n",
      "Dropout-Rate Faltungsschicht 3: 0.24209449821680215\n",
      "Anzahl der Filter-Maps Faltungsschicht 4: 128\n",
      "Dropout-Rate Faltungsschicht 4: 0.47065615224871427\n",
      "Anzahl der Neuronen des Fully Connected Layer: 128\n",
      "Dropout-Rate Fully Connected Layer: 0.3212478490516954\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "6084/6084 [==============================] - 144s 24ms/step - loss: 10.7476 - acc: 0.3329 - val_loss: 10.7414 - val_acc: 0.3336\n",
      "Epoch 2/10\n",
      "6084/6084 [==============================] - 141s 23ms/step - loss: 10.7501 - acc: 0.3330 - val_loss: 10.7401 - val_acc: 0.3337\n",
      "Epoch 3/10\n",
      "6084/6084 [==============================] - 142s 23ms/step - loss: 10.7508 - acc: 0.3330 - val_loss: 10.7414 - val_acc: 0.3336\n",
      "Epoch 4/10\n",
      "6084/6084 [==============================] - 141s 23ms/step - loss: 10.7504 - acc: 0.3330 - val_loss: 10.7414 - val_acc: 0.3336\n",
      "Epoch 5/10\n",
      "6084/6084 [==============================] - 141s 23ms/step - loss: 10.7733 - acc: 0.3315 - val_loss: 10.7308 - val_acc: 0.3342\n",
      "Epoch 6/10\n",
      "6084/6084 [==============================] - 141s 23ms/step - loss: 10.7721 - acc: 0.3316 - val_loss: 10.7441 - val_acc: 0.3334\n",
      "Epoch 7/10\n",
      "6084/6084 [==============================] - 141s 23ms/step - loss: 10.7508 - acc: 0.3330 - val_loss: 10.7401 - val_acc: 0.3337\n",
      "Epoch 8/10\n",
      "6084/6084 [==============================] - 141s 23ms/step - loss: 10.7498 - acc: 0.3331 - val_loss: 10.7401 - val_acc: 0.3337\n",
      "Epoch 9/10\n",
      "6084/6084 [==============================] - 141s 23ms/step - loss: 10.7504 - acc: 0.3330 - val_loss: 10.7414 - val_acc: 0.3336\n",
      "Epoch 10/10\n",
      "6084/6084 [==============================] - 141s 23ms/step - loss: 10.7504 - acc: 0.3330 - val_loss: 10.7401 - val_acc: 0.3337\n",
      "Test score: 10.741423042568229\n",
      "Test accuracy: 0.33357988165680474\n",
      "Stapelgroesse (batchSize): 16\n",
      "Anzahl der Faltungsschichten: 5Layer\n",
      "Aktivierungsfunktion: relu\n",
      "Optimierungsfunktion: RMSprop\n",
      "Dropout-Rate Faltungsschicht 1: 0.1611811624500729\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 64\n",
      "Dropout-Rate Faltungsschicht 2: 0.26908596491655257\n",
      "Anzahl der Faltungsschichten: 5Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 128\n",
      "Dropout-Rate Faltungsschicht 3: 0.034636694400194755\n",
      "Anzahl der Filter-Maps Faltungsschicht 4: 64\n",
      "Dropout-Rate Faltungsschicht 4: 0.6907288802727536\n",
      "Anzahl der Filter-Maps Faltungsschicht 5: 64\n",
      "Dropout-Rate Faltungsschicht 5: 0.13406699491026297\n",
      "Anzahl der Neuronen des Fully Connected Layer: 64\n",
      "Dropout-Rate Fully Connected Layer: 0.10524234787979274\n",
      "Faltungsnetz wird trainiert...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3042/3042 [==============================] - 123s 41ms/step - loss: 1.0499 - acc: 0.4482 - val_loss: 0.9863 - val_acc: 0.4961\n",
      "Epoch 2/10\n",
      "3042/3042 [==============================] - 121s 40ms/step - loss: 0.9862 - acc: 0.5074 - val_loss: 0.9347 - val_acc: 0.5422\n",
      "Epoch 3/10\n",
      "3042/3042 [==============================] - 121s 40ms/step - loss: 0.9663 - acc: 0.5188 - val_loss: 0.9681 - val_acc: 0.5244\n",
      "Epoch 4/10\n",
      "3042/3042 [==============================] - 121s 40ms/step - loss: 0.9625 - acc: 0.5266 - val_loss: 0.9434 - val_acc: 0.5613\n",
      "Epoch 5/10\n",
      "3042/3042 [==============================] - 121s 40ms/step - loss: 0.9599 - acc: 0.5292 - val_loss: 0.9766 - val_acc: 0.5221\n",
      "Epoch 6/10\n",
      "3042/3042 [==============================] - 121s 40ms/step - loss: 0.9579 - acc: 0.5310 - val_loss: 1.0370 - val_acc: 0.5120\n",
      "Epoch 7/10\n",
      "3042/3042 [==============================] - 121s 40ms/step - loss: 0.9642 - acc: 0.5290 - val_loss: 1.0147 - val_acc: 0.5318\n",
      "Epoch 8/10\n",
      "3042/3042 [==============================] - 121s 40ms/step - loss: 0.9740 - acc: 0.5277 - val_loss: 1.0407 - val_acc: 0.4742\n",
      "Epoch 9/10\n",
      "3042/3042 [==============================] - 121s 40ms/step - loss: 3.6360 - acc: 0.4692 - val_loss: 1.0534 - val_acc: 0.4190\n",
      "Epoch 10/10\n",
      "3042/3042 [==============================] - 121s 40ms/step - loss: 0.9769 - acc: 0.5183 - val_loss: 0.9985 - val_acc: 0.4961\n",
      "Test score: 0.9984505639264458\n",
      "Test accuracy: 0.49613486842105264\n",
      "Stapelgroesse (batchSize): 16\n",
      "Anzahl der Faltungsschichten: 3Layer\n",
      "Aktivierungsfunktion: elu\n",
      "Optimierungsfunktion: Adam\n",
      "Dropout-Rate Faltungsschicht 1: 0.5430043368754298\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 64\n",
      "Dropout-Rate Faltungsschicht 2: 0.06715690813728312\n",
      "Anzahl der Faltungsschichten: 3Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 128\n",
      "Dropout-Rate Faltungsschicht 3: 0.1964720930822093\n",
      "Anzahl der Neuronen des Fully Connected Layer: 64\n",
      "Dropout-Rate Fully Connected Layer: 0.5045534914382528\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "3042/3042 [==============================] - 125s 41ms/step - loss: 1.2157 - acc: 0.3352 - val_loss: 1.0979 - val_acc: 0.3405\n",
      "Epoch 2/10\n",
      "3042/3042 [==============================] - 123s 41ms/step - loss: 1.1069 - acc: 0.3312 - val_loss: 1.0989 - val_acc: 0.3336\n",
      "Epoch 3/10\n",
      "3042/3042 [==============================] - 123s 41ms/step - loss: 1.1069 - acc: 0.3298 - val_loss: 1.0993 - val_acc: 0.3335\n",
      "Epoch 4/10\n",
      "3042/3042 [==============================] - 123s 41ms/step - loss: 1.1145 - acc: 0.3319 - val_loss: 1.0995 - val_acc: 0.3335\n",
      "Epoch 5/10\n",
      "3042/3042 [==============================] - 123s 40ms/step - loss: 1.1070 - acc: 0.3306 - val_loss: 1.1003 - val_acc: 0.3335\n",
      "Epoch 6/10\n",
      "3042/3042 [==============================] - 123s 41ms/step - loss: 1.1069 - acc: 0.3313 - val_loss: 1.1005 - val_acc: 0.3336\n",
      "Epoch 7/10\n",
      "3042/3042 [==============================] - 123s 41ms/step - loss: 1.1067 - acc: 0.3321 - val_loss: 1.1005 - val_acc: 0.3334\n",
      "Epoch 8/10\n",
      "3042/3042 [==============================] - 123s 41ms/step - loss: 1.1067 - acc: 0.3319 - val_loss: 1.1014 - val_acc: 0.3337\n",
      "Epoch 9/10\n",
      "3042/3042 [==============================] - 123s 40ms/step - loss: 1.1070 - acc: 0.3314 - val_loss: 1.1012 - val_acc: 0.3337\n",
      "Epoch 10/10\n",
      "3042/3042 [==============================] - 123s 41ms/step - loss: 1.1072 - acc: 0.3295 - val_loss: 1.1019 - val_acc: 0.3337\n",
      "Test score: 1.101934928172513\n",
      "Test accuracy: 0.3335526315789474\n",
      "Stapelgroesse (batchSize): 32\n",
      "Anzahl der Faltungsschichten: 4Layer\n",
      "Aktivierungsfunktion: relu\n",
      "Optimierungsfunktion: Adam\n",
      "Dropout-Rate Faltungsschicht 1: 0.4161322928479168\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 64\n",
      "Dropout-Rate Faltungsschicht 2: 0.07400274876473123\n",
      "Anzahl der Faltungsschichten: 4Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 64\n",
      "Dropout-Rate Faltungsschicht 3: 0.5207759314862476\n",
      "Anzahl der Filter-Maps Faltungsschicht 4: 128\n",
      "Dropout-Rate Faltungsschicht 4: 0.2108262930092021\n",
      "Anzahl der Neuronen des Fully Connected Layer: 128\n",
      "Dropout-Rate Fully Connected Layer: 0.3102813339921929\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "1521/1521 [==============================] - 100s 66ms/step - loss: 1.0506 - acc: 0.4258 - val_loss: 1.0350 - val_acc: 0.4490\n",
      "Epoch 2/10\n",
      "1521/1521 [==============================] - 97s 64ms/step - loss: 1.0082 - acc: 0.4688 - val_loss: 1.0301 - val_acc: 0.4502\n",
      "Epoch 3/10\n",
      "1521/1521 [==============================] - 97s 64ms/step - loss: 0.9921 - acc: 0.4856 - val_loss: 1.0138 - val_acc: 0.4565\n",
      "Epoch 4/10\n",
      "1521/1521 [==============================] - 97s 64ms/step - loss: 0.9786 - acc: 0.4961 - val_loss: 0.9828 - val_acc: 0.4942\n",
      "Epoch 5/10\n",
      "1521/1521 [==============================] - 97s 64ms/step - loss: 0.9568 - acc: 0.5144 - val_loss: 1.0000 - val_acc: 0.4883\n",
      "Epoch 6/10\n",
      "1521/1521 [==============================] - 97s 64ms/step - loss: 0.9395 - acc: 0.5271 - val_loss: 1.0226 - val_acc: 0.4627\n",
      "Epoch 7/10\n",
      "1521/1521 [==============================] - 97s 64ms/step - loss: 0.9209 - acc: 0.5420 - val_loss: 1.0006 - val_acc: 0.4868\n",
      "Epoch 8/10\n",
      "1521/1521 [==============================] - 97s 64ms/step - loss: 0.9050 - acc: 0.5554 - val_loss: 0.9671 - val_acc: 0.5143\n",
      "Epoch 9/10\n",
      "1521/1521 [==============================] - 97s 64ms/step - loss: 0.8942 - acc: 0.5592 - val_loss: 1.0089 - val_acc: 0.4814\n",
      "Epoch 10/10\n",
      "1521/1521 [==============================] - 97s 64ms/step - loss: 0.8864 - acc: 0.5665 - val_loss: 1.0394 - val_acc: 0.4585\n",
      "Test score: 1.0398296306007786\n",
      "Test accuracy: 0.4586348684210526\n",
      "Stapelgroesse (batchSize): 16\n",
      "Anzahl der Faltungsschichten: 3Layer\n",
      "Aktivierungsfunktion: relu\n",
      "Optimierungsfunktion: RMSprop\n",
      "Dropout-Rate Faltungsschicht 1: 0.33451399159802375\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 32\n",
      "Dropout-Rate Faltungsschicht 2: 0.25767222312424426\n",
      "Anzahl der Faltungsschichten: 3Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 64\n",
      "Dropout-Rate Faltungsschicht 3: 0.36484964265506686\n",
      "Anzahl der Neuronen des Fully Connected Layer: 64\n",
      "Dropout-Rate Fully Connected Layer: 0.4207538587369814\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "3042/3042 [==============================] - 98s 32ms/step - loss: 1.0518 - acc: 0.4436 - val_loss: 1.0106 - val_acc: 0.4676\n",
      "Epoch 2/10\n",
      "3042/3042 [==============================] - 95s 31ms/step - loss: 0.9885 - acc: 0.5013 - val_loss: 0.9928 - val_acc: 0.4707\n",
      "Epoch 3/10\n",
      "3042/3042 [==============================] - 95s 31ms/step - loss: 0.9662 - acc: 0.5167 - val_loss: 0.9830 - val_acc: 0.4844\n",
      "Epoch 4/10\n",
      "3042/3042 [==============================] - 95s 31ms/step - loss: 0.9627 - acc: 0.5244 - val_loss: 0.9866 - val_acc: 0.4787\n",
      "Epoch 5/10\n",
      "3042/3042 [==============================] - 95s 31ms/step - loss: 0.9557 - acc: 0.5265 - val_loss: 1.0501 - val_acc: 0.3993\n",
      "Epoch 6/10\n",
      "3042/3042 [==============================] - 95s 31ms/step - loss: 0.9494 - acc: 0.5306 - val_loss: 0.9687 - val_acc: 0.5005\n",
      "Epoch 7/10\n",
      "3042/3042 [==============================] - 95s 31ms/step - loss: 0.9501 - acc: 0.5280 - val_loss: 0.9489 - val_acc: 0.5220\n",
      "Epoch 8/10\n",
      "3042/3042 [==============================] - 95s 31ms/step - loss: 0.9498 - acc: 0.5268 - val_loss: 0.9717 - val_acc: 0.4835\n",
      "Epoch 9/10\n",
      "3042/3042 [==============================] - 95s 31ms/step - loss: 0.9510 - acc: 0.5296 - val_loss: 0.9978 - val_acc: 0.4459\n",
      "Epoch 10/10\n",
      "3042/3042 [==============================] - 95s 31ms/step - loss: 0.9554 - acc: 0.5306 - val_loss: 0.9912 - val_acc: 0.4460\n",
      "Test score: 0.9913722431973407\n",
      "Test accuracy: 0.44564144736842104\n",
      "Stapelgroesse (batchSize): 32\n",
      "Anzahl der Faltungsschichten: 3Layer\n",
      "Aktivierungsfunktion: relu\n",
      "Optimierungsfunktion: RMSprop\n",
      "Dropout-Rate Faltungsschicht 1: 0.5898991773841765\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 32\n",
      "Dropout-Rate Faltungsschicht 2: 0.15111715163692865\n",
      "Anzahl der Faltungsschichten: 3Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 128\n",
      "Dropout-Rate Faltungsschicht 3: 0.0037666668932191723\n",
      "Anzahl der Neuronen des Fully Connected Layer: 64\n",
      "Dropout-Rate Fully Connected Layer: 0.23702840408628864\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "1521/1521 [==============================] - 173s 114ms/step - loss: 1.0611 - acc: 0.4398 - val_loss: 0.9999 - val_acc: 0.4726\n",
      "Epoch 2/10\n",
      "1521/1521 [==============================] - 123s 81ms/step - loss: 1.0067 - acc: 0.4783 - val_loss: 0.9713 - val_acc: 0.5002\n",
      "Epoch 3/10\n",
      "1521/1521 [==============================] - 87s 57ms/step - loss: 0.9818 - acc: 0.5005 - val_loss: 0.9715 - val_acc: 0.4992\n",
      "Epoch 4/10\n",
      "1521/1521 [==============================] - 87s 57ms/step - loss: 0.9593 - acc: 0.5211 - val_loss: 0.9602 - val_acc: 0.5231\n",
      "Epoch 5/10\n",
      "1521/1521 [==============================] - 87s 57ms/step - loss: 0.9392 - acc: 0.5337 - val_loss: 0.9319 - val_acc: 0.5543\n",
      "Epoch 6/10\n",
      "1521/1521 [==============================] - 88s 58ms/step - loss: 0.9311 - acc: 0.5416 - val_loss: 0.9338 - val_acc: 0.5478\n",
      "Epoch 7/10\n",
      "1521/1521 [==============================] - 88s 58ms/step - loss: 0.9168 - acc: 0.5499 - val_loss: 0.9647 - val_acc: 0.5096\n",
      "Epoch 8/10\n",
      "1521/1521 [==============================] - 87s 57ms/step - loss: 0.9126 - acc: 0.5559 - val_loss: 0.9750 - val_acc: 0.4991\n",
      "Epoch 9/10\n",
      "1521/1521 [==============================] - 87s 57ms/step - loss: 0.9077 - acc: 0.5563 - val_loss: 0.9300 - val_acc: 0.5661\n",
      "Epoch 10/10\n",
      "1521/1521 [==============================] - 87s 57ms/step - loss: 0.9068 - acc: 0.5605 - val_loss: 0.9012 - val_acc: 0.5651\n",
      "Test score: 0.9011892619885896\n",
      "Test accuracy: 0.5652960526315789\n",
      "Stapelgroesse (batchSize): 16\n",
      "Anzahl der Faltungsschichten: 4Layer\n",
      "Aktivierungsfunktion: elu\n",
      "Optimierungsfunktion: Adam\n",
      "Dropout-Rate Faltungsschicht 1: 0.6258522155693698\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 64\n",
      "Dropout-Rate Faltungsschicht 2: 0.12809110971442503\n",
      "Anzahl der Faltungsschichten: 4Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 128\n",
      "Dropout-Rate Faltungsschicht 3: 0.3483976272358616\n",
      "Anzahl der Filter-Maps Faltungsschicht 4: 64\n",
      "Dropout-Rate Faltungsschicht 4: 0.46689731505582\n",
      "Anzahl der Neuronen des Fully Connected Layer: 128\n",
      "Dropout-Rate Fully Connected Layer: 0.6432023455140354\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "3042/3042 [==============================] - 124s 41ms/step - loss: 1.0592 - acc: 0.4376 - val_loss: 1.0152 - val_acc: 0.4484\n",
      "Epoch 2/10\n",
      "3042/3042 [==============================] - 121s 40ms/step - loss: 1.0323 - acc: 0.4532 - val_loss: 1.1137 - val_acc: 0.4302\n",
      "Epoch 3/10\n",
      "3042/3042 [==============================] - 121s 40ms/step - loss: 1.0286 - acc: 0.4595 - val_loss: 1.0162 - val_acc: 0.4120\n",
      "Epoch 4/10\n",
      "3042/3042 [==============================] - 121s 40ms/step - loss: 1.0242 - acc: 0.4581 - val_loss: 0.9987 - val_acc: 0.3833\n",
      "Epoch 5/10\n",
      "3042/3042 [==============================] - 121s 40ms/step - loss: 1.0234 - acc: 0.4573 - val_loss: 0.9683 - val_acc: 0.5005\n",
      "Epoch 6/10\n",
      "3042/3042 [==============================] - 121s 40ms/step - loss: 1.0336 - acc: 0.4489 - val_loss: 1.0739 - val_acc: 0.3651\n",
      "Epoch 7/10\n",
      "3042/3042 [==============================] - 121s 40ms/step - loss: 1.0364 - acc: 0.4469 - val_loss: 0.9936 - val_acc: 0.4963\n",
      "Epoch 8/10\n",
      "3042/3042 [==============================] - 121s 40ms/step - loss: 1.0395 - acc: 0.4433 - val_loss: 1.0334 - val_acc: 0.4401\n",
      "Epoch 9/10\n",
      "3042/3042 [==============================] - 121s 40ms/step - loss: 1.0478 - acc: 0.4317 - val_loss: 1.0651 - val_acc: 0.3818\n",
      "Epoch 10/10\n",
      "3042/3042 [==============================] - 121s 40ms/step - loss: 1.0558 - acc: 0.4248 - val_loss: 1.0140 - val_acc: 0.4314\n",
      "Test score: 1.0140524523822885\n",
      "Test accuracy: 0.43116776315789473\n",
      "Stapelgroesse (batchSize): 16\n",
      "Anzahl der Faltungsschichten: 5Layer\n",
      "Aktivierungsfunktion: elu\n",
      "Optimierungsfunktion: RMSprop\n",
      "Dropout-Rate Faltungsschicht 1: 0.6719647718091494\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 32\n",
      "Dropout-Rate Faltungsschicht 2: 0.4334039533095252\n",
      "Anzahl der Faltungsschichten: 5Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 64\n",
      "Dropout-Rate Faltungsschicht 3: 0.3852434388296144\n",
      "Anzahl der Filter-Maps Faltungsschicht 4: 128\n",
      "Dropout-Rate Faltungsschicht 4: 0.05792646430464581\n",
      "Anzahl der Filter-Maps Faltungsschicht 5: 128\n",
      "Dropout-Rate Faltungsschicht 5: 0.6201374941396659\n",
      "Anzahl der Neuronen des Fully Connected Layer: 128\n",
      "Dropout-Rate Fully Connected Layer: 0.057200250266939485\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "3042/3042 [==============================] - 108s 36ms/step - loss: 1.1545 - acc: 0.4220 - val_loss: 1.1245 - val_acc: 0.4169\n",
      "Epoch 2/10\n",
      "3042/3042 [==============================] - 106s 35ms/step - loss: 1.0064 - acc: 0.4852 - val_loss: 1.0680 - val_acc: 0.4448\n",
      "Epoch 3/10\n",
      "3042/3042 [==============================] - 106s 35ms/step - loss: 0.9908 - acc: 0.4959 - val_loss: 1.2316 - val_acc: 0.4600\n",
      "Epoch 4/10\n",
      "3042/3042 [==============================] - 106s 35ms/step - loss: 0.9813 - acc: 0.5031 - val_loss: 1.0822 - val_acc: 0.4164\n",
      "Epoch 5/10\n",
      "3042/3042 [==============================] - 106s 35ms/step - loss: 0.9770 - acc: 0.5064 - val_loss: 0.9839 - val_acc: 0.4306\n",
      "Epoch 6/10\n",
      "3042/3042 [==============================] - 106s 35ms/step - loss: 0.9787 - acc: 0.5086 - val_loss: 1.4179 - val_acc: 0.4737\n",
      "Epoch 7/10\n",
      "3042/3042 [==============================] - 106s 35ms/step - loss: 0.9793 - acc: 0.5038 - val_loss: 1.0846 - val_acc: 0.5188\n",
      "Epoch 8/10\n",
      "3042/3042 [==============================] - 106s 35ms/step - loss: 0.9777 - acc: 0.5034 - val_loss: 1.2037 - val_acc: 0.4999\n",
      "Epoch 9/10\n",
      "3042/3042 [==============================] - 106s 35ms/step - loss: 0.9786 - acc: 0.5042 - val_loss: 0.9833 - val_acc: 0.5549\n",
      "Epoch 10/10\n",
      "3042/3042 [==============================] - 106s 35ms/step - loss: 0.9836 - acc: 0.5045 - val_loss: 0.9231 - val_acc: 0.5286\n",
      "Test score: 0.9235436964191889\n",
      "Test accuracy: 0.5282894736842105\n",
      "Stapelgroesse (batchSize): 16\n",
      "Anzahl der Faltungsschichten: 3Layer\n",
      "Aktivierungsfunktion: elu\n",
      "Optimierungsfunktion: RMSprop\n",
      "Dropout-Rate Faltungsschicht 1: 0.3389652104130198\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 32\n",
      "Dropout-Rate Faltungsschicht 2: 0.1104098405551635\n",
      "Anzahl der Faltungsschichten: 3Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 128\n",
      "Dropout-Rate Faltungsschicht 3: 0.5296528997881041\n",
      "Anzahl der Neuronen des Fully Connected Layer: 64\n",
      "Dropout-Rate Fully Connected Layer: 0.13987297505909094\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "3042/3042 [==============================] - 110s 36ms/step - loss: 10.7411 - acc: 0.3333 - val_loss: 10.7326 - val_acc: 0.3341\n",
      "Epoch 2/10\n",
      "3042/3042 [==============================] - 107s 35ms/step - loss: 10.7497 - acc: 0.3331 - val_loss: 10.7339 - val_acc: 0.3340\n",
      "Epoch 3/10\n",
      "3042/3042 [==============================] - 107s 35ms/step - loss: 10.7510 - acc: 0.3330 - val_loss: 10.7339 - val_acc: 0.3340\n",
      "Epoch 4/10\n",
      "3042/3042 [==============================] - 107s 35ms/step - loss: 10.7500 - acc: 0.3330 - val_loss: 10.7339 - val_acc: 0.3340\n",
      "Epoch 5/10\n",
      "3042/3042 [==============================] - 107s 35ms/step - loss: 10.7507 - acc: 0.3330 - val_loss: 10.7326 - val_acc: 0.3341\n",
      "Epoch 6/10\n",
      "3042/3042 [==============================] - 107s 35ms/step - loss: 10.7497 - acc: 0.3331 - val_loss: 10.7326 - val_acc: 0.3341\n",
      "Epoch 7/10\n",
      "3042/3042 [==============================] - 107s 35ms/step - loss: 10.7507 - acc: 0.3330 - val_loss: 10.7299 - val_acc: 0.3343\n",
      "Epoch 8/10\n",
      "3042/3042 [==============================] - 107s 35ms/step - loss: 10.7514 - acc: 0.3330 - val_loss: 10.7299 - val_acc: 0.3343\n",
      "Epoch 9/10\n",
      "3042/3042 [==============================] - 107s 35ms/step - loss: 10.7497 - acc: 0.3331 - val_loss: 10.7379 - val_acc: 0.3338\n",
      "Epoch 10/10\n",
      "3042/3042 [==============================] - 107s 35ms/step - loss: 10.7507 - acc: 0.3330 - val_loss: 10.7366 - val_acc: 0.3339\n",
      "Test score: 10.732583765607131\n",
      "Test accuracy: 0.3341282894736842\n",
      "Stapelgroesse (batchSize): 32\n",
      "Anzahl der Faltungsschichten: 4Layer\n",
      "Aktivierungsfunktion: elu\n",
      "Optimierungsfunktion: RMSprop\n",
      "Dropout-Rate Faltungsschicht 1: 0.19303830922066545\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 64\n",
      "Dropout-Rate Faltungsschicht 2: 0.0840127538592151\n",
      "Anzahl der Faltungsschichten: 4Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 128\n",
      "Dropout-Rate Faltungsschicht 3: 0.4642820273620739\n",
      "Anzahl der Filter-Maps Faltungsschicht 4: 64\n",
      "Dropout-Rate Faltungsschicht 4: 0.09875567645458869\n",
      "Anzahl der Neuronen des Fully Connected Layer: 128\n",
      "Dropout-Rate Fully Connected Layer: 0.6570201177778964\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1521/1521 [==============================] - 107s 70ms/step - loss: 1.1086 - acc: 0.4226 - val_loss: 0.9242 - val_acc: 0.5343\n",
      "Epoch 2/10\n",
      "1521/1521 [==============================] - 104s 68ms/step - loss: 0.9278 - acc: 0.5384 - val_loss: 0.8901 - val_acc: 0.5444\n",
      "Epoch 3/10\n",
      "1521/1521 [==============================] - 104s 68ms/step - loss: 0.9014 - acc: 0.5574 - val_loss: 0.9102 - val_acc: 0.4960\n",
      "Epoch 4/10\n",
      "1521/1521 [==============================] - 104s 68ms/step - loss: 0.8913 - acc: 0.5646 - val_loss: 0.8546 - val_acc: 0.5732\n",
      "Epoch 5/10\n",
      "1521/1521 [==============================] - 104s 68ms/step - loss: 0.8850 - acc: 0.5691 - val_loss: 0.8387 - val_acc: 0.5987\n",
      "Epoch 6/10\n",
      "1521/1521 [==============================] - 104s 68ms/step - loss: 0.8809 - acc: 0.5731 - val_loss: 0.9636 - val_acc: 0.5258\n",
      "Epoch 7/10\n",
      "1521/1521 [==============================] - 104s 68ms/step - loss: 0.8799 - acc: 0.5731 - val_loss: 0.8485 - val_acc: 0.6052\n",
      "Epoch 8/10\n",
      "1521/1521 [==============================] - 104s 68ms/step - loss: 0.8818 - acc: 0.5750 - val_loss: 0.8655 - val_acc: 0.5843\n",
      "Epoch 9/10\n",
      "1521/1521 [==============================] - 104s 68ms/step - loss: 0.8820 - acc: 0.5752 - val_loss: 0.8415 - val_acc: 0.6050\n",
      "Epoch 10/10\n",
      "1521/1521 [==============================] - 104s 68ms/step - loss: 0.8821 - acc: 0.5746 - val_loss: 0.8229 - val_acc: 0.6147\n",
      "Test score: 0.8230994994703092\n",
      "Test accuracy: 0.6147203947368421\n",
      "Stapelgroesse (batchSize): 32\n",
      "Anzahl der Faltungsschichten: 5Layer\n",
      "Aktivierungsfunktion: elu\n",
      "Optimierungsfunktion: RMSprop\n",
      "Dropout-Rate Faltungsschicht 1: 0.0790967978439296\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 32\n",
      "Dropout-Rate Faltungsschicht 2: 0.5028747003251189\n",
      "Anzahl der Faltungsschichten: 5Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 128\n",
      "Dropout-Rate Faltungsschicht 3: 0.317826785536165\n",
      "Anzahl der Filter-Maps Faltungsschicht 4: 128\n",
      "Dropout-Rate Faltungsschicht 4: 0.007773182337481476\n",
      "Anzahl der Filter-Maps Faltungsschicht 5: 64\n",
      "Dropout-Rate Faltungsschicht 5: 0.6840469439004162\n",
      "Anzahl der Neuronen des Fully Connected Layer: 64\n",
      "Dropout-Rate Fully Connected Layer: 0.5558192810676943\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "1521/1521 [==============================] - 96s 63ms/step - loss: 1.0891 - acc: 0.4054 - val_loss: 0.9994 - val_acc: 0.4720\n",
      "Epoch 2/10\n",
      "1521/1521 [==============================] - 94s 61ms/step - loss: 0.9817 - acc: 0.5001 - val_loss: 0.9274 - val_acc: 0.5219\n",
      "Epoch 3/10\n",
      "1521/1521 [==============================] - 93s 61ms/step - loss: 0.9201 - acc: 0.5458 - val_loss: 0.8562 - val_acc: 0.5982\n",
      "Epoch 4/10\n",
      "1521/1521 [==============================] - 93s 61ms/step - loss: 0.9039 - acc: 0.5592 - val_loss: 0.8308 - val_acc: 0.6097\n",
      "Epoch 5/10\n",
      "1521/1521 [==============================] - 93s 61ms/step - loss: 0.8991 - acc: 0.5631 - val_loss: 0.8329 - val_acc: 0.6023\n",
      "Epoch 6/10\n",
      "1521/1521 [==============================] - 93s 61ms/step - loss: 0.8970 - acc: 0.5623 - val_loss: 0.8690 - val_acc: 0.5592\n",
      "Epoch 7/10\n",
      "1521/1521 [==============================] - 93s 61ms/step - loss: 0.8979 - acc: 0.5626 - val_loss: 0.8198 - val_acc: 0.6144\n",
      "Epoch 8/10\n",
      "1521/1521 [==============================] - 93s 61ms/step - loss: 0.8990 - acc: 0.5605 - val_loss: 0.9058 - val_acc: 0.5578\n",
      "Epoch 9/10\n",
      "1521/1521 [==============================] - 94s 61ms/step - loss: 0.9010 - acc: 0.5581 - val_loss: 0.9686 - val_acc: 0.5443\n",
      "Epoch 10/10\n",
      "1521/1521 [==============================] - 93s 61ms/step - loss: 0.9029 - acc: 0.5588 - val_loss: 0.8369 - val_acc: 0.5978\n",
      "Test score: 0.8367964957889757\n",
      "Test accuracy: 0.5979440789473685\n",
      "Stapelgroesse (batchSize): 32\n",
      "Anzahl der Faltungsschichten: 4Layer\n",
      "Aktivierungsfunktion: elu\n",
      "Optimierungsfunktion: RMSprop\n",
      "Dropout-Rate Faltungsschicht 1: 0.1318344215856727\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 32\n",
      "Dropout-Rate Faltungsschicht 2: 0.17097711507589985\n",
      "Anzahl der Faltungsschichten: 4Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 128\n",
      "Dropout-Rate Faltungsschicht 3: 0.13585476438335367\n",
      "Anzahl der Filter-Maps Faltungsschicht 4: 64\n",
      "Dropout-Rate Faltungsschicht 4: 0.0016035241808834422\n",
      "Anzahl der Neuronen des Fully Connected Layer: 128\n",
      "Dropout-Rate Fully Connected Layer: 0.19487524692545927\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "1521/1521 [==============================] - 93s 61ms/step - loss: 10.7223 - acc: 0.3340 - val_loss: 10.7617 - val_acc: 0.3323\n",
      "Epoch 2/10\n",
      "1521/1521 [==============================] - 90s 59ms/step - loss: 10.7355 - acc: 0.3339 - val_loss: 10.7587 - val_acc: 0.3325\n",
      "Epoch 3/10\n",
      "1521/1521 [==============================] - 90s 59ms/step - loss: 10.7362 - acc: 0.3339 - val_loss: 10.7587 - val_acc: 0.3325\n",
      "Epoch 4/10\n",
      "1521/1521 [==============================] - 90s 59ms/step - loss: 10.7372 - acc: 0.3338 - val_loss: 10.7587 - val_acc: 0.3325\n",
      "Epoch 5/10\n",
      "1521/1521 [==============================] - 90s 59ms/step - loss: 10.7349 - acc: 0.3340 - val_loss: 10.7587 - val_acc: 0.3325\n",
      "Epoch 6/10\n",
      "1521/1521 [==============================] - 90s 59ms/step - loss: 10.7355 - acc: 0.3339 - val_loss: 10.7587 - val_acc: 0.3325\n",
      "Epoch 7/10\n",
      "1521/1521 [==============================] - 90s 59ms/step - loss: 10.7375 - acc: 0.3338 - val_loss: 10.7587 - val_acc: 0.3325\n",
      "Epoch 8/10\n",
      "1521/1521 [==============================] - 90s 59ms/step - loss: 10.7355 - acc: 0.3339 - val_loss: 10.7587 - val_acc: 0.3325\n",
      "Epoch 9/10\n",
      "1521/1521 [==============================] - 90s 59ms/step - loss: 10.7359 - acc: 0.3339 - val_loss: 10.7587 - val_acc: 0.3325\n",
      "Epoch 10/10\n",
      "1521/1521 [==============================] - 90s 59ms/step - loss: 10.7359 - acc: 0.3339 - val_loss: 10.7587 - val_acc: 0.3325\n",
      "Test score: 10.76174474264446\n",
      "Test accuracy: 0.3323190789473684\n",
      "Stapelgroesse (batchSize): 8\n",
      "Anzahl der Faltungsschichten: 5Layer\n",
      "Aktivierungsfunktion: relu\n",
      "Optimierungsfunktion: Adam\n",
      "Dropout-Rate Faltungsschicht 1: 0.13872893014276017\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 32\n",
      "Dropout-Rate Faltungsschicht 2: 0.3829173687339869\n",
      "Anzahl der Faltungsschichten: 5Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 128\n",
      "Dropout-Rate Faltungsschicht 3: 0.07484323080037802\n",
      "Anzahl der Filter-Maps Faltungsschicht 4: 128\n",
      "Dropout-Rate Faltungsschicht 4: 0.1051655735427622\n",
      "Anzahl der Filter-Maps Faltungsschicht 5: 128\n",
      "Dropout-Rate Faltungsschicht 5: 0.682767227755717\n",
      "Anzahl der Neuronen des Fully Connected Layer: 64\n",
      "Dropout-Rate Fully Connected Layer: 0.5310225268328831\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "6084/6084 [==============================] - 149s 24ms/step - loss: 1.0992 - acc: 0.3319 - val_loss: 1.0988 - val_acc: 0.3323\n",
      "Epoch 2/10\n",
      "6084/6084 [==============================] - 146s 24ms/step - loss: 1.0988 - acc: 0.3344 - val_loss: 1.0988 - val_acc: 0.3325\n",
      "Epoch 3/10\n",
      "6084/6084 [==============================] - 146s 24ms/step - loss: 1.0988 - acc: 0.3321 - val_loss: 1.0988 - val_acc: 0.3323\n",
      "Epoch 4/10\n",
      "6084/6084 [==============================] - 146s 24ms/step - loss: 1.0988 - acc: 0.3321 - val_loss: 1.0988 - val_acc: 0.3322\n",
      "Epoch 5/10\n",
      "6084/6084 [==============================] - 146s 24ms/step - loss: 1.0988 - acc: 0.3321 - val_loss: 1.0988 - val_acc: 0.3324\n",
      "Epoch 6/10\n",
      "6084/6084 [==============================] - 146s 24ms/step - loss: 1.0988 - acc: 0.3321 - val_loss: 1.0988 - val_acc: 0.3324\n",
      "Epoch 7/10\n",
      "6084/6084 [==============================] - 146s 24ms/step - loss: 1.0988 - acc: 0.3321 - val_loss: 1.0988 - val_acc: 0.3324\n",
      "Epoch 8/10\n",
      "6084/6084 [==============================] - 146s 24ms/step - loss: 1.0988 - acc: 0.3321 - val_loss: 1.0988 - val_acc: 0.3324\n",
      "Epoch 9/10\n",
      "6084/6084 [==============================] - 146s 24ms/step - loss: 1.0988 - acc: 0.3321 - val_loss: 1.0988 - val_acc: 0.3324\n",
      "Epoch 10/10\n",
      "6084/6084 [==============================] - 146s 24ms/step - loss: 1.0988 - acc: 0.3321 - val_loss: 1.0988 - val_acc: 0.3323\n",
      "Test score: 1.0988241953727527\n",
      "Test accuracy: 0.33226495726495725\n",
      "Stapelgroesse (batchSize): 32\n",
      "Anzahl der Faltungsschichten: 4Layer\n",
      "Aktivierungsfunktion: elu\n",
      "Optimierungsfunktion: Adam\n",
      "Dropout-Rate Faltungsschicht 1: 0.3069084930878262\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 32\n",
      "Dropout-Rate Faltungsschicht 2: 0.37556917170016396\n",
      "Anzahl der Faltungsschichten: 4Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 64\n",
      "Dropout-Rate Faltungsschicht 3: 0.3627725845511256\n",
      "Anzahl der Filter-Maps Faltungsschicht 4: 64\n",
      "Dropout-Rate Faltungsschicht 4: 0.01796643755811378\n",
      "Anzahl der Neuronen des Fully Connected Layer: 128\n",
      "Dropout-Rate Fully Connected Layer: 0.21273340082487735\n",
      "Faltungsnetz wird trainiert...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1521/1521 [==============================] - 88s 58ms/step - loss: 1.0317 - acc: 0.4521 - val_loss: 0.9688 - val_acc: 0.4936\n",
      "Epoch 2/10\n",
      "1521/1521 [==============================] - 85s 56ms/step - loss: 0.9774 - acc: 0.4965 - val_loss: 0.9296 - val_acc: 0.5235\n",
      "Epoch 3/10\n",
      "1521/1521 [==============================] - 85s 56ms/step - loss: 0.9127 - acc: 0.5476 - val_loss: 0.9145 - val_acc: 0.5537\n",
      "Epoch 4/10\n",
      "1521/1521 [==============================] - 85s 56ms/step - loss: 0.8767 - acc: 0.5712 - val_loss: 0.8829 - val_acc: 0.5727\n",
      "Epoch 5/10\n",
      "1521/1521 [==============================] - 85s 56ms/step - loss: 0.8584 - acc: 0.5843 - val_loss: 0.9264 - val_acc: 0.5671\n",
      "Epoch 6/10\n",
      "1521/1521 [==============================] - 85s 56ms/step - loss: 0.8422 - acc: 0.5926 - val_loss: 0.8925 - val_acc: 0.5793\n",
      "Epoch 7/10\n",
      "1521/1521 [==============================] - 85s 56ms/step - loss: 0.8266 - acc: 0.6045 - val_loss: 0.8873 - val_acc: 0.5923\n",
      "Epoch 8/10\n",
      "1521/1521 [==============================] - 85s 56ms/step - loss: 0.8128 - acc: 0.6110 - val_loss: 0.8749 - val_acc: 0.5963\n",
      "Epoch 9/10\n",
      "1521/1521 [==============================] - 85s 56ms/step - loss: 0.7954 - acc: 0.6228 - val_loss: 0.9075 - val_acc: 0.5823\n",
      "Epoch 10/10\n",
      "1521/1521 [==============================] - 85s 56ms/step - loss: 0.7829 - acc: 0.6257 - val_loss: 0.9202 - val_acc: 0.5970\n",
      "Test score: 0.9202187770291379\n",
      "Test accuracy: 0.5972039473684211\n",
      "Stapelgroesse (batchSize): 8\n",
      "Anzahl der Faltungsschichten: 3Layer\n",
      "Aktivierungsfunktion: elu\n",
      "Optimierungsfunktion: Adam\n",
      "Dropout-Rate Faltungsschicht 1: 0.3941294727270916\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 64\n",
      "Dropout-Rate Faltungsschicht 2: 0.35096049291960096\n",
      "Anzahl der Faltungsschichten: 3Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 128\n",
      "Dropout-Rate Faltungsschicht 3: 0.5756135869158329\n",
      "Anzahl der Neuronen des Fully Connected Layer: 64\n",
      "Dropout-Rate Fully Connected Layer: 0.16966042548763868\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "6084/6084 [==============================] - 164s 27ms/step - loss: 10.7412 - acc: 0.3333 - val_loss: 10.7322 - val_acc: 0.3342\n",
      "Epoch 2/10\n",
      "6084/6084 [==============================] - 160s 26ms/step - loss: 10.7490 - acc: 0.3331 - val_loss: 10.7335 - val_acc: 0.3341\n",
      "Epoch 3/10\n",
      "6084/6084 [==============================] - 160s 26ms/step - loss: 10.7494 - acc: 0.3331 - val_loss: 10.7335 - val_acc: 0.3341\n",
      "Epoch 4/10\n",
      "6084/6084 [==============================] - 160s 26ms/step - loss: 10.7497 - acc: 0.3331 - val_loss: 10.7321 - val_acc: 0.3342\n",
      "Epoch 5/10\n",
      "6084/6084 [==============================] - 160s 26ms/step - loss: 10.7500 - acc: 0.3330 - val_loss: 10.7335 - val_acc: 0.3341\n",
      "Epoch 6/10\n",
      "6084/6084 [==============================] - 160s 26ms/step - loss: 10.7494 - acc: 0.3331 - val_loss: 10.7335 - val_acc: 0.3341\n",
      "Epoch 7/10\n",
      "6084/6084 [==============================] - 160s 26ms/step - loss: 10.7494 - acc: 0.3331 - val_loss: 10.7308 - val_acc: 0.3342\n",
      "Epoch 8/10\n",
      "6084/6084 [==============================] - 160s 26ms/step - loss: 10.7497 - acc: 0.3331 - val_loss: 10.7361 - val_acc: 0.3339\n",
      "Epoch 9/10\n",
      "6084/6084 [==============================] - 160s 26ms/step - loss: 10.7497 - acc: 0.3331 - val_loss: 10.7361 - val_acc: 0.3339\n",
      "Epoch 10/10\n",
      "6084/6084 [==============================] - 160s 26ms/step - loss: 10.7497 - acc: 0.3331 - val_loss: 10.7361 - val_acc: 0.3339\n",
      "Test score: 10.732150633973092\n",
      "Test accuracy: 0.334155161078238\n",
      "Stapelgroesse (batchSize): 8\n",
      "Anzahl der Faltungsschichten: 4Layer\n",
      "Aktivierungsfunktion: elu\n",
      "Optimierungsfunktion: Adam\n",
      "Dropout-Rate Faltungsschicht 1: 0.6376219441266817\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 64\n",
      "Dropout-Rate Faltungsschicht 2: 0.22525415691307785\n",
      "Anzahl der Faltungsschichten: 4Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 64\n",
      "Dropout-Rate Faltungsschicht 3: 0.204913954813469\n",
      "Anzahl der Filter-Maps Faltungsschicht 4: 128\n",
      "Dropout-Rate Faltungsschicht 4: 0.3863542731207837\n",
      "Anzahl der Neuronen des Fully Connected Layer: 128\n",
      "Dropout-Rate Fully Connected Layer: 0.019006025011648452\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "6084/6084 [==============================] - 155s 26ms/step - loss: 7.6646 - acc: 0.3365 - val_loss: 10.7626 - val_acc: 0.3323\n",
      "Epoch 2/10\n",
      "6084/6084 [==============================] - 152s 25ms/step - loss: 10.7371 - acc: 0.3339 - val_loss: 10.7586 - val_acc: 0.3325\n",
      "Epoch 3/10\n",
      "6084/6084 [==============================] - 152s 25ms/step - loss: 10.7361 - acc: 0.3339 - val_loss: 10.7613 - val_acc: 0.3323\n",
      "Epoch 4/10\n",
      "6084/6084 [==============================] - 152s 25ms/step - loss: 10.7361 - acc: 0.3339 - val_loss: 10.7613 - val_acc: 0.3323\n",
      "Epoch 5/10\n",
      "6084/6084 [==============================] - 152s 25ms/step - loss: 10.7357 - acc: 0.3339 - val_loss: 10.7640 - val_acc: 0.3322\n",
      "Epoch 6/10\n",
      "6084/6084 [==============================] - 152s 25ms/step - loss: 10.7367 - acc: 0.3339 - val_loss: 10.7600 - val_acc: 0.3324\n",
      "Epoch 7/10\n",
      "6084/6084 [==============================] - 152s 25ms/step - loss: 10.7361 - acc: 0.3339 - val_loss: 10.7600 - val_acc: 0.3324\n",
      "Epoch 8/10\n",
      "6084/6084 [==============================] - 152s 25ms/step - loss: 10.7367 - acc: 0.3339 - val_loss: 10.7600 - val_acc: 0.3324\n",
      "Epoch 9/10\n",
      "6084/6084 [==============================] - 152s 25ms/step - loss: 10.7361 - acc: 0.3339 - val_loss: 10.7613 - val_acc: 0.3323\n",
      "Epoch 10/10\n",
      "6084/6084 [==============================] - 152s 25ms/step - loss: 10.7361 - acc: 0.3339 - val_loss: 10.7600 - val_acc: 0.3324\n",
      "Test score: 10.762617119357117\n",
      "Test accuracy: 0.33226495726495725\n",
      "Stapelgroesse (batchSize): 32\n",
      "Anzahl der Faltungsschichten: 3Layer\n",
      "Aktivierungsfunktion: elu\n",
      "Optimierungsfunktion: Adam\n",
      "Dropout-Rate Faltungsschicht 1: 0.025863278157258317\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 64\n",
      "Dropout-Rate Faltungsschicht 2: 0.09769179708236805\n",
      "Anzahl der Faltungsschichten: 3Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 64\n",
      "Dropout-Rate Faltungsschicht 3: 0.34928374397389894\n",
      "Anzahl der Neuronen des Fully Connected Layer: 128\n",
      "Dropout-Rate Fully Connected Layer: 0.2827461472638423\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "1521/1521 [==============================] - 101s 67ms/step - loss: 1.0457 - acc: 0.4586 - val_loss: 0.9385 - val_acc: 0.5318\n",
      "Epoch 2/10\n",
      "1521/1521 [==============================] - 98s 65ms/step - loss: 0.9139 - acc: 0.5435 - val_loss: 0.8549 - val_acc: 0.5876\n",
      "Epoch 3/10\n",
      "1521/1521 [==============================] - 98s 65ms/step - loss: 0.8554 - acc: 0.5851 - val_loss: 0.8285 - val_acc: 0.6052\n",
      "Epoch 4/10\n",
      "1521/1521 [==============================] - 98s 64ms/step - loss: 0.8158 - acc: 0.6124 - val_loss: 0.8515 - val_acc: 0.5975\n",
      "Epoch 5/10\n",
      "1521/1521 [==============================] - 98s 64ms/step - loss: 0.7554 - acc: 0.6473 - val_loss: 0.8934 - val_acc: 0.5849\n",
      "Epoch 6/10\n",
      "1521/1521 [==============================] - 98s 64ms/step - loss: 0.6925 - acc: 0.6822 - val_loss: 0.9650 - val_acc: 0.5609\n",
      "Epoch 7/10\n",
      "1521/1521 [==============================] - 98s 65ms/step - loss: 0.6278 - acc: 0.7157 - val_loss: 1.0624 - val_acc: 0.5774\n",
      "Epoch 8/10\n",
      "1521/1521 [==============================] - 98s 64ms/step - loss: 0.5847 - acc: 0.7436 - val_loss: 1.1098 - val_acc: 0.5552\n",
      "Epoch 9/10\n",
      "1521/1521 [==============================] - 98s 64ms/step - loss: 0.5359 - acc: 0.7699 - val_loss: 1.1037 - val_acc: 0.5536\n",
      "Epoch 10/10\n",
      "1521/1521 [==============================] - 98s 64ms/step - loss: 0.4973 - acc: 0.7873 - val_loss: 1.2783 - val_acc: 0.5568\n",
      "Test score: 1.278306133025571\n",
      "Test accuracy: 0.5564144736842105\n",
      "Stapelgroesse (batchSize): 8\n",
      "Anzahl der Faltungsschichten: 3Layer\n",
      "Aktivierungsfunktion: relu\n",
      "Optimierungsfunktion: RMSprop\n",
      "Dropout-Rate Faltungsschicht 1: 0.34423045967715427\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 32\n",
      "Dropout-Rate Faltungsschicht 2: 0.5220785099718168\n",
      "Anzahl der Faltungsschichten: 3Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 128\n",
      "Dropout-Rate Faltungsschicht 3: 0.6787324686833386\n",
      "Anzahl der Neuronen des Fully Connected Layer: 128\n",
      "Dropout-Rate Fully Connected Layer: 0.07605034525389257\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "6084/6084 [==============================] - 159s 26ms/step - loss: 10.7502 - acc: 0.3329 - val_loss: 9.5530 - val_acc: 0.3336\n",
      "Epoch 2/10\n",
      "6084/6084 [==============================] - 156s 26ms/step - loss: 3.8370 - acc: 0.3999 - val_loss: 1.0619 - val_acc: 0.4286\n",
      "Epoch 3/10\n",
      "6084/6084 [==============================] - 156s 26ms/step - loss: 1.0516 - acc: 0.4450 - val_loss: 1.0702 - val_acc: 0.4328\n",
      "Epoch 4/10\n",
      "6084/6084 [==============================] - 156s 26ms/step - loss: 1.0474 - acc: 0.4495 - val_loss: 1.0613 - val_acc: 0.3848\n",
      "Epoch 5/10\n",
      "6084/6084 [==============================] - 155s 26ms/step - loss: 1.0419 - acc: 0.4587 - val_loss: 1.0994 - val_acc: 0.3351\n",
      "Epoch 6/10\n",
      "6084/6084 [==============================] - 155s 26ms/step - loss: 1.0482 - acc: 0.4636 - val_loss: 1.1160 - val_acc: 0.3334\n",
      "Epoch 7/10\n",
      "6084/6084 [==============================] - 155s 26ms/step - loss: 1.0406 - acc: 0.4610 - val_loss: 1.1133 - val_acc: 0.3337\n",
      "Epoch 8/10\n",
      "6084/6084 [==============================] - 155s 25ms/step - loss: 1.0400 - acc: 0.4591 - val_loss: 1.1087 - val_acc: 0.3337\n",
      "Epoch 9/10\n",
      "6084/6084 [==============================] - 155s 25ms/step - loss: 1.0417 - acc: 0.4611 - val_loss: 1.1144 - val_acc: 0.3338\n",
      "Epoch 10/10\n",
      "6084/6084 [==============================] - 155s 26ms/step - loss: 1.0440 - acc: 0.4619 - val_loss: 1.1234 - val_acc: 0.3342\n",
      "Test score: 1.1234502656768608\n",
      "Test accuracy: 0.33407297830374755\n",
      "Stapelgroesse (batchSize): 8\n",
      "Anzahl der Faltungsschichten: 3Layer\n",
      "Aktivierungsfunktion: elu\n",
      "Optimierungsfunktion: RMSprop\n",
      "Dropout-Rate Faltungsschicht 1: 0.574501349360165\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 32\n",
      "Dropout-Rate Faltungsschicht 2: 0.6881741885335925\n",
      "Anzahl der Faltungsschichten: 3Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 128\n",
      "Dropout-Rate Faltungsschicht 3: 0.13519528146110238\n",
      "Anzahl der Neuronen des Fully Connected Layer: 128\n",
      "Dropout-Rate Fully Connected Layer: 0.23283440746125775\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "6084/6084 [==============================] - 160s 26ms/step - loss: 10.7502 - acc: 0.3329 - val_loss: 10.7396 - val_acc: 0.3336\n",
      "Epoch 2/10\n",
      "6084/6084 [==============================] - 157s 26ms/step - loss: 10.7501 - acc: 0.3330 - val_loss: 10.7422 - val_acc: 0.3334\n",
      "Epoch 3/10\n",
      "6084/6084 [==============================] - 157s 26ms/step - loss: 10.7508 - acc: 0.3330 - val_loss: 10.7396 - val_acc: 0.3336\n",
      "Epoch 4/10\n",
      "6084/6084 [==============================] - 157s 26ms/step - loss: 10.7504 - acc: 0.3330 - val_loss: 10.7396 - val_acc: 0.3336\n",
      "Epoch 5/10\n",
      "6084/6084 [==============================] - 157s 26ms/step - loss: 10.7504 - acc: 0.3330 - val_loss: 10.7396 - val_acc: 0.3336\n",
      "Epoch 6/10\n",
      "6084/6084 [==============================] - 158s 26ms/step - loss: 10.7501 - acc: 0.3330 - val_loss: 10.7422 - val_acc: 0.3334\n",
      "Epoch 7/10\n",
      "6084/6084 [==============================] - 158s 26ms/step - loss: 10.7508 - acc: 0.3330 - val_loss: 10.7422 - val_acc: 0.3334\n",
      "Epoch 8/10\n",
      "6084/6084 [==============================] - 157s 26ms/step - loss: 10.7498 - acc: 0.3331 - val_loss: 10.7382 - val_acc: 0.3337\n",
      "Epoch 9/10\n",
      "6084/6084 [==============================] - 157s 26ms/step - loss: 10.7504 - acc: 0.3330 - val_loss: 10.7382 - val_acc: 0.3337\n",
      "Epoch 10/10\n",
      "6084/6084 [==============================] - 158s 26ms/step - loss: 10.7504 - acc: 0.3330 - val_loss: 10.7396 - val_acc: 0.3336\n",
      "Test score: 10.739573832956449\n",
      "Test accuracy: 0.33357988165680474\n",
      "Stapelgroesse (batchSize): 16\n",
      "Anzahl der Faltungsschichten: 4Layer\n",
      "Aktivierungsfunktion: elu\n",
      "Optimierungsfunktion: Adam\n",
      "Dropout-Rate Faltungsschicht 1: 0.47100000809439185\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 64\n",
      "Dropout-Rate Faltungsschicht 2: 0.27609439041357026\n",
      "Anzahl der Faltungsschichten: 4Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 64\n",
      "Dropout-Rate Faltungsschicht 3: 0.1983065174980258\n",
      "Anzahl der Filter-Maps Faltungsschicht 4: 64\n",
      "Dropout-Rate Faltungsschicht 4: 0.10491229892012294\n",
      "Anzahl der Neuronen des Fully Connected Layer: 64\n",
      "Dropout-Rate Fully Connected Layer: 0.6381767709036781\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "3042/3042 [==============================] - 117s 38ms/step - loss: 1.0519 - acc: 0.4439 - val_loss: 0.9926 - val_acc: 0.4602\n",
      "Epoch 2/10\n",
      "3042/3042 [==============================] - 114s 38ms/step - loss: 0.9846 - acc: 0.4930 - val_loss: 0.9212 - val_acc: 0.5169\n",
      "Epoch 3/10\n",
      "3042/3042 [==============================] - 114s 38ms/step - loss: 0.9421 - acc: 0.5257 - val_loss: 0.9049 - val_acc: 0.5108\n",
      "Epoch 4/10\n",
      "3042/3042 [==============================] - 114s 38ms/step - loss: 0.9312 - acc: 0.5349 - val_loss: 0.8897 - val_acc: 0.5403\n",
      "Epoch 5/10\n",
      "3042/3042 [==============================] - 114s 37ms/step - loss: 0.9353 - acc: 0.5291 - val_loss: 0.8962 - val_acc: 0.5750\n",
      "Epoch 6/10\n",
      "3042/3042 [==============================] - 114s 38ms/step - loss: 0.9384 - acc: 0.5230 - val_loss: 0.8723 - val_acc: 0.5662\n",
      "Epoch 7/10\n",
      "3042/3042 [==============================] - 114s 38ms/step - loss: 0.9414 - acc: 0.5199 - val_loss: 0.8963 - val_acc: 0.5786\n",
      "Epoch 8/10\n",
      "3042/3042 [==============================] - 114s 37ms/step - loss: 0.9448 - acc: 0.5208 - val_loss: 0.9381 - val_acc: 0.4908\n",
      "Epoch 9/10\n",
      "3042/3042 [==============================] - 114s 37ms/step - loss: 0.9506 - acc: 0.5099 - val_loss: 0.8764 - val_acc: 0.5876\n",
      "Epoch 10/10\n",
      "3042/3042 [==============================] - 114s 37ms/step - loss: 0.9567 - acc: 0.5060 - val_loss: 0.9291 - val_acc: 0.5346\n",
      "Test score: 0.929176218650843\n",
      "Test accuracy: 0.5344572368421052\n",
      "Stapelgroesse (batchSize): 32\n",
      "Anzahl der Faltungsschichten: 5Layer\n",
      "Aktivierungsfunktion: elu\n",
      "Optimierungsfunktion: RMSprop\n",
      "Dropout-Rate Faltungsschicht 1: 0.14159238265557203\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 32\n",
      "Dropout-Rate Faltungsschicht 2: 0.6135572308481826\n",
      "Anzahl der Faltungsschichten: 5Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 128\n",
      "Dropout-Rate Faltungsschicht 3: 0.5760079672729373\n",
      "Anzahl der Filter-Maps Faltungsschicht 4: 128\n",
      "Dropout-Rate Faltungsschicht 4: 0.696122414469801\n",
      "Anzahl der Filter-Maps Faltungsschicht 5: 128\n",
      "Dropout-Rate Faltungsschicht 5: 0.10918026514337834\n",
      "Anzahl der Neuronen des Fully Connected Layer: 128\n",
      "Dropout-Rate Fully Connected Layer: 0.6415234621121069\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "1521/1521 [==============================] - 98s 65ms/step - loss: 1.1134 - acc: 0.3988 - val_loss: 1.0344 - val_acc: 0.4458\n",
      "Epoch 2/10\n",
      "1521/1521 [==============================] - 95s 62ms/step - loss: 1.0395 - acc: 0.4471 - val_loss: 1.0270 - val_acc: 0.4223\n",
      "Epoch 3/10\n",
      "1521/1521 [==============================] - 95s 62ms/step - loss: 1.0292 - acc: 0.4579 - val_loss: 0.9838 - val_acc: 0.4764\n",
      "Epoch 4/10\n",
      "1521/1521 [==============================] - 95s 62ms/step - loss: 1.0210 - acc: 0.4665 - val_loss: 1.0263 - val_acc: 0.4530\n",
      "Epoch 5/10\n",
      "1521/1521 [==============================] - 95s 62ms/step - loss: 1.0073 - acc: 0.4771 - val_loss: 1.0095 - val_acc: 0.4706\n",
      "Epoch 6/10\n",
      "1521/1521 [==============================] - 95s 62ms/step - loss: 0.9864 - acc: 0.4981 - val_loss: 0.9718 - val_acc: 0.5476\n",
      "Epoch 7/10\n",
      "1521/1521 [==============================] - 95s 62ms/step - loss: 0.9783 - acc: 0.5043 - val_loss: 0.9728 - val_acc: 0.5130\n",
      "Epoch 8/10\n",
      "1521/1521 [==============================] - 95s 62ms/step - loss: 0.9731 - acc: 0.5097 - val_loss: 0.8627 - val_acc: 0.5858\n",
      "Epoch 9/10\n",
      "1521/1521 [==============================] - 95s 62ms/step - loss: 0.9692 - acc: 0.5118 - val_loss: 0.8944 - val_acc: 0.5589\n",
      "Epoch 10/10\n",
      "1521/1521 [==============================] - 95s 62ms/step - loss: 0.9713 - acc: 0.5100 - val_loss: 0.8540 - val_acc: 0.5798\n",
      "Test score: 0.8541821313531776\n",
      "Test accuracy: 0.5799342105263158\n",
      "Stapelgroesse (batchSize): 32\n",
      "Anzahl der Faltungsschichten: 5Layer\n",
      "Aktivierungsfunktion: elu\n",
      "Optimierungsfunktion: RMSprop\n",
      "Dropout-Rate Faltungsschicht 1: 0.42664357921515855\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 64\n",
      "Dropout-Rate Faltungsschicht 2: 0.35339644344344656\n",
      "Anzahl der Faltungsschichten: 5Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 64\n",
      "Dropout-Rate Faltungsschicht 3: 0.4514951691589727\n",
      "Anzahl der Filter-Maps Faltungsschicht 4: 128\n",
      "Dropout-Rate Faltungsschicht 4: 0.5503352154343591\n",
      "Anzahl der Filter-Maps Faltungsschicht 5: 128\n",
      "Dropout-Rate Faltungsschicht 5: 0.038754255841003525\n",
      "Anzahl der Neuronen des Fully Connected Layer: 128\n",
      "Dropout-Rate Fully Connected Layer: 0.015249081192082901\n",
      "Faltungsnetz wird trainiert...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1521/1521 [==============================] - 105s 69ms/step - loss: 1.1180 - acc: 0.4117 - val_loss: 1.0364 - val_acc: 0.4278\n",
      "Epoch 2/10\n",
      "1521/1521 [==============================] - 102s 67ms/step - loss: 1.0030 - acc: 0.4795 - val_loss: 0.9917 - val_acc: 0.4712\n",
      "Epoch 3/10\n",
      "1521/1521 [==============================] - 101s 67ms/step - loss: 0.9516 - acc: 0.5236 - val_loss: 1.3049 - val_acc: 0.3628\n",
      "Epoch 4/10\n",
      "1521/1521 [==============================] - 101s 67ms/step - loss: 0.9230 - acc: 0.5412 - val_loss: 0.8984 - val_acc: 0.5400\n",
      "Epoch 5/10\n",
      "1521/1521 [==============================] - 101s 67ms/step - loss: 0.9073 - acc: 0.5518 - val_loss: 0.8548 - val_acc: 0.5809\n",
      "Epoch 6/10\n",
      "1521/1521 [==============================] - 101s 67ms/step - loss: 0.8937 - acc: 0.5643 - val_loss: 0.9704 - val_acc: 0.4938\n",
      "Epoch 7/10\n",
      "1521/1521 [==============================] - 102s 67ms/step - loss: 0.8812 - acc: 0.5700 - val_loss: 0.8990 - val_acc: 0.5770\n",
      "Epoch 8/10\n",
      "1521/1521 [==============================] - 101s 67ms/step - loss: 0.8747 - acc: 0.5759 - val_loss: 0.9366 - val_acc: 0.5380\n",
      "Epoch 9/10\n",
      "1521/1521 [==============================] - 101s 67ms/step - loss: 0.8709 - acc: 0.5762 - val_loss: 0.9932 - val_acc: 0.5007\n",
      "Epoch 10/10\n",
      "1521/1521 [==============================] - 101s 67ms/step - loss: 0.8680 - acc: 0.5782 - val_loss: 0.8353 - val_acc: 0.6123\n",
      "Test score: 0.8353744627613771\n",
      "Test accuracy: 0.6124177631578948\n",
      "Stapelgroesse (batchSize): 16\n",
      "Anzahl der Faltungsschichten: 3Layer\n",
      "Aktivierungsfunktion: relu\n",
      "Optimierungsfunktion: Adam\n",
      "Dropout-Rate Faltungsschicht 1: 0.2213089568208069\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 64\n",
      "Dropout-Rate Faltungsschicht 2: 0.23756832394413926\n",
      "Anzahl der Faltungsschichten: 3Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 128\n",
      "Dropout-Rate Faltungsschicht 3: 0.49086307725000733\n",
      "Anzahl der Neuronen des Fully Connected Layer: 128\n",
      "Dropout-Rate Fully Connected Layer: 0.32436239488469354\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "3042/3042 [==============================] - 136s 45ms/step - loss: 1.1017 - acc: 0.3342 - val_loss: 1.0989 - val_acc: 0.3323\n",
      "Epoch 2/10\n",
      "3042/3042 [==============================] - 133s 44ms/step - loss: 1.0866 - acc: 0.3618 - val_loss: 1.0147 - val_acc: 0.4747\n",
      "Epoch 3/10\n",
      "3042/3042 [==============================] - 133s 44ms/step - loss: 1.0090 - acc: 0.4798 - val_loss: 0.9582 - val_acc: 0.5174\n",
      "Epoch 4/10\n",
      "3042/3042 [==============================] - 133s 44ms/step - loss: 0.9520 - acc: 0.5268 - val_loss: 0.9485 - val_acc: 0.5324\n",
      "Epoch 5/10\n",
      "3042/3042 [==============================] - 133s 44ms/step - loss: 0.9266 - acc: 0.5427 - val_loss: 0.9728 - val_acc: 0.5186\n",
      "Epoch 6/10\n",
      "3042/3042 [==============================] - 133s 44ms/step - loss: 0.9106 - acc: 0.5509 - val_loss: 1.0108 - val_acc: 0.5059\n",
      "Epoch 7/10\n",
      "3042/3042 [==============================] - 133s 44ms/step - loss: 0.8987 - acc: 0.5610 - val_loss: 0.9845 - val_acc: 0.5097\n",
      "Epoch 8/10\n",
      "3042/3042 [==============================] - 133s 44ms/step - loss: 0.8871 - acc: 0.5667 - val_loss: 0.9495 - val_acc: 0.5352\n",
      "Epoch 9/10\n",
      "3042/3042 [==============================] - 133s 44ms/step - loss: 0.8808 - acc: 0.5721 - val_loss: 0.9674 - val_acc: 0.5190\n",
      "Epoch 10/10\n",
      "3042/3042 [==============================] - 133s 44ms/step - loss: 0.8732 - acc: 0.5770 - val_loss: 1.0028 - val_acc: 0.5027\n",
      "Test score: 1.0028241872003203\n",
      "Test accuracy: 0.5029605263157895\n",
      "Stapelgroesse (batchSize): 32\n",
      "Anzahl der Faltungsschichten: 4Layer\n",
      "Aktivierungsfunktion: relu\n",
      "Optimierungsfunktion: Adam\n",
      "Dropout-Rate Faltungsschicht 1: 0.5333709865046569\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 32\n",
      "Dropout-Rate Faltungsschicht 2: 0.018979800917943133\n",
      "Anzahl der Faltungsschichten: 4Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 128\n",
      "Dropout-Rate Faltungsschicht 3: 0.21477056697604296\n",
      "Anzahl der Filter-Maps Faltungsschicht 4: 128\n",
      "Dropout-Rate Faltungsschicht 4: 0.412194001775003\n",
      "Anzahl der Neuronen des Fully Connected Layer: 128\n",
      "Dropout-Rate Fully Connected Layer: 0.42627434890842325\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "1521/1521 [==============================] - 96s 63ms/step - loss: 1.0996 - acc: 0.3350 - val_loss: 1.0987 - val_acc: 0.3323\n",
      "Epoch 2/10\n",
      "1521/1521 [==============================] - 92s 60ms/step - loss: 1.0987 - acc: 0.3324 - val_loss: 1.0987 - val_acc: 0.3325\n",
      "Epoch 3/10\n",
      "1521/1521 [==============================] - 91s 60ms/step - loss: 1.0987 - acc: 0.3343 - val_loss: 1.0987 - val_acc: 0.3325\n",
      "Epoch 4/10\n",
      "1521/1521 [==============================] - 91s 60ms/step - loss: 1.0987 - acc: 0.3334 - val_loss: 1.0987 - val_acc: 0.3325\n",
      "Epoch 5/10\n",
      "1521/1521 [==============================] - 91s 60ms/step - loss: 1.0987 - acc: 0.3332 - val_loss: 1.0987 - val_acc: 0.3325\n",
      "Epoch 6/10\n",
      "1521/1521 [==============================] - 91s 60ms/step - loss: 1.0987 - acc: 0.3333 - val_loss: 1.0987 - val_acc: 0.3325\n",
      "Epoch 7/10\n",
      "1521/1521 [==============================] - 92s 60ms/step - loss: 1.0987 - acc: 0.3328 - val_loss: 1.0987 - val_acc: 0.3325\n",
      "Epoch 8/10\n",
      "1521/1521 [==============================] - 91s 60ms/step - loss: 1.0987 - acc: 0.3326 - val_loss: 1.0987 - val_acc: 0.3325\n",
      "Epoch 9/10\n",
      "1521/1521 [==============================] - 91s 60ms/step - loss: 1.0987 - acc: 0.3325 - val_loss: 1.0987 - val_acc: 0.3325\n",
      "Epoch 10/10\n",
      "1521/1521 [==============================] - 91s 60ms/step - loss: 1.0987 - acc: 0.3325 - val_loss: 1.0987 - val_acc: 0.3325\n",
      "Test score: 1.0986950698651765\n",
      "Test accuracy: 0.3323190789473684\n",
      "Stapelgroesse (batchSize): 8\n",
      "Anzahl der Faltungsschichten: 3Layer\n",
      "Aktivierungsfunktion: elu\n",
      "Optimierungsfunktion: RMSprop\n",
      "Dropout-Rate Faltungsschicht 1: 0.359044909299503\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 64\n",
      "Dropout-Rate Faltungsschicht 2: 0.024657376746418068\n",
      "Anzahl der Faltungsschichten: 3Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 128\n",
      "Dropout-Rate Faltungsschicht 3: 0.5453216256406395\n",
      "Anzahl der Neuronen des Fully Connected Layer: 64\n",
      "Dropout-Rate Fully Connected Layer: 0.6560403219765505\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "6084/6084 [==============================] - 161s 26ms/step - loss: 1.2559 - acc: 0.4127 - val_loss: 0.9715 - val_acc: 0.4942\n",
      "Epoch 2/10\n",
      "6084/6084 [==============================] - 158s 26ms/step - loss: 1.0382 - acc: 0.4792 - val_loss: 1.0532 - val_acc: 0.4778\n",
      "Epoch 3/10\n",
      "6084/6084 [==============================] - 157s 26ms/step - loss: 1.0389 - acc: 0.4841 - val_loss: 0.9375 - val_acc: 0.5030\n",
      "Epoch 4/10\n",
      "6084/6084 [==============================] - 157s 26ms/step - loss: 1.0401 - acc: 0.4797 - val_loss: 0.9060 - val_acc: 0.5308\n",
      "Epoch 5/10\n",
      "6084/6084 [==============================] - 157s 26ms/step - loss: 1.0527 - acc: 0.4777 - val_loss: 1.0554 - val_acc: 0.4771\n",
      "Epoch 6/10\n",
      "6084/6084 [==============================] - 157s 26ms/step - loss: 1.0575 - acc: 0.4788 - val_loss: 0.9768 - val_acc: 0.4966\n",
      "Epoch 7/10\n",
      "6084/6084 [==============================] - 158s 26ms/step - loss: 1.0620 - acc: 0.4715 - val_loss: 0.9467 - val_acc: 0.4738\n",
      "Epoch 8/10\n",
      "6084/6084 [==============================] - 157s 26ms/step - loss: 1.0688 - acc: 0.4700 - val_loss: 0.9503 - val_acc: 0.5670\n",
      "Epoch 9/10\n",
      "6084/6084 [==============================] - 157s 26ms/step - loss: 1.0776 - acc: 0.4652 - val_loss: 0.9733 - val_acc: 0.5221\n",
      "Epoch 10/10\n",
      "6084/6084 [==============================] - 157s 26ms/step - loss: 1.0868 - acc: 0.4648 - val_loss: 0.9295 - val_acc: 0.5016\n",
      "Test score: 0.9294394445920915\n",
      "Test accuracy: 0.5014792899408284\n",
      "Stapelgroesse (batchSize): 8\n",
      "Anzahl der Faltungsschichten: 5Layer\n",
      "Aktivierungsfunktion: elu\n",
      "Optimierungsfunktion: RMSprop\n",
      "Dropout-Rate Faltungsschicht 1: 0.25538765350136594\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 32\n",
      "Dropout-Rate Faltungsschicht 2: 0.6620047902547548\n",
      "Anzahl der Faltungsschichten: 5Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 64\n",
      "Dropout-Rate Faltungsschicht 3: 0.08908946831009872\n",
      "Anzahl der Filter-Maps Faltungsschicht 4: 128\n",
      "Dropout-Rate Faltungsschicht 4: 0.5913692508775148\n",
      "Anzahl der Filter-Maps Faltungsschicht 5: 128\n",
      "Dropout-Rate Faltungsschicht 5: 0.4335169964924364\n",
      "Anzahl der Neuronen des Fully Connected Layer: 64\n",
      "Dropout-Rate Fully Connected Layer: 0.6689639674061256\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6084/6084 [==============================] - 145s 24ms/step - loss: 1.1177 - acc: 0.3915 - val_loss: 1.0376 - val_acc: 0.4553\n",
      "Epoch 2/10\n",
      "6084/6084 [==============================] - 141s 23ms/step - loss: 1.1067 - acc: 0.3991 - val_loss: 1.0639 - val_acc: 0.4436\n",
      "Epoch 3/10\n",
      "6084/6084 [==============================] - 141s 23ms/step - loss: 1.1257 - acc: 0.3702 - val_loss: 2.4993 - val_acc: 0.3919\n",
      "Epoch 4/10\n",
      "6084/6084 [==============================] - 140s 23ms/step - loss: 1.1281 - acc: 0.3411 - val_loss: 1.1033 - val_acc: 0.3704\n",
      "Epoch 5/10\n",
      "6084/6084 [==============================] - 141s 23ms/step - loss: 1.1173 - acc: 0.3331 - val_loss: 1.1090 - val_acc: 0.3322\n",
      "Epoch 6/10\n",
      "6084/6084 [==============================] - 141s 23ms/step - loss: 1.1137 - acc: 0.3336 - val_loss: 1.1247 - val_acc: 0.3308\n",
      "Epoch 7/10\n",
      "6084/6084 [==============================] - 141s 23ms/step - loss: 1.1144 - acc: 0.3309 - val_loss: 1.1009 - val_acc: 0.3323\n",
      "Epoch 8/10\n",
      "6084/6084 [==============================] - 141s 23ms/step - loss: 1.1136 - acc: 0.3303 - val_loss: 1.0997 - val_acc: 0.3324\n",
      "Epoch 9/10\n",
      "6084/6084 [==============================] - 141s 23ms/step - loss: 1.1139 - acc: 0.3325 - val_loss: 1.2360 - val_acc: 0.2906\n",
      "Epoch 10/10\n",
      "6084/6084 [==============================] - 141s 23ms/step - loss: 1.1134 - acc: 0.3326 - val_loss: 1.2763 - val_acc: 0.3666\n",
      "Test score: 1.276227374636445\n",
      "Test accuracy: 0.36653517422748194\n",
      "Stapelgroesse (batchSize): 16\n",
      "Anzahl der Faltungsschichten: 5Layer\n",
      "Aktivierungsfunktion: elu\n",
      "Optimierungsfunktion: Adam\n",
      "Dropout-Rate Faltungsschicht 1: 0.10708794110871896\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 32\n",
      "Dropout-Rate Faltungsschicht 2: 0.16857689755736183\n",
      "Anzahl der Faltungsschichten: 5Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 64\n",
      "Dropout-Rate Faltungsschicht 3: 0.5916376099174318\n",
      "Anzahl der Filter-Maps Faltungsschicht 4: 64\n",
      "Dropout-Rate Faltungsschicht 4: 0.36718406309428453\n",
      "Anzahl der Filter-Maps Faltungsschicht 5: 128\n",
      "Dropout-Rate Faltungsschicht 5: 0.4829262941054116\n",
      "Anzahl der Neuronen des Fully Connected Layer: 64\n",
      "Dropout-Rate Fully Connected Layer: 0.02407284845771257\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "3042/3042 [==============================] - 110s 36ms/step - loss: 1.0546 - acc: 0.4333 - val_loss: 1.0122 - val_acc: 0.4558\n",
      "Epoch 2/10\n",
      "3042/3042 [==============================] - 106s 35ms/step - loss: 0.9970 - acc: 0.4758 - val_loss: 0.8915 - val_acc: 0.5607\n",
      "Epoch 3/10\n",
      "3042/3042 [==============================] - 106s 35ms/step - loss: 0.9121 - acc: 0.5472 - val_loss: 0.8934 - val_acc: 0.5488\n",
      "Epoch 4/10\n",
      "3042/3042 [==============================] - 106s 35ms/step - loss: 0.8821 - acc: 0.5663 - val_loss: 0.8371 - val_acc: 0.5945\n",
      "Epoch 5/10\n",
      "3042/3042 [==============================] - 106s 35ms/step - loss: 0.8712 - acc: 0.5720 - val_loss: 0.8333 - val_acc: 0.5995\n",
      "Epoch 6/10\n",
      "3042/3042 [==============================] - 106s 35ms/step - loss: 0.8662 - acc: 0.5762 - val_loss: 0.8402 - val_acc: 0.5779\n",
      "Epoch 7/10\n",
      "3042/3042 [==============================] - 106s 35ms/step - loss: 0.8620 - acc: 0.5779 - val_loss: 1.0499 - val_acc: 0.5611\n",
      "Epoch 8/10\n",
      "3042/3042 [==============================] - 106s 35ms/step - loss: 0.8560 - acc: 0.5812 - val_loss: 0.8624 - val_acc: 0.5825\n",
      "Epoch 9/10\n",
      "3042/3042 [==============================] - 106s 35ms/step - loss: 0.8581 - acc: 0.5789 - val_loss: 0.8670 - val_acc: 0.6075\n",
      "Epoch 10/10\n",
      "3042/3042 [==============================] - 106s 35ms/step - loss: 0.8531 - acc: 0.5840 - val_loss: 0.8675 - val_acc: 0.5939\n",
      "Test score: 0.867403960071112\n",
      "Test accuracy: 0.5938322368421053\n",
      "Stapelgroesse (batchSize): 16\n",
      "Anzahl der Faltungsschichten: 5Layer\n",
      "Aktivierungsfunktion: elu\n",
      "Optimierungsfunktion: RMSprop\n",
      "Dropout-Rate Faltungsschicht 1: 0.21698475914963355\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 64\n",
      "Dropout-Rate Faltungsschicht 2: 0.17289511557392406\n",
      "Anzahl der Faltungsschichten: 5Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 64\n",
      "Dropout-Rate Faltungsschicht 3: 0.09436418717566777\n",
      "Anzahl der Filter-Maps Faltungsschicht 4: 64\n",
      "Dropout-Rate Faltungsschicht 4: 0.20311099523053433\n",
      "Anzahl der Filter-Maps Faltungsschicht 5: 64\n",
      "Dropout-Rate Faltungsschicht 5: 0.271046632669442\n",
      "Anzahl der Neuronen des Fully Connected Layer: 128\n",
      "Dropout-Rate Fully Connected Layer: 0.42707347385220085\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "3042/3042 [==============================] - 122s 40ms/step - loss: 1.0504 - acc: 0.4441 - val_loss: 0.9306 - val_acc: 0.5327\n",
      "Epoch 2/10\n",
      "3042/3042 [==============================] - 118s 39ms/step - loss: 0.9274 - acc: 0.5397 - val_loss: 0.8552 - val_acc: 0.5885\n",
      "Epoch 3/10\n",
      "3042/3042 [==============================] - 118s 39ms/step - loss: 0.9043 - acc: 0.5558 - val_loss: 0.8988 - val_acc: 0.5255\n",
      "Epoch 4/10\n",
      "3042/3042 [==============================] - 118s 39ms/step - loss: 0.9021 - acc: 0.5576 - val_loss: 0.8434 - val_acc: 0.5959\n",
      "Epoch 5/10\n",
      "3042/3042 [==============================] - 118s 39ms/step - loss: 0.9034 - acc: 0.5603 - val_loss: 0.8856 - val_acc: 0.5222\n",
      "Epoch 6/10\n",
      "3042/3042 [==============================] - 118s 39ms/step - loss: 0.9069 - acc: 0.5543 - val_loss: 0.9279 - val_acc: 0.5457\n",
      "Epoch 7/10\n",
      "3042/3042 [==============================] - 118s 39ms/step - loss: 0.9089 - acc: 0.5531 - val_loss: 0.8454 - val_acc: 0.6076\n",
      "Epoch 8/10\n",
      "3042/3042 [==============================] - 118s 39ms/step - loss: 0.9166 - acc: 0.5502 - val_loss: 0.8778 - val_acc: 0.5871\n",
      "Epoch 9/10\n",
      "3042/3042 [==============================] - 118s 39ms/step - loss: 0.9221 - acc: 0.5475 - val_loss: 0.8903 - val_acc: 0.5716\n",
      "Epoch 10/10\n",
      "3042/3042 [==============================] - 118s 39ms/step - loss: 0.9310 - acc: 0.5409 - val_loss: 0.8718 - val_acc: 0.5565\n",
      "Test score: 0.8720221004203746\n",
      "Test accuracy: 0.5563322368421053\n",
      "Stapelgroesse (batchSize): 8\n",
      "Anzahl der Faltungsschichten: 5Layer\n",
      "Aktivierungsfunktion: relu\n",
      "Optimierungsfunktion: Adam\n",
      "Dropout-Rate Faltungsschicht 1: 0.1258157021033706\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 32\n",
      "Dropout-Rate Faltungsschicht 2: 0.13527286629130778\n",
      "Anzahl der Faltungsschichten: 5Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 64\n",
      "Dropout-Rate Faltungsschicht 3: 0.6382370746797458\n",
      "Anzahl der Filter-Maps Faltungsschicht 4: 64\n",
      "Dropout-Rate Faltungsschicht 4: 0.050726060935707945\n",
      "Anzahl der Filter-Maps Faltungsschicht 5: 64\n",
      "Dropout-Rate Faltungsschicht 5: 0.6740138303454847\n",
      "Anzahl der Neuronen des Fully Connected Layer: 128\n",
      "Dropout-Rate Fully Connected Layer: 0.187900543570194\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "6084/6084 [==============================] - 143s 23ms/step - loss: 1.0991 - acc: 0.3327 - val_loss: 1.0988 - val_acc: 0.3323\n",
      "Epoch 2/10\n",
      "6084/6084 [==============================] - 139s 23ms/step - loss: 1.0988 - acc: 0.3337 - val_loss: 1.0989 - val_acc: 0.3323\n",
      "Epoch 3/10\n",
      "6084/6084 [==============================] - 139s 23ms/step - loss: 1.0988 - acc: 0.3313 - val_loss: 1.0988 - val_acc: 0.3325\n",
      "Epoch 4/10\n",
      "6084/6084 [==============================] - 139s 23ms/step - loss: 1.0988 - acc: 0.3321 - val_loss: 1.0988 - val_acc: 0.3323\n",
      "Epoch 5/10\n",
      "6084/6084 [==============================] - 139s 23ms/step - loss: 1.0988 - acc: 0.3321 - val_loss: 1.0988 - val_acc: 0.3323\n",
      "Epoch 6/10\n",
      "6084/6084 [==============================] - 139s 23ms/step - loss: 1.0988 - acc: 0.3321 - val_loss: 1.0988 - val_acc: 0.3322\n",
      "Epoch 7/10\n",
      "6084/6084 [==============================] - 139s 23ms/step - loss: 1.0988 - acc: 0.3321 - val_loss: 1.0988 - val_acc: 0.3324\n",
      "Epoch 8/10\n",
      "6084/6084 [==============================] - 139s 23ms/step - loss: 1.0988 - acc: 0.3321 - val_loss: 1.0988 - val_acc: 0.3324\n",
      "Epoch 9/10\n",
      "6084/6084 [==============================] - 139s 23ms/step - loss: 1.0988 - acc: 0.3321 - val_loss: 1.0988 - val_acc: 0.3324\n",
      "Epoch 10/10\n",
      "6084/6084 [==============================] - 139s 23ms/step - loss: 1.0988 - acc: 0.3321 - val_loss: 1.0988 - val_acc: 0.3323\n",
      "Test score: 1.0988241953727527\n",
      "Test accuracy: 0.33226495726495725\n",
      "Stapelgroesse (batchSize): 16\n",
      "Anzahl der Faltungsschichten: 3Layer\n",
      "Aktivierungsfunktion: relu\n",
      "Optimierungsfunktion: RMSprop\n",
      "Dropout-Rate Faltungsschicht 1: 0.4951734864524238\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 32\n",
      "Dropout-Rate Faltungsschicht 2: 0.578122650697853\n",
      "Anzahl der Faltungsschichten: 3Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 64\n",
      "Dropout-Rate Faltungsschicht 3: 0.3900306299971166\n",
      "Anzahl der Neuronen des Fully Connected Layer: 64\n",
      "Dropout-Rate Fully Connected Layer: 0.3955417566913732\n",
      "Faltungsnetz wird trainiert...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3042/3042 [==============================] - 102s 33ms/step - loss: 1.0599 - acc: 0.4339 - val_loss: 1.0732 - val_acc: 0.3743\n",
      "Epoch 2/10\n",
      "3042/3042 [==============================] - 98s 32ms/step - loss: 1.0267 - acc: 0.4659 - val_loss: 1.0825 - val_acc: 0.3810\n",
      "Epoch 3/10\n",
      "3042/3042 [==============================] - 98s 32ms/step - loss: 0.9989 - acc: 0.4898 - val_loss: 1.1141 - val_acc: 0.3624\n",
      "Epoch 4/10\n",
      "3042/3042 [==============================] - 98s 32ms/step - loss: 0.9901 - acc: 0.4956 - val_loss: 1.1093 - val_acc: 0.3666\n",
      "Epoch 5/10\n",
      "3042/3042 [==============================] - 97s 32ms/step - loss: 0.9875 - acc: 0.5022 - val_loss: 1.1372 - val_acc: 0.3451\n",
      "Epoch 6/10\n",
      "3042/3042 [==============================] - 98s 32ms/step - loss: 0.9865 - acc: 0.5029 - val_loss: 1.1672 - val_acc: 0.3369\n",
      "Epoch 7/10\n",
      "3042/3042 [==============================] - 98s 32ms/step - loss: 0.9909 - acc: 0.5002 - val_loss: 1.1061 - val_acc: 0.3627\n",
      "Epoch 8/10\n",
      "3042/3042 [==============================] - 97s 32ms/step - loss: 0.9937 - acc: 0.4985 - val_loss: 1.1506 - val_acc: 0.3338\n",
      "Epoch 9/10\n",
      "3042/3042 [==============================] - 97s 32ms/step - loss: 0.9866 - acc: 0.5017 - val_loss: 1.1169 - val_acc: 0.3626\n",
      "Epoch 10/10\n",
      "3042/3042 [==============================] - 98s 32ms/step - loss: 0.9869 - acc: 0.5021 - val_loss: 1.0783 - val_acc: 0.3832\n",
      "Test score: 1.078309458729468\n",
      "Test accuracy: 0.38330592105263156\n",
      "Stapelgroesse (batchSize): 8\n",
      "Anzahl der Faltungsschichten: 3Layer\n",
      "Aktivierungsfunktion: relu\n",
      "Optimierungsfunktion: Adam\n",
      "Dropout-Rate Faltungsschicht 1: 0.13222469701785414\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 64\n",
      "Dropout-Rate Faltungsschicht 2: 0.46155967454661834\n",
      "Anzahl der Faltungsschichten: 3Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 64\n",
      "Dropout-Rate Faltungsschicht 3: 0.36375913271863597\n",
      "Anzahl der Neuronen des Fully Connected Layer: 64\n",
      "Dropout-Rate Fully Connected Layer: 0.5908070034329119\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "6084/6084 [==============================] - 146s 24ms/step - loss: 1.1004 - acc: 0.3330 - val_loss: 1.0988 - val_acc: 0.3323\n",
      "Epoch 2/10\n",
      "6084/6084 [==============================] - 142s 23ms/step - loss: 1.0988 - acc: 0.3318 - val_loss: 1.0988 - val_acc: 0.3323\n",
      "Epoch 3/10\n",
      "6084/6084 [==============================] - 142s 23ms/step - loss: 1.0988 - acc: 0.3321 - val_loss: 1.0988 - val_acc: 0.3323\n",
      "Epoch 4/10\n",
      "6084/6084 [==============================] - 142s 23ms/step - loss: 1.0988 - acc: 0.3321 - val_loss: 1.0988 - val_acc: 0.3323\n",
      "Epoch 5/10\n",
      "6084/6084 [==============================] - 142s 23ms/step - loss: 1.0988 - acc: 0.3321 - val_loss: 1.0988 - val_acc: 0.3323\n",
      "Epoch 6/10\n",
      "6084/6084 [==============================] - 142s 23ms/step - loss: 1.0988 - acc: 0.3321 - val_loss: 1.0988 - val_acc: 0.3322\n",
      "Epoch 7/10\n",
      "6084/6084 [==============================] - 142s 23ms/step - loss: 1.0988 - acc: 0.3321 - val_loss: 1.0988 - val_acc: 0.3324\n",
      "Epoch 8/10\n",
      "6084/6084 [==============================] - 142s 23ms/step - loss: 1.0988 - acc: 0.3321 - val_loss: 1.0988 - val_acc: 0.3324\n",
      "Epoch 9/10\n",
      "6084/6084 [==============================] - 142s 23ms/step - loss: 1.0988 - acc: 0.3321 - val_loss: 1.0988 - val_acc: 0.3323\n",
      "Epoch 10/10\n",
      "6084/6084 [==============================] - 142s 23ms/step - loss: 1.0988 - acc: 0.3321 - val_loss: 1.0988 - val_acc: 0.3323\n",
      "Test score: 1.0988241953727527\n",
      "Test accuracy: 0.33226495726495725\n",
      "Stapelgroesse (batchSize): 8\n",
      "Anzahl der Faltungsschichten: 5Layer\n",
      "Aktivierungsfunktion: elu\n",
      "Optimierungsfunktion: RMSprop\n",
      "Dropout-Rate Faltungsschicht 1: 0.6792231737343013\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 64\n",
      "Dropout-Rate Faltungsschicht 2: 0.3093562096369121\n",
      "Anzahl der Faltungsschichten: 5Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 64\n",
      "Dropout-Rate Faltungsschicht 3: 0.4315036240930641\n",
      "Anzahl der Filter-Maps Faltungsschicht 4: 64\n",
      "Dropout-Rate Faltungsschicht 4: 0.4751575788425747\n",
      "Anzahl der Filter-Maps Faltungsschicht 5: 64\n",
      "Dropout-Rate Faltungsschicht 5: 0.06147997220119776\n",
      "Anzahl der Neuronen des Fully Connected Layer: 64\n",
      "Dropout-Rate Fully Connected Layer: 0.5038170608472976\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "6084/6084 [==============================] - 156s 26ms/step - loss: 1.0960 - acc: 0.4082 - val_loss: 1.0286 - val_acc: 0.4346\n",
      "Epoch 2/10\n",
      "6084/6084 [==============================] - 152s 25ms/step - loss: 1.0660 - acc: 0.4304 - val_loss: 1.0189 - val_acc: 0.4356\n",
      "Epoch 3/10\n",
      "6084/6084 [==============================] - 152s 25ms/step - loss: 1.0692 - acc: 0.4284 - val_loss: 1.0743 - val_acc: 0.4688\n",
      "Epoch 4/10\n",
      "6084/6084 [==============================] - 151s 25ms/step - loss: 1.0711 - acc: 0.4312 - val_loss: 6.5568 - val_acc: 0.3390\n",
      "Epoch 5/10\n",
      "6084/6084 [==============================] - 152s 25ms/step - loss: 1.0784 - acc: 0.4268 - val_loss: 7.0282 - val_acc: 0.3897\n",
      "Epoch 6/10\n",
      "6084/6084 [==============================] - 224s 37ms/step - loss: 1.0879 - acc: 0.4236 - val_loss: 7.6749 - val_acc: 0.3881\n",
      "Epoch 7/10\n",
      "6084/6084 [==============================] - 160s 26ms/step - loss: 6.2796 - acc: 0.3695 - val_loss: 10.7640 - val_acc: 0.3322\n",
      "Epoch 8/10\n",
      "6084/6084 [==============================] - 151s 25ms/step - loss: 10.7371 - acc: 0.3339 - val_loss: 10.7600 - val_acc: 0.3324\n",
      "Epoch 9/10\n",
      "6084/6084 [==============================] - 153s 25ms/step - loss: 10.7367 - acc: 0.3339 - val_loss: 10.7613 - val_acc: 0.3323\n",
      "Epoch 10/10\n",
      "6084/6084 [==============================] - 154s 25ms/step - loss: 10.7377 - acc: 0.3338 - val_loss: 10.7600 - val_acc: 0.3324\n",
      "Test score: 10.762617119357117\n",
      "Test accuracy: 0.33226495726495725\n",
      "Stapelgroesse (batchSize): 8\n",
      "Anzahl der Faltungsschichten: 5Layer\n",
      "Aktivierungsfunktion: relu\n",
      "Optimierungsfunktion: Adam\n",
      "Dropout-Rate Faltungsschicht 1: 0.30708821143141685\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 32\n",
      "Dropout-Rate Faltungsschicht 2: 0.425181051865627\n",
      "Anzahl der Faltungsschichten: 5Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 64\n",
      "Dropout-Rate Faltungsschicht 3: 0.14062459552915882\n",
      "Anzahl der Filter-Maps Faltungsschicht 4: 128\n",
      "Dropout-Rate Faltungsschicht 4: 0.590648620829584\n",
      "Anzahl der Filter-Maps Faltungsschicht 5: 64\n",
      "Dropout-Rate Faltungsschicht 5: 0.33525070377372224\n",
      "Anzahl der Neuronen des Fully Connected Layer: 128\n",
      "Dropout-Rate Fully Connected Layer: 0.46580928432226254\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "6084/6084 [==============================] - 150s 25ms/step - loss: 1.0994 - acc: 0.3339 - val_loss: 1.0988 - val_acc: 0.3323\n",
      "Epoch 2/10\n",
      "6084/6084 [==============================] - 146s 24ms/step - loss: 1.0989 - acc: 0.3327 - val_loss: 1.0988 - val_acc: 0.3325\n",
      "Epoch 3/10\n",
      "6084/6084 [==============================] - 144s 24ms/step - loss: 1.0988 - acc: 0.3366 - val_loss: 1.0988 - val_acc: 0.3323\n",
      "Epoch 4/10\n",
      "6084/6084 [==============================] - 145s 24ms/step - loss: 1.0988 - acc: 0.3322 - val_loss: 1.0988 - val_acc: 0.3323\n",
      "Epoch 5/10\n",
      "6084/6084 [==============================] - 145s 24ms/step - loss: 1.0988 - acc: 0.3321 - val_loss: 1.0988 - val_acc: 0.3322\n",
      "Epoch 6/10\n",
      "6084/6084 [==============================] - 143s 24ms/step - loss: 1.0988 - acc: 0.3321 - val_loss: 1.0988 - val_acc: 0.3322\n",
      "Epoch 7/10\n",
      "6084/6084 [==============================] - 145s 24ms/step - loss: 1.0988 - acc: 0.3321 - val_loss: 1.0988 - val_acc: 0.3324\n",
      "Epoch 8/10\n",
      "6084/6084 [==============================] - 143s 23ms/step - loss: 1.0988 - acc: 0.3321 - val_loss: 1.0988 - val_acc: 0.3323\n",
      "Epoch 9/10\n",
      "6084/6084 [==============================] - 143s 23ms/step - loss: 1.0988 - acc: 0.3321 - val_loss: 1.0988 - val_acc: 0.3324\n",
      "Epoch 10/10\n",
      "6084/6084 [==============================] - 143s 24ms/step - loss: 1.0988 - acc: 0.3321 - val_loss: 1.0988 - val_acc: 0.3324\n",
      "Test score: 1.0988241953727527\n",
      "Test accuracy: 0.33226495726495725\n",
      "Stapelgroesse (batchSize): 32\n",
      "Anzahl der Faltungsschichten: 4Layer\n",
      "Aktivierungsfunktion: elu\n",
      "Optimierungsfunktion: Adam\n",
      "Dropout-Rate Faltungsschicht 1: 0.5248153728682362\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 64\n",
      "Dropout-Rate Faltungsschicht 2: 0.013771620713159093\n",
      "Anzahl der Faltungsschichten: 4Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 64\n",
      "Dropout-Rate Faltungsschicht 3: 0.06296007551272016\n",
      "Anzahl der Filter-Maps Faltungsschicht 4: 64\n",
      "Dropout-Rate Faltungsschicht 4: 0.4130000695689919\n",
      "Anzahl der Neuronen des Fully Connected Layer: 128\n",
      "Dropout-Rate Fully Connected Layer: 0.5014865063866658\n",
      "Faltungsnetz wird trainiert...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1521/1521 [==============================] - 104s 69ms/step - loss: 1.0363 - acc: 0.4526 - val_loss: 0.9934 - val_acc: 0.4921\n",
      "Epoch 2/10\n",
      "1521/1521 [==============================] - 99s 65ms/step - loss: 0.9568 - acc: 0.5175 - val_loss: 0.9295 - val_acc: 0.5018\n",
      "Epoch 3/10\n",
      "1521/1521 [==============================] - 100s 66ms/step - loss: 0.9039 - acc: 0.5530 - val_loss: 0.8530 - val_acc: 0.5838\n",
      "Epoch 4/10\n",
      "1521/1521 [==============================] - 100s 66ms/step - loss: 0.8884 - acc: 0.5649 - val_loss: 0.8625 - val_acc: 0.5937\n",
      "Epoch 5/10\n",
      "1521/1521 [==============================] - 101s 66ms/step - loss: 0.8793 - acc: 0.5687 - val_loss: 0.8339 - val_acc: 0.6100\n",
      "Epoch 6/10\n",
      "1521/1521 [==============================] - 100s 66ms/step - loss: 0.8703 - acc: 0.5756 - val_loss: 0.9249 - val_acc: 0.5659\n",
      "Epoch 7/10\n",
      "1521/1521 [==============================] - 100s 66ms/step - loss: 0.8676 - acc: 0.5761 - val_loss: 0.8247 - val_acc: 0.6163\n",
      "Epoch 8/10\n",
      "1521/1521 [==============================] - 100s 66ms/step - loss: 0.8655 - acc: 0.5748 - val_loss: 0.9428 - val_acc: 0.5602\n",
      "Epoch 9/10\n",
      "1521/1521 [==============================] - 100s 66ms/step - loss: 0.8604 - acc: 0.5779 - val_loss: 0.8197 - val_acc: 0.6139\n",
      "Epoch 10/10\n",
      "1521/1521 [==============================] - 100s 66ms/step - loss: 0.8614 - acc: 0.5779 - val_loss: 0.9364 - val_acc: 0.5755\n",
      "Test score: 0.936642724746152\n",
      "Test accuracy: 0.5757401315789473\n",
      "Stapelgroesse (batchSize): 8\n",
      "Anzahl der Faltungsschichten: 3Layer\n",
      "Aktivierungsfunktion: relu\n",
      "Optimierungsfunktion: RMSprop\n",
      "Dropout-Rate Faltungsschicht 1: 0.06307711151849073\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 64\n",
      "Dropout-Rate Faltungsschicht 2: 0.20382445694216605\n",
      "Anzahl der Faltungsschichten: 3Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 128\n",
      "Dropout-Rate Faltungsschicht 3: 0.12248203532640545\n",
      "Anzahl der Neuronen des Fully Connected Layer: 64\n",
      "Dropout-Rate Fully Connected Layer: 0.6866387698783794\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "6084/6084 [==============================] - 163s 27ms/step - loss: 1.0883 - acc: 0.4074 - val_loss: 1.0784 - val_acc: 0.4647\n",
      "Epoch 2/10\n",
      "6084/6084 [==============================] - 159s 26ms/step - loss: 1.0619 - acc: 0.4464 - val_loss: 1.0335 - val_acc: 0.4391\n",
      "Epoch 3/10\n",
      "6084/6084 [==============================] - 158s 26ms/step - loss: 1.0698 - acc: 0.4417 - val_loss: 1.0663 - val_acc: 0.4113\n",
      "Epoch 4/10\n",
      "6084/6084 [==============================] - 158s 26ms/step - loss: 1.0861 - acc: 0.4334 - val_loss: 1.0981 - val_acc: 0.4171\n",
      "Epoch 5/10\n",
      "6084/6084 [==============================] - 158s 26ms/step - loss: 1.0820 - acc: 0.4377 - val_loss: 1.0952 - val_acc: 0.3439\n",
      "Epoch 6/10\n",
      "6084/6084 [==============================] - 158s 26ms/step - loss: 1.0936 - acc: 0.4367 - val_loss: 1.0980 - val_acc: 0.3333\n",
      "Epoch 7/10\n",
      "6084/6084 [==============================] - 157s 26ms/step - loss: 1.0812 - acc: 0.4325 - val_loss: 1.1060 - val_acc: 0.3457\n",
      "Epoch 8/10\n",
      "6084/6084 [==============================] - 157s 26ms/step - loss: 1.0919 - acc: 0.4384 - val_loss: 1.1063 - val_acc: 0.3337\n",
      "Epoch 9/10\n",
      "6084/6084 [==============================] - 158s 26ms/step - loss: 1.0943 - acc: 0.4309 - val_loss: 1.1143 - val_acc: 0.3337\n",
      "Epoch 10/10\n",
      "6084/6084 [==============================] - 157s 26ms/step - loss: 1.0932 - acc: 0.4323 - val_loss: 1.1891 - val_acc: 0.3340\n",
      "Test score: 1.1891658110182823\n",
      "Test accuracy: 0.3339086127547666\n",
      "Stapelgroesse (batchSize): 8\n",
      "Anzahl der Faltungsschichten: 3Layer\n",
      "Aktivierungsfunktion: relu\n",
      "Optimierungsfunktion: Adam\n",
      "Dropout-Rate Faltungsschicht 1: 0.36856104697946446\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 64\n",
      "Dropout-Rate Faltungsschicht 2: 0.5417435612067958\n",
      "Anzahl der Faltungsschichten: 3Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 128\n",
      "Dropout-Rate Faltungsschicht 3: 0.006998887818033605\n",
      "Anzahl der Neuronen des Fully Connected Layer: 64\n",
      "Dropout-Rate Fully Connected Layer: 0.378686634478515\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "6084/6084 [==============================] - 168s 28ms/step - loss: 1.1034 - acc: 0.3327 - val_loss: 1.0988 - val_acc: 0.3323\n",
      "Epoch 2/10\n",
      "6084/6084 [==============================] - 164s 27ms/step - loss: 1.0989 - acc: 0.3320 - val_loss: 1.0988 - val_acc: 0.3323\n",
      "Epoch 3/10\n",
      "6084/6084 [==============================] - 164s 27ms/step - loss: 1.0988 - acc: 0.3319 - val_loss: 1.0988 - val_acc: 0.3325\n",
      "Epoch 4/10\n",
      "6084/6084 [==============================] - 163s 27ms/step - loss: 1.0988 - acc: 0.3321 - val_loss: 1.0988 - val_acc: 0.3323\n",
      "Epoch 5/10\n",
      "6084/6084 [==============================] - 163s 27ms/step - loss: 1.0988 - acc: 0.3321 - val_loss: 1.0988 - val_acc: 0.3323\n",
      "Epoch 6/10\n",
      "6084/6084 [==============================] - 163s 27ms/step - loss: 1.0988 - acc: 0.3321 - val_loss: 1.0988 - val_acc: 0.3323\n",
      "Epoch 7/10\n",
      "6084/6084 [==============================] - 163s 27ms/step - loss: 1.0988 - acc: 0.3321 - val_loss: 1.0988 - val_acc: 0.3323\n",
      "Epoch 8/10\n",
      "6084/6084 [==============================] - 164s 27ms/step - loss: 1.0988 - acc: 0.3321 - val_loss: 1.0988 - val_acc: 0.3323\n",
      "Epoch 9/10\n",
      "6084/6084 [==============================] - 163s 27ms/step - loss: 1.0988 - acc: 0.3321 - val_loss: 1.0988 - val_acc: 0.3323\n",
      "Epoch 10/10\n",
      "6084/6084 [==============================] - 163s 27ms/step - loss: 1.0988 - acc: 0.3321 - val_loss: 1.0988 - val_acc: 0.3323\n",
      "Test score: 1.0988241953727527\n",
      "Test accuracy: 0.33226495726495725\n",
      "Stapelgroesse (batchSize): 8\n",
      "Anzahl der Faltungsschichten: 4Layer\n",
      "Aktivierungsfunktion: elu\n",
      "Optimierungsfunktion: Adam\n",
      "Dropout-Rate Faltungsschicht 1: 0.34597742155086353\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 32\n",
      "Dropout-Rate Faltungsschicht 2: 0.6186099992277365\n",
      "Anzahl der Faltungsschichten: 4Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 64\n",
      "Dropout-Rate Faltungsschicht 3: 0.13728851926848906\n",
      "Anzahl der Filter-Maps Faltungsschicht 4: 128\n",
      "Dropout-Rate Faltungsschicht 4: 0.13883546240860023\n",
      "Anzahl der Neuronen des Fully Connected Layer: 64\n",
      "Dropout-Rate Fully Connected Layer: 0.44849361739763\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "6084/6084 [==============================] - 146s 24ms/step - loss: 1.0764 - acc: 0.4290 - val_loss: 1.0140 - val_acc: 0.4649\n",
      "Epoch 2/10\n",
      "6084/6084 [==============================] - 143s 23ms/step - loss: 1.0248 - acc: 0.4605 - val_loss: 0.9890 - val_acc: 0.4840\n",
      "Epoch 3/10\n",
      "6084/6084 [==============================] - 142s 23ms/step - loss: 0.9979 - acc: 0.4818 - val_loss: 0.9164 - val_acc: 0.5212\n",
      "Epoch 4/10\n",
      "6084/6084 [==============================] - 141s 23ms/step - loss: 1.0005 - acc: 0.4734 - val_loss: 0.9408 - val_acc: 0.5308\n",
      "Epoch 5/10\n",
      "6084/6084 [==============================] - 231s 38ms/step - loss: 1.0042 - acc: 0.4647 - val_loss: 1.8098 - val_acc: 0.4454\n",
      "Epoch 6/10\n",
      "6084/6084 [==============================] - 163s 27ms/step - loss: 1.0342 - acc: 0.4310 - val_loss: 1.1010 - val_acc: 0.3343\n",
      "Epoch 7/10\n",
      "6084/6084 [==============================] - 142s 23ms/step - loss: 1.1104 - acc: 0.3314 - val_loss: 1.1014 - val_acc: 0.3322\n",
      "Epoch 8/10\n",
      "6084/6084 [==============================] - 141s 23ms/step - loss: 1.1095 - acc: 0.3316 - val_loss: 1.1007 - val_acc: 0.3324\n",
      "Epoch 9/10\n",
      "6084/6084 [==============================] - 140s 23ms/step - loss: 1.1097 - acc: 0.3333 - val_loss: 1.1005 - val_acc: 0.3323\n",
      "Epoch 10/10\n",
      "6084/6084 [==============================] - 141s 23ms/step - loss: 1.1096 - acc: 0.3292 - val_loss: 1.1012 - val_acc: 0.3324\n",
      "Test score: 1.1012044905989207\n",
      "Test accuracy: 0.33226495726495725\n",
      "Stapelgroesse (batchSize): 32\n",
      "Anzahl der Faltungsschichten: 3Layer\n",
      "Aktivierungsfunktion: elu\n",
      "Optimierungsfunktion: RMSprop\n",
      "Dropout-Rate Faltungsschicht 1: 0.39369408404795847\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 32\n",
      "Dropout-Rate Faltungsschicht 2: 0.04882610210582032\n",
      "Anzahl der Faltungsschichten: 3Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 64\n",
      "Dropout-Rate Faltungsschicht 3: 0.06540654020186992\n",
      "Anzahl der Neuronen des Fully Connected Layer: 128\n",
      "Dropout-Rate Fully Connected Layer: 0.2192185143600429\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "1521/1521 [==============================] - 92s 61ms/step - loss: 10.7455 - acc: 0.3329 - val_loss: 10.7419 - val_acc: 0.3336\n",
      "Epoch 2/10\n",
      "1521/1521 [==============================] - 87s 57ms/step - loss: 10.7486 - acc: 0.3331 - val_loss: 10.7441 - val_acc: 0.3334\n",
      "Epoch 3/10\n",
      "1521/1521 [==============================] - 87s 57ms/step - loss: 10.7480 - acc: 0.3332 - val_loss: 10.7441 - val_acc: 0.3334\n",
      "Epoch 4/10\n",
      "1521/1521 [==============================] - 87s 57ms/step - loss: 10.7473 - acc: 0.3332 - val_loss: 10.7441 - val_acc: 0.3334\n",
      "Epoch 5/10\n",
      "1521/1521 [==============================] - 87s 57ms/step - loss: 10.7490 - acc: 0.3331 - val_loss: 10.7441 - val_acc: 0.3334\n",
      "Epoch 6/10\n",
      "1521/1521 [==============================] - 87s 57ms/step - loss: 10.7477 - acc: 0.3332 - val_loss: 10.7441 - val_acc: 0.3334\n",
      "Epoch 7/10\n",
      "1521/1521 [==============================] - 87s 58ms/step - loss: 10.7473 - acc: 0.3332 - val_loss: 10.7441 - val_acc: 0.3334\n",
      "Epoch 8/10\n",
      "1521/1521 [==============================] - 88s 58ms/step - loss: 10.7486 - acc: 0.3331 - val_loss: 10.7441 - val_acc: 0.3334\n",
      "Epoch 9/10\n",
      "1521/1521 [==============================] - 87s 57ms/step - loss: 10.7493 - acc: 0.3331 - val_loss: 10.7441 - val_acc: 0.3334\n",
      "Epoch 10/10\n",
      "1521/1521 [==============================] - 88s 58ms/step - loss: 10.7490 - acc: 0.3331 - val_loss: 10.7441 - val_acc: 0.3334\n",
      "Test score: 10.74186221674869\n",
      "Test accuracy: 0.3335526315789474\n",
      "Stapelgroesse (batchSize): 32\n",
      "Anzahl der Faltungsschichten: 3Layer\n",
      "Aktivierungsfunktion: relu\n",
      "Optimierungsfunktion: Adam\n",
      "Dropout-Rate Faltungsschicht 1: 0.4938729716239127\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 64\n",
      "Dropout-Rate Faltungsschicht 2: 0.3577944426633437\n",
      "Anzahl der Faltungsschichten: 3Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 64\n",
      "Dropout-Rate Faltungsschicht 3: 0.648938910990447\n",
      "Anzahl der Neuronen des Fully Connected Layer: 128\n",
      "Dropout-Rate Fully Connected Layer: 0.09833143896930721\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "1521/1521 [==============================] - 103s 68ms/step - loss: 1.0908 - acc: 0.3653 - val_loss: 1.0192 - val_acc: 0.4586\n",
      "Epoch 2/10\n",
      "1521/1521 [==============================] - 97s 64ms/step - loss: 1.0201 - acc: 0.4638 - val_loss: 1.0127 - val_acc: 0.4680\n",
      "Epoch 3/10\n",
      "1521/1521 [==============================] - 97s 64ms/step - loss: 0.9945 - acc: 0.4834 - val_loss: 1.0275 - val_acc: 0.4507\n",
      "Epoch 4/10\n",
      "1521/1521 [==============================] - 97s 64ms/step - loss: 0.9801 - acc: 0.4911 - val_loss: 0.9819 - val_acc: 0.4976\n",
      "Epoch 5/10\n",
      "1521/1521 [==============================] - 98s 64ms/step - loss: 0.9649 - acc: 0.5057 - val_loss: 0.9819 - val_acc: 0.4885\n",
      "Epoch 6/10\n",
      "1521/1521 [==============================] - 99s 65ms/step - loss: 0.9480 - acc: 0.5221 - val_loss: 0.9864 - val_acc: 0.4967\n",
      "Epoch 7/10\n",
      "1521/1521 [==============================] - 98s 65ms/step - loss: 0.9314 - acc: 0.5359 - val_loss: 1.0021 - val_acc: 0.4988\n",
      "Epoch 8/10\n",
      "1521/1521 [==============================] - 98s 65ms/step - loss: 0.9180 - acc: 0.5438 - val_loss: 0.9714 - val_acc: 0.5081\n",
      "Epoch 9/10\n",
      "1521/1521 [==============================] - 98s 64ms/step - loss: 0.9076 - acc: 0.5527 - val_loss: 1.0126 - val_acc: 0.4810\n",
      "Epoch 10/10\n",
      "1521/1521 [==============================] - 97s 64ms/step - loss: 0.8962 - acc: 0.5597 - val_loss: 0.9989 - val_acc: 0.4797\n",
      "Test score: 0.9990403600429234\n",
      "Test accuracy: 0.47952302631578947\n",
      "Stapelgroesse (batchSize): 16\n",
      "Anzahl der Faltungsschichten: 3Layer\n",
      "Aktivierungsfunktion: elu\n",
      "Optimierungsfunktion: RMSprop\n",
      "Dropout-Rate Faltungsschicht 1: 0.0030492067550873855\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 32\n",
      "Dropout-Rate Faltungsschicht 2: 0.36724989130755226\n",
      "Anzahl der Faltungsschichten: 3Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 64\n",
      "Dropout-Rate Faltungsschicht 3: 0.09099759210319268\n",
      "Anzahl der Neuronen des Fully Connected Layer: 128\n",
      "Dropout-Rate Fully Connected Layer: 0.3253263629327289\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "3042/3042 [==============================] - 111s 36ms/step - loss: 10.7487 - acc: 0.3329 - val_loss: 10.7419 - val_acc: 0.3336\n",
      "Epoch 2/10\n",
      "3042/3042 [==============================] - 106s 35ms/step - loss: 10.7498 - acc: 0.3331 - val_loss: 10.7432 - val_acc: 0.3335\n",
      "Epoch 3/10\n",
      "3042/3042 [==============================] - 105s 35ms/step - loss: 10.7498 - acc: 0.3331 - val_loss: 10.7432 - val_acc: 0.3335\n",
      "Epoch 4/10\n",
      "3042/3042 [==============================] - 106s 35ms/step - loss: 10.7498 - acc: 0.3331 - val_loss: 10.7432 - val_acc: 0.3335\n",
      "Epoch 5/10\n",
      "3042/3042 [==============================] - 107s 35ms/step - loss: 10.7403 - acc: 0.3336 - val_loss: 10.7339 - val_acc: 0.3340\n",
      "Epoch 6/10\n",
      "3042/3042 [==============================] - 106s 35ms/step - loss: 10.7497 - acc: 0.3331 - val_loss: 10.7339 - val_acc: 0.3340\n",
      "Epoch 7/10\n",
      "3042/3042 [==============================] - 107s 35ms/step - loss: 10.7507 - acc: 0.3330 - val_loss: 10.7339 - val_acc: 0.3340\n",
      "Epoch 8/10\n",
      "3042/3042 [==============================] - 106s 35ms/step - loss: 10.7514 - acc: 0.3330 - val_loss: 10.7339 - val_acc: 0.3340\n",
      "Epoch 9/10\n",
      "3042/3042 [==============================] - 106s 35ms/step - loss: 10.7497 - acc: 0.3331 - val_loss: 10.7339 - val_acc: 0.3340\n",
      "Epoch 10/10\n",
      "3042/3042 [==============================] - 107s 35ms/step - loss: 10.7507 - acc: 0.3330 - val_loss: 10.7339 - val_acc: 0.3340\n",
      "Test score: 10.732583765607131\n",
      "Test accuracy: 0.3341282894736842\n",
      "Stapelgroesse (batchSize): 32\n",
      "Anzahl der Faltungsschichten: 3Layer\n",
      "Aktivierungsfunktion: relu\n",
      "Optimierungsfunktion: RMSprop\n",
      "Dropout-Rate Faltungsschicht 1: 0.630488342187824\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 64\n",
      "Dropout-Rate Faltungsschicht 2: 0.656156000747102\n",
      "Anzahl der Faltungsschichten: 3Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 64\n",
      "Dropout-Rate Faltungsschicht 3: 0.22607023985917013\n",
      "Anzahl der Neuronen des Fully Connected Layer: 64\n",
      "Dropout-Rate Fully Connected Layer: 0.07759341739291725\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "1521/1521 [==============================] - 101s 66ms/step - loss: 10.4328 - acc: 0.3330 - val_loss: 1.0958 - val_acc: 0.3865\n",
      "Epoch 2/10\n",
      "1521/1521 [==============================] - 96s 63ms/step - loss: 1.0401 - acc: 0.4478 - val_loss: 0.9832 - val_acc: 0.4862\n",
      "Epoch 3/10\n",
      "1521/1521 [==============================] - 95s 63ms/step - loss: 1.0078 - acc: 0.4727 - val_loss: 0.9902 - val_acc: 0.4912\n",
      "Epoch 4/10\n",
      "1521/1521 [==============================] - 95s 63ms/step - loss: 0.9820 - acc: 0.4989 - val_loss: 1.0821 - val_acc: 0.4136\n",
      "Epoch 5/10\n",
      "1521/1521 [==============================] - 95s 63ms/step - loss: 0.9589 - acc: 0.5203 - val_loss: 1.0072 - val_acc: 0.4722\n",
      "Epoch 6/10\n",
      "1521/1521 [==============================] - 95s 63ms/step - loss: 0.9491 - acc: 0.5263 - val_loss: 1.0327 - val_acc: 0.4467\n",
      "Epoch 7/10\n",
      "1521/1521 [==============================] - 96s 63ms/step - loss: 0.9418 - acc: 0.5312 - val_loss: 1.0634 - val_acc: 0.4013\n",
      "Epoch 8/10\n",
      "1521/1521 [==============================] - 95s 63ms/step - loss: 0.9384 - acc: 0.5370 - val_loss: 1.0973 - val_acc: 0.3536\n",
      "Epoch 9/10\n",
      "1521/1521 [==============================] - 95s 63ms/step - loss: 0.9328 - acc: 0.5404 - val_loss: 1.0625 - val_acc: 0.3927\n",
      "Epoch 10/10\n",
      "1521/1521 [==============================] - 95s 63ms/step - loss: 0.9334 - acc: 0.5413 - val_loss: 0.9990 - val_acc: 0.4909\n",
      "Test score: 0.9990915030241012\n",
      "Test accuracy: 0.4908717105263158\n",
      "Stapelgroesse (batchSize): 8\n",
      "Anzahl der Faltungsschichten: 4Layer\n",
      "Aktivierungsfunktion: relu\n",
      "Optimierungsfunktion: RMSprop\n",
      "Dropout-Rate Faltungsschicht 1: 0.2054401692759159\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 64\n",
      "Dropout-Rate Faltungsschicht 2: 0.6715296954099724\n",
      "Anzahl der Faltungsschichten: 4Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 128\n",
      "Dropout-Rate Faltungsschicht 3: 0.11478226426028586\n",
      "Anzahl der Filter-Maps Faltungsschicht 4: 128\n",
      "Dropout-Rate Faltungsschicht 4: 0.6160082475762262\n",
      "Anzahl der Neuronen des Fully Connected Layer: 64\n",
      "Dropout-Rate Fully Connected Layer: 0.23722270891400718\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "6084/6084 [==============================] - 164s 27ms/step - loss: 1.0693 - acc: 0.4380 - val_loss: 1.0084 - val_acc: 0.4857\n",
      "Epoch 2/10\n",
      "6084/6084 [==============================] - 159s 26ms/step - loss: 1.0428 - acc: 0.4610 - val_loss: 1.0630 - val_acc: 0.3880\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6084/6084 [==============================] - 159s 26ms/step - loss: 1.0482 - acc: 0.4575 - val_loss: 1.0619 - val_acc: 0.4248\n",
      "Epoch 4/10\n",
      "6084/6084 [==============================] - 159s 26ms/step - loss: 1.0515 - acc: 0.4591 - val_loss: 1.0160 - val_acc: 0.4598\n",
      "Epoch 5/10\n",
      "6084/6084 [==============================] - 157s 26ms/step - loss: 1.0519 - acc: 0.4571 - val_loss: 1.0409 - val_acc: 0.3927\n",
      "Epoch 6/10\n",
      "6084/6084 [==============================] - 159s 26ms/step - loss: 1.0565 - acc: 0.4505 - val_loss: 1.0585 - val_acc: 0.4151\n",
      "Epoch 7/10\n",
      "6084/6084 [==============================] - 160s 26ms/step - loss: 1.0721 - acc: 0.4382 - val_loss: 1.0193 - val_acc: 0.4819\n",
      "Epoch 8/10\n",
      "6084/6084 [==============================] - 157s 26ms/step - loss: 1.0766 - acc: 0.4362 - val_loss: 1.0561 - val_acc: 0.3813\n",
      "Epoch 9/10\n",
      "6084/6084 [==============================] - 159s 26ms/step - loss: 1.0700 - acc: 0.4369 - val_loss: 1.0254 - val_acc: 0.4828\n",
      "Epoch 10/10\n",
      "6084/6084 [==============================] - 158s 26ms/step - loss: 1.0760 - acc: 0.4386 - val_loss: 1.0370 - val_acc: 0.4201\n",
      "Test score: 1.0369844222680116\n",
      "Test accuracy: 0.42011834319526625\n",
      "Stapelgroesse (batchSize): 8\n",
      "Anzahl der Faltungsschichten: 4Layer\n",
      "Aktivierungsfunktion: relu\n",
      "Optimierungsfunktion: Adam\n",
      "Dropout-Rate Faltungsschicht 1: 0.5350208375977319\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 32\n",
      "Dropout-Rate Faltungsschicht 2: 0.07974928784802193\n",
      "Anzahl der Faltungsschichten: 4Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 128\n",
      "Dropout-Rate Faltungsschicht 3: 0.576047989074638\n",
      "Anzahl der Filter-Maps Faltungsschicht 4: 64\n",
      "Dropout-Rate Faltungsschicht 4: 0.3257886399132629\n",
      "Anzahl der Neuronen des Fully Connected Layer: 128\n",
      "Dropout-Rate Fully Connected Layer: 0.5671192786132916\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "6084/6084 [==============================] - 148s 24ms/step - loss: 1.0997 - acc: 0.3333 - val_loss: 1.0989 - val_acc: 0.3323\n",
      "Epoch 2/10\n",
      "6084/6084 [==============================] - 143s 24ms/step - loss: 1.0990 - acc: 0.3314 - val_loss: 1.0988 - val_acc: 0.3325\n",
      "Epoch 3/10\n",
      "6084/6084 [==============================] - 144s 24ms/step - loss: 1.0988 - acc: 0.3310 - val_loss: 1.0988 - val_acc: 0.3323\n",
      "Epoch 4/10\n",
      "6084/6084 [==============================] - 143s 24ms/step - loss: 1.0988 - acc: 0.3329 - val_loss: 1.0988 - val_acc: 0.3323\n",
      "Epoch 5/10\n",
      "6084/6084 [==============================] - 144s 24ms/step - loss: 1.0988 - acc: 0.3320 - val_loss: 1.0988 - val_acc: 0.3323\n",
      "Epoch 6/10\n",
      "6084/6084 [==============================] - 144s 24ms/step - loss: 1.0988 - acc: 0.3321 - val_loss: 1.0988 - val_acc: 0.3322\n",
      "Epoch 7/10\n",
      "6084/6084 [==============================] - 145s 24ms/step - loss: 1.0988 - acc: 0.3321 - val_loss: 1.0988 - val_acc: 0.3324\n",
      "Epoch 8/10\n",
      "6084/6084 [==============================] - 146s 24ms/step - loss: 1.0988 - acc: 0.3321 - val_loss: 1.0988 - val_acc: 0.3324\n",
      "Epoch 9/10\n",
      "6084/6084 [==============================] - 144s 24ms/step - loss: 1.0988 - acc: 0.3321 - val_loss: 1.0988 - val_acc: 0.3324\n",
      "Epoch 10/10\n",
      "6084/6084 [==============================] - 145s 24ms/step - loss: 1.0988 - acc: 0.3321 - val_loss: 1.0988 - val_acc: 0.3324\n",
      "Test score: 1.0988241953727527\n",
      "Test accuracy: 0.33226495726495725\n",
      "Stapelgroesse (batchSize): 32\n",
      "Anzahl der Faltungsschichten: 3Layer\n",
      "Aktivierungsfunktion: elu\n",
      "Optimierungsfunktion: Adam\n",
      "Dropout-Rate Faltungsschicht 1: 0.18858047055448576\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 32\n",
      "Dropout-Rate Faltungsschicht 2: 0.4476969990061666\n",
      "Anzahl der Faltungsschichten: 3Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 128\n",
      "Dropout-Rate Faltungsschicht 3: 0.6569615931883058\n",
      "Anzahl der Neuronen des Fully Connected Layer: 128\n",
      "Dropout-Rate Fully Connected Layer: 0.1546339895111094\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "1521/1521 [==============================] - 105s 69ms/step - loss: 10.7296 - acc: 0.3337 - val_loss: 10.7617 - val_acc: 0.3323\n",
      "Epoch 2/10\n",
      "1521/1521 [==============================] - 99s 65ms/step - loss: 10.7355 - acc: 0.3339 - val_loss: 10.7587 - val_acc: 0.3325\n",
      "Epoch 3/10\n",
      "1521/1521 [==============================] - 99s 65ms/step - loss: 10.7362 - acc: 0.3339 - val_loss: 10.7587 - val_acc: 0.3325\n",
      "Epoch 4/10\n",
      "1521/1521 [==============================] - 99s 65ms/step - loss: 10.7372 - acc: 0.3338 - val_loss: 10.7587 - val_acc: 0.3325\n",
      "Epoch 5/10\n",
      "1521/1521 [==============================] - 100s 66ms/step - loss: 10.7349 - acc: 0.3340 - val_loss: 10.7587 - val_acc: 0.3325\n",
      "Epoch 6/10\n",
      "1521/1521 [==============================] - 99s 65ms/step - loss: 10.7355 - acc: 0.3339 - val_loss: 10.7587 - val_acc: 0.3325\n",
      "Epoch 7/10\n",
      "1521/1521 [==============================] - 99s 65ms/step - loss: 10.7375 - acc: 0.3338 - val_loss: 10.7587 - val_acc: 0.3325\n",
      "Epoch 8/10\n",
      "1521/1521 [==============================] - 99s 65ms/step - loss: 10.7355 - acc: 0.3339 - val_loss: 10.7587 - val_acc: 0.3325\n",
      "Epoch 9/10\n",
      "1521/1521 [==============================] - 100s 66ms/step - loss: 10.7359 - acc: 0.3339 - val_loss: 10.7587 - val_acc: 0.3325\n",
      "Epoch 10/10\n",
      "1521/1521 [==============================] - 98s 65ms/step - loss: 10.7359 - acc: 0.3339 - val_loss: 10.7587 - val_acc: 0.3325\n",
      "Test score: 10.76174474264446\n",
      "Test accuracy: 0.3323190789473684\n",
      "Stapelgroesse (batchSize): 32\n",
      "Anzahl der Faltungsschichten: 3Layer\n",
      "Aktivierungsfunktion: relu\n",
      "Optimierungsfunktion: Adam\n",
      "Dropout-Rate Faltungsschicht 1: 0.5613716767868046\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 32\n",
      "Dropout-Rate Faltungsschicht 2: 0.5852087626516953\n",
      "Anzahl der Faltungsschichten: 3Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 128\n",
      "Dropout-Rate Faltungsschicht 3: 0.2755490740591204\n",
      "Anzahl der Neuronen des Fully Connected Layer: 64\n",
      "Dropout-Rate Fully Connected Layer: 0.6158482815230839\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "1521/1521 [==============================] - 98s 65ms/step - loss: 1.1127 - acc: 0.3419 - val_loss: 1.0988 - val_acc: 0.3323\n",
      "Epoch 2/10\n",
      "1521/1521 [==============================] - 93s 61ms/step - loss: 1.0945 - acc: 0.3480 - val_loss: 1.0912 - val_acc: 0.3521\n",
      "Epoch 3/10\n",
      "1521/1521 [==============================] - 92s 61ms/step - loss: 1.0539 - acc: 0.4249 - val_loss: 1.0541 - val_acc: 0.4114\n",
      "Epoch 4/10\n",
      "1521/1521 [==============================] - 94s 62ms/step - loss: 1.0338 - acc: 0.4536 - val_loss: 1.0433 - val_acc: 0.4053\n",
      "Epoch 5/10\n",
      "1521/1521 [==============================] - 91s 60ms/step - loss: 1.0258 - acc: 0.4641 - val_loss: 1.0305 - val_acc: 0.4759\n",
      "Epoch 6/10\n",
      "1521/1521 [==============================] - 92s 61ms/step - loss: 1.0112 - acc: 0.4738 - val_loss: 1.0174 - val_acc: 0.4816\n",
      "Epoch 7/10\n",
      "1521/1521 [==============================] - 93s 61ms/step - loss: 1.0035 - acc: 0.4861 - val_loss: 1.0299 - val_acc: 0.4360\n",
      "Epoch 8/10\n",
      "1521/1521 [==============================] - 91s 60ms/step - loss: 0.9954 - acc: 0.4913 - val_loss: 1.0233 - val_acc: 0.4491\n",
      "Epoch 9/10\n",
      "1521/1521 [==============================] - 92s 60ms/step - loss: 0.9898 - acc: 0.4975 - val_loss: 1.0372 - val_acc: 0.4639\n",
      "Epoch 10/10\n",
      "1521/1521 [==============================] - 92s 60ms/step - loss: 0.9870 - acc: 0.5009 - val_loss: 1.0391 - val_acc: 0.4509\n",
      "Test score: 1.0391624340885564\n",
      "Test accuracy: 0.4509046052631579\n",
      "Stapelgroesse (batchSize): 8\n",
      "Anzahl der Faltungsschichten: 3Layer\n",
      "Aktivierungsfunktion: relu\n",
      "Optimierungsfunktion: Adam\n",
      "Dropout-Rate Faltungsschicht 1: 0.21758720280497001\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 32\n",
      "Dropout-Rate Faltungsschicht 2: 0.3363371996167399\n",
      "Anzahl der Faltungsschichten: 3Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 64\n",
      "Dropout-Rate Faltungsschicht 3: 0.3304329619875355\n",
      "Anzahl der Neuronen des Fully Connected Layer: 128\n",
      "Dropout-Rate Fully Connected Layer: 0.05575408127472528\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "6084/6084 [==============================] - 151s 25ms/step - loss: 1.0790 - acc: 0.3753 - val_loss: 1.0250 - val_acc: 0.4648\n",
      "Epoch 2/10\n",
      "6084/6084 [==============================] - 147s 24ms/step - loss: 1.0086 - acc: 0.4694 - val_loss: 1.0140 - val_acc: 0.4638\n",
      "Epoch 3/10\n",
      "6084/6084 [==============================] - 146s 24ms/step - loss: 0.9897 - acc: 0.4825 - val_loss: 0.9855 - val_acc: 0.4969\n",
      "Epoch 4/10\n",
      "6084/6084 [==============================] - 146s 24ms/step - loss: 0.9772 - acc: 0.4922 - val_loss: 1.0020 - val_acc: 0.4974\n",
      "Epoch 5/10\n",
      "6084/6084 [==============================] - 147s 24ms/step - loss: 0.9641 - acc: 0.4991 - val_loss: 1.0066 - val_acc: 0.4927\n",
      "Epoch 6/10\n",
      "6084/6084 [==============================] - 146s 24ms/step - loss: 0.9555 - acc: 0.5119 - val_loss: 0.9894 - val_acc: 0.5003\n",
      "Epoch 7/10\n",
      "6084/6084 [==============================] - 146s 24ms/step - loss: 0.9469 - acc: 0.5164 - val_loss: 0.9961 - val_acc: 0.4854\n",
      "Epoch 8/10\n",
      "6084/6084 [==============================] - 145s 24ms/step - loss: 0.9440 - acc: 0.5192 - val_loss: 0.9678 - val_acc: 0.5283\n",
      "Epoch 9/10\n",
      "6084/6084 [==============================] - 145s 24ms/step - loss: 0.9343 - acc: 0.5263 - val_loss: 0.9879 - val_acc: 0.4996\n",
      "Epoch 10/10\n",
      "6084/6084 [==============================] - 144s 24ms/step - loss: 0.9203 - acc: 0.5365 - val_loss: 0.9888 - val_acc: 0.4946\n",
      "Test score: 0.9886837568743929\n",
      "Test accuracy: 0.4944937541091387\n",
      "Stapelgroesse (batchSize): 16\n",
      "Anzahl der Faltungsschichten: 5Layer\n",
      "Aktivierungsfunktion: relu\n",
      "Optimierungsfunktion: Adam\n",
      "Dropout-Rate Faltungsschicht 1: 0.3174428272991308\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 32\n",
      "Dropout-Rate Faltungsschicht 2: 0.621925032040878\n",
      "Anzahl der Faltungsschichten: 5Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 128\n",
      "Dropout-Rate Faltungsschicht 3: 0.5751935015974993\n",
      "Anzahl der Filter-Maps Faltungsschicht 4: 128\n",
      "Dropout-Rate Faltungsschicht 4: 0.4478389595207329\n",
      "Anzahl der Filter-Maps Faltungsschicht 5: 128\n",
      "Dropout-Rate Faltungsschicht 5: 0.35642409048822615\n",
      "Anzahl der Neuronen des Fully Connected Layer: 64\n",
      "Dropout-Rate Fully Connected Layer: 0.12907281413447744\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "3042/3042 [==============================] - 123s 40ms/step - loss: 1.1000 - acc: 0.3365 - val_loss: 1.0987 - val_acc: 0.3341\n",
      "Epoch 2/10\n",
      "3042/3042 [==============================] - 115s 38ms/step - loss: 1.0988 - acc: 0.3341 - val_loss: 1.0988 - val_acc: 0.3325\n",
      "Epoch 3/10\n",
      "3042/3042 [==============================] - 115s 38ms/step - loss: 1.0988 - acc: 0.3330 - val_loss: 1.0988 - val_acc: 0.3325\n",
      "Epoch 4/10\n",
      "3042/3042 [==============================] - 114s 38ms/step - loss: 1.0987 - acc: 0.3337 - val_loss: 1.0988 - val_acc: 0.3325\n",
      "Epoch 5/10\n",
      "3042/3042 [==============================] - 114s 37ms/step - loss: 1.0988 - acc: 0.3350 - val_loss: 1.0988 - val_acc: 0.3325\n",
      "Epoch 6/10\n",
      "3042/3042 [==============================] - 114s 37ms/step - loss: 1.0989 - acc: 0.3336 - val_loss: 1.0988 - val_acc: 0.3325\n",
      "Epoch 7/10\n",
      "3042/3042 [==============================] - 114s 37ms/step - loss: 1.0987 - acc: 0.3330 - val_loss: 1.0988 - val_acc: 0.3325\n",
      "Epoch 8/10\n",
      "3042/3042 [==============================] - 113s 37ms/step - loss: 1.0987 - acc: 0.3340 - val_loss: 1.0988 - val_acc: 0.3325\n",
      "Epoch 9/10\n",
      "3042/3042 [==============================] - 113s 37ms/step - loss: 1.0987 - acc: 0.3341 - val_loss: 1.0987 - val_acc: 0.3325\n",
      "Epoch 10/10\n",
      "3042/3042 [==============================] - 114s 37ms/step - loss: 1.0987 - acc: 0.3338 - val_loss: 1.0987 - val_acc: 0.3325\n",
      "Test score: 1.0987519562244414\n",
      "Test accuracy: 0.3323190789473684\n",
      "Stapelgroesse (batchSize): 8\n",
      "Anzahl der Faltungsschichten: 5Layer\n",
      "Aktivierungsfunktion: elu\n",
      "Optimierungsfunktion: Adam\n",
      "Dropout-Rate Faltungsschicht 1: 0.21113199297716778\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 64\n",
      "Dropout-Rate Faltungsschicht 2: 0.6081229344054953\n",
      "Anzahl der Faltungsschichten: 5Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 128\n",
      "Dropout-Rate Faltungsschicht 3: 0.5069408523662012\n",
      "Anzahl der Filter-Maps Faltungsschicht 4: 128\n",
      "Dropout-Rate Faltungsschicht 4: 0.6087434813719508\n",
      "Anzahl der Filter-Maps Faltungsschicht 5: 128\n",
      "Dropout-Rate Faltungsschicht 5: 0.6174763542737483\n",
      "Anzahl der Neuronen des Fully Connected Layer: 64\n",
      "Dropout-Rate Fully Connected Layer: 0.013953199559087713\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "6084/6084 [==============================] - 173s 28ms/step - loss: 1.1179 - acc: 0.3381 - val_loss: 1.1023 - val_acc: 0.3323\n",
      "Epoch 2/10\n",
      "6084/6084 [==============================] - 167s 27ms/step - loss: 1.1123 - acc: 0.3269 - val_loss: 1.1028 - val_acc: 0.3323\n",
      "Epoch 3/10\n",
      "6084/6084 [==============================] - 167s 27ms/step - loss: 1.1101 - acc: 0.3287 - val_loss: 1.1037 - val_acc: 0.3323\n",
      "Epoch 4/10\n",
      "6084/6084 [==============================] - 167s 27ms/step - loss: 1.1093 - acc: 0.3274 - val_loss: 1.1042 - val_acc: 0.3323\n",
      "Epoch 5/10\n",
      "6084/6084 [==============================] - 170s 28ms/step - loss: 1.1092 - acc: 0.3284 - val_loss: 1.1051 - val_acc: 0.3323\n",
      "Epoch 6/10\n",
      "6084/6084 [==============================] - 217s 36ms/step - loss: 1.1083 - acc: 0.3281 - val_loss: 1.1052 - val_acc: 0.3323\n",
      "Epoch 7/10\n",
      "6084/6084 [==============================] - 295s 48ms/step - loss: 1.1078 - acc: 0.3278 - val_loss: 1.1052 - val_acc: 0.3323\n",
      "Epoch 8/10\n",
      "6084/6084 [==============================] - 208s 34ms/step - loss: 1.1076 - acc: 0.3274 - val_loss: 1.1054 - val_acc: 0.3323\n",
      "Epoch 9/10\n",
      "6084/6084 [==============================] - 168s 28ms/step - loss: 1.1071 - acc: 0.3284 - val_loss: 1.1054 - val_acc: 0.3322\n",
      "Epoch 10/10\n",
      "6084/6084 [==============================] - 167s 27ms/step - loss: 1.1069 - acc: 0.3293 - val_loss: 1.1046 - val_acc: 0.3324\n",
      "Test score: 1.1046811865788397\n",
      "Test accuracy: 0.33226495726495725\n",
      "Stapelgroesse (batchSize): 16\n",
      "Anzahl der Faltungsschichten: 3Layer\n",
      "Aktivierungsfunktion: elu\n",
      "Optimierungsfunktion: RMSprop\n",
      "Dropout-Rate Faltungsschicht 1: 0.3829922841779661\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 32\n",
      "Dropout-Rate Faltungsschicht 2: 0.49783688567670176\n",
      "Anzahl der Faltungsschichten: 3Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 64\n",
      "Dropout-Rate Faltungsschicht 3: 0.022374882170599006\n",
      "Anzahl der Neuronen des Fully Connected Layer: 64\n",
      "Dropout-Rate Fully Connected Layer: 0.0505294469449968\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "3042/3042 [==============================] - 107s 35ms/step - loss: 10.7367 - acc: 0.3334 - val_loss: 10.7419 - val_acc: 0.3336\n",
      "Epoch 2/10\n",
      "3042/3042 [==============================] - 101s 33ms/step - loss: 10.7498 - acc: 0.3331 - val_loss: 10.7432 - val_acc: 0.3335\n",
      "Epoch 3/10\n",
      "3042/3042 [==============================] - 102s 33ms/step - loss: 10.7498 - acc: 0.3331 - val_loss: 10.7432 - val_acc: 0.3335\n",
      "Epoch 4/10\n",
      "3042/3042 [==============================] - 101s 33ms/step - loss: 10.7498 - acc: 0.3331 - val_loss: 10.7432 - val_acc: 0.3335\n",
      "Epoch 5/10\n",
      "3042/3042 [==============================] - 101s 33ms/step - loss: 10.7492 - acc: 0.3331 - val_loss: 10.7432 - val_acc: 0.3335\n",
      "Epoch 6/10\n",
      "3042/3042 [==============================] - 101s 33ms/step - loss: 10.7498 - acc: 0.3331 - val_loss: 10.7432 - val_acc: 0.3335\n",
      "Epoch 7/10\n",
      "3042/3042 [==============================] - 101s 33ms/step - loss: 10.7485 - acc: 0.3331 - val_loss: 10.7432 - val_acc: 0.3335\n",
      "Epoch 8/10\n",
      "3042/3042 [==============================] - 101s 33ms/step - loss: 10.7498 - acc: 0.3331 - val_loss: 10.7432 - val_acc: 0.3335\n",
      "Epoch 9/10\n",
      "3042/3042 [==============================] - 102s 33ms/step - loss: 10.7502 - acc: 0.3330 - val_loss: 10.7432 - val_acc: 0.3335\n",
      "Epoch 10/10\n",
      "3042/3042 [==============================] - 101s 33ms/step - loss: 10.7495 - acc: 0.3331 - val_loss: 10.7432 - val_acc: 0.3335\n",
      "Test score: 10.741862261295319\n",
      "Test accuracy: 0.3335526315789474\n",
      "Stapelgroesse (batchSize): 16\n",
      "Anzahl der Faltungsschichten: 5Layer\n",
      "Aktivierungsfunktion: elu\n",
      "Optimierungsfunktion: Adam\n",
      "Dropout-Rate Faltungsschicht 1: 0.22962711046270404\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 32\n",
      "Dropout-Rate Faltungsschicht 2: 0.5245955294997489\n",
      "Anzahl der Faltungsschichten: 5Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 128\n",
      "Dropout-Rate Faltungsschicht 3: 0.6720840701148673\n",
      "Anzahl der Filter-Maps Faltungsschicht 4: 64\n",
      "Dropout-Rate Faltungsschicht 4: 0.42616405256026657\n",
      "Anzahl der Filter-Maps Faltungsschicht 5: 128\n",
      "Dropout-Rate Faltungsschicht 5: 0.2782009184195105\n",
      "Anzahl der Neuronen des Fully Connected Layer: 64\n",
      "Dropout-Rate Fully Connected Layer: 0.3792584060542794\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3042/3042 [==============================] - 120s 39ms/step - loss: 1.0582 - acc: 0.4335 - val_loss: 0.9967 - val_acc: 0.4745\n",
      "Epoch 2/10\n",
      "3042/3042 [==============================] - 114s 37ms/step - loss: 1.0199 - acc: 0.4590 - val_loss: 0.9937 - val_acc: 0.4308\n",
      "Epoch 3/10\n",
      "3042/3042 [==============================] - 114s 37ms/step - loss: 1.0158 - acc: 0.4647 - val_loss: 0.9728 - val_acc: 0.4967\n",
      "Epoch 4/10\n",
      "3042/3042 [==============================] - 114s 37ms/step - loss: 0.9945 - acc: 0.4840 - val_loss: 0.9878 - val_acc: 0.4353\n",
      "Epoch 5/10\n",
      "3042/3042 [==============================] - 114s 37ms/step - loss: 0.9746 - acc: 0.5013 - val_loss: 0.9592 - val_acc: 0.5092\n",
      "Epoch 6/10\n",
      "3042/3042 [==============================] - 114s 37ms/step - loss: 0.9545 - acc: 0.5160 - val_loss: 0.9034 - val_acc: 0.5360\n",
      "Epoch 7/10\n",
      "3042/3042 [==============================] - 114s 37ms/step - loss: 0.9459 - acc: 0.5216 - val_loss: 0.9413 - val_acc: 0.5002\n",
      "Epoch 8/10\n",
      "3042/3042 [==============================] - 114s 37ms/step - loss: 0.9368 - acc: 0.5236 - val_loss: 0.8938 - val_acc: 0.5376\n",
      "Epoch 9/10\n",
      "3042/3042 [==============================] - 114s 37ms/step - loss: 0.9315 - acc: 0.5301 - val_loss: 0.8843 - val_acc: 0.5608\n",
      "Epoch 10/10\n",
      "3042/3042 [==============================] - 114s 37ms/step - loss: 0.9239 - acc: 0.5402 - val_loss: 0.8819 - val_acc: 0.5641\n",
      "Test score: 0.8821613910951113\n",
      "Test accuracy: 0.5640625\n",
      "Stapelgroesse (batchSize): 16\n",
      "Anzahl der Faltungsschichten: 3Layer\n",
      "Aktivierungsfunktion: elu\n",
      "Optimierungsfunktion: Adam\n",
      "Dropout-Rate Faltungsschicht 1: 0.15370988223089438\n",
      "Anzahl der Filter-Maps Faltungsschicht 2: 64\n",
      "Dropout-Rate Faltungsschicht 2: 0.6827003437767334\n",
      "Anzahl der Faltungsschichten: 3Layer\n",
      "Anzahl der Filter-Maps Faltungsschicht 3: 64\n",
      "Dropout-Rate Faltungsschicht 3: 0.05832367795781866\n",
      "Anzahl der Neuronen des Fully Connected Layer: 128\n",
      "Dropout-Rate Fully Connected Layer: 0.014396755468086919\n",
      "Faltungsnetz wird trainiert...\n",
      "Epoch 1/10\n",
      "3042/3042 [==============================] - 126s 42ms/step - loss: 1.1318 - acc: 0.3383 - val_loss: 1.1140 - val_acc: 0.3323\n",
      "Epoch 2/10\n",
      "3042/3042 [==============================] - 120s 40ms/step - loss: 1.1155 - acc: 0.3316 - val_loss: 1.1089 - val_acc: 0.3325\n",
      "Epoch 3/10\n",
      "3042/3042 [==============================] - 120s 40ms/step - loss: 1.1142 - acc: 0.3324 - val_loss: 1.1090 - val_acc: 0.3325\n",
      "Epoch 4/10\n",
      "2363/3042 [======================>.......] - ETA: 23s - loss: 1.1141 - acc: 0.3294"
     ]
    }
   ],
   "source": [
    "# Die Hyperas Methode optim sucht im Suchraum die Parameter \n",
    "# Bei einer Änderung des Methodenrumpf der Methode model() muss der Notebook Kernel neu gestartet werden\n",
    "bestRun, bestModel = optim.minimize(model=model,               \n",
    "                                          data=data,\n",
    "                                          algo=rand.suggest,   # Algorithmus: Random Search\n",
    "                                          max_evals=100,          \n",
    "                                          trials=Trials(),      # eine Liste von Verzeichnissen, die alles über die Suche enthalten.\n",
    "                                          notebook_name='CNN_experiment5')  # Der Name des Notebooks sollte als String angegeben werden\n",
    "print(bestRun)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Versuch 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mischen der Daten\n",
    "xShuffle, yShuffle = shuffle(xShuffle,yShuffle, random_state=42)\n",
    "# Testdaten zuweisen\n",
    "xShuffle, xTest, yShuffle, yTest = train_test_split(xShuffle, yShuffle, test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18090"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Anzahl der Werte einer Klasse berechnen\n",
    "countValues = np.count_nonzero(np.argmax(yShuffle, axis=1) == 1)\n",
    "countValues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Die Zelle Normiert die Anzahl der Repräsentanten pro Klasse\n",
    "# Dabei werden die Klassen normiert auf die Anzahl der hälfte der bestimmten Klasse (countValues) \n",
    "class1Number = 0\n",
    "class2Number = 0 \n",
    "class3Number = 0\n",
    "maxClasses = countValues / 2\n",
    "indexToDelete = [] \n",
    "i = -1\n",
    "for label in yShuffle:\n",
    "    i = i + 1\n",
    "    labelNumber = np.argmax(label,axis=0)\n",
    "    if labelNumber == 0 and class1Number < maxClasses:\n",
    "        class1Number = class1Number + 1\n",
    "        continue\n",
    "    elif labelNumber == 0:\n",
    "        indexToDelete.append(i)\n",
    "    #if labelNumber == 1 and class2Number < maxClasses:\n",
    "    #    class2Number = class2Number + 1\n",
    "    #    continue\n",
    "    #elif labelNumber == 1:\n",
    "    #    indexToDelete.append(i)     \n",
    "    if labelNumber == 2 and class3Number < maxClasses:\n",
    "        class3Number = class3Number + 1\n",
    "        continue\n",
    "    elif labelNumber == 2:\n",
    "        indexToDelete.append(i)\n",
    "xShuffle = [i for j, i in enumerate(xShuffle) if j not in indexToDelete]\n",
    "yShuffle = [i for j, i in enumerate(yShuffle) if j not in indexToDelete]\n",
    "yShuffle = np.asarray(yShuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 18090, 1: 18090})"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collections.Counter(np.argmax(yShuffle, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36180, 3)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yShuffle.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "xShuffle, yShuffle = shuffle(xShuffle,yShuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  1.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       ..., \n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yShuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for label in yShuffle:\n",
    "    labelNumber = np.argmax(label,axis=0)\n",
    "    if labelNumber == 2:\n",
    "        yShuffle[i] = [ 0.,  1.,  0.]\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\morro\\AppData\\Local\\conda\\conda\\envs\\bachelor\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: in the future out of bounds indices will raise an error instead of being ignored by `numpy.delete`.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "yShuffle = np.delete(yShuffle, [2,yShuffle.shape[0]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aufteilung in Trainings und Validationdaten \n",
    "xTrain, xVal, yTrain, yVal = train_test_split(xShuffle, yShuffle, test_size=0.20) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28944, 2)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yTrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter für das CNN\n",
    "inputShape     = (368, 70, 3)   # Eingangs Array-Form \n",
    "numNeuronsC1   = 32                # Anzahl der Filter / 1 Faltungsschicht\n",
    "numNeuronsC2   = 32                # Anzahl der Filter / 2 Faltungsschicht\n",
    "numNeuronsC3   = 64                # Anzahl der Filter / 3 Faltungsschicht\n",
    "numNeuronsC4   = 64\n",
    "numNeuronsC5   = 64\n",
    "numNeuronsD1   = 64                # Anzahl der Neuronen des Fully connected layer - vollverbundene Schicht\n",
    "poolSize       = 2                 # Größe der Pooling-Layer\n",
    "convKernelSize = 3                 # Größe des Faltungskern n*n\n",
    "batchSize      = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "902/902 [==============================] - 220s 244ms/step - loss: 0.6622 - acc: 0.6073 - val_loss: 0.6214 - val_acc: 0.6418\n",
      "Epoch 2/100\n",
      "902/902 [==============================] - 106s 118ms/step - loss: 0.6289 - acc: 0.6483 - val_loss: 0.6253 - val_acc: 0.6820 - loss: 0.6289 - acc: - ETA: 8s - loss: 0.6290  - ETA: 7s - loss: 0.6 - ETA: 1s - loss: 0.6284 -  - ETA: 0s - loss: 0.6288 - acc:\n",
      "Epoch 3/100\n",
      "902/902 [==============================] - 108s 120ms/step - loss: 0.6018 - acc: 0.6815 - val_loss: 0.5853 - val_acc: 0.7112\n",
      "Epoch 4/100\n",
      "902/902 [==============================] - 107s 118ms/step - loss: 0.5662 - acc: 0.7151 - val_loss: 0.5627 - val_acc: 0.70932s - lo\n",
      "Epoch 5/100\n",
      "902/902 [==============================] - 106s 117ms/step - loss: 0.5381 - acc: 0.7345 - val_loss: 0.5435 - val_acc: 0.72711s - loss: 0.537\n",
      "Epoch 6/100\n",
      "902/902 [==============================] - 107s 118ms/step - loss: 0.5231 - acc: 0.7462 - val_loss: 0.5277 - val_acc: 0.7496: 0.5223 - ac - ETA: 1s - loss: 0.5\n",
      "Epoch 7/100\n",
      "902/902 [==============================] - 109s 121ms/step - loss: 0.5122 - acc: 0.7521 - val_loss: 0.5207 - val_acc: 0.7496\n",
      "Epoch 8/100\n",
      "902/902 [==============================] - 105s 117ms/step - loss: 0.5026 - acc: 0.7569 - val_loss: 0.5193 - val_acc: 0.7503 - loss: 0.5011 - acc: 0. - ETA: 3s - loss: 0.5013  - ETA: 2s - l\n",
      "Epoch 9/100\n",
      "902/902 [==============================] - 108s 119ms/step - loss: 0.4976 - acc: 0.7615 - val_loss: 0.5250 - val_acc: 0.7356s - loss: 0.49 - ET - ETA: 4s  - ETA: 1s - loss: 0.4973 - ac - ETA: 0s - loss: 0.4969 - acc: 0.761 - ETA: 0s - loss: 0.4970 - acc\n",
      "Epoch 10/100\n",
      "902/902 [==============================] - 107s 119ms/step - loss: 0.4927 - acc: 0.7658 - val_loss: 0.5098 - val_acc: 0.7538- ETA: 7s - loss: 0.4916  - ETA: 2s - \n",
      "Epoch 11/100\n",
      "902/902 [==============================] - 109s 121ms/step - loss: 0.4917 - acc: 0.7649 - val_loss: 0.5053 - val_acc: 0.750210s\n",
      "Epoch 12/100\n",
      "902/902 [==============================] - 107s 118ms/step - loss: 0.4867 - acc: 0.7686 - val_loss: 0.5101 - val_acc: 0.7506s - l - ETA: 9s - loss: 0.4855 - acc: 0 - ETA: 9s -  - ETA: 2s - loss: 0.4 - ETA: 1s - loss: 0.4867 - \n",
      "Epoch 13/100\n",
      "902/902 [==============================] - 106s 118ms/step - loss: 0.4862 - acc: 0.7692 - val_loss: 0.5105 - val_acc: 0.7556 - ETA: 6s - loss: 0.4857 - acc: 0. - ETA: 6s - los - ETA: 3s - loss: 0.4857 - acc: 0.769 - ETA: 3s - loss: 0. - ETA: 1s - loss: 0\n",
      "Epoch 14/100\n",
      "902/902 [==============================] - 107s 118ms/step - loss: 0.4795 - acc: 0.7712 - val_loss: 0.5026 - val_acc: 0.7545: 0.4790 - - ETA: 10s - loss: 0.4789 - acc: 0.77 - E - ETA: 3s  - ETA: 0s - loss: 0.4795 - acc\n",
      "Epoch 15/100\n",
      "902/902 [==============================] - 106s 118ms/step - loss: 0.4790 - acc: 0.7735 - val_loss: 0.5084 - val_acc: 0.7602\n",
      "Epoch 16/100\n",
      "902/902 [==============================] - 106s 117ms/step - loss: 0.4729 - acc: 0.7759 - val_loss: 0.5033 - val_acc: 0.7653\n",
      "Epoch 17/100\n",
      "902/902 [==============================] - 107s 118ms/step - loss: 0.4703 - acc: 0.7774 - val_loss: 0.5042 - val_acc: 0.7601\n",
      "Epoch 18/100\n",
      "902/902 [==============================] - 111s 123ms/step - loss: 0.4716 - acc: 0.7768 - val_loss: 0.5194 - val_acc: 0.7642\n",
      "Epoch 19/100\n",
      "902/902 [==============================] - 111s 123ms/step - loss: 0.4696 - acc: 0.7756 - val_loss: 0.5052 - val_acc: 0.7638\n",
      "Epoch 20/100\n",
      "902/902 [==============================] - 108s 119ms/step - loss: 0.4681 - acc: 0.7790 - val_loss: 0.4968 - val_acc: 0.7653 0.4676 - acc: 0.7 - ETA: 1s - loss: 0.4680\n",
      "Epoch 21/100\n",
      "902/902 [==============================] - 108s 120ms/step - loss: 0.4688 - acc: 0.7782 - val_loss: 0.5010 - val_acc: 0.7588\n",
      "Epoch 22/100\n",
      "902/902 [==============================] - 108s 120ms/step - loss: 0.4643 - acc: 0.7784 - val_loss: 0.5072 - val_acc: 0.7606s: 0.4632 - a - ETA: 2s - loss: 0.4637 - acc: 0.778 - ETA: 2s - loss: 0.4636 - acc: 0.778 - ETA: 2s - loss:\n",
      "Epoch 23/100\n",
      "902/902 [==============================] - 108s 119ms/step - loss: 0.4648 - acc: 0.7801 - val_loss: 0.5075 - val_acc: 0.76060.4645 - acc: 0.780 - ETA: 1s - loss: 0.4646 \n",
      "Epoch 24/100\n",
      "902/902 [==============================] - 105s 116ms/step - loss: 0.4613 - acc: 0.7826 - val_loss: 0.5095 - val_acc: 0.76484s - loss: 0.4608 - acc: 0 \n",
      "Epoch 25/100\n",
      "902/902 [==============================] - 105s 116ms/step - loss: 0.4625 - acc: 0.7781 - val_loss: 0.5465 - val_acc: 0.7491\n",
      "Epoch 26/100\n",
      "902/902 [==============================] - 105s 116ms/step - loss: 0.4608 - acc: 0.7806 - val_loss: 0.5143 - val_acc: 0.7603\n",
      "Epoch 27/100\n",
      "902/902 [==============================] - 105s 116ms/step - loss: 0.4545 - acc: 0.7813 - val_loss: 0.5270 - val_acc: 0.7550\n",
      "Epoch 28/100\n",
      "902/902 [==============================] - 105s 116ms/step - loss: 0.4575 - acc: 0.7806 - val_loss: 0.5047 - val_acc: 0.7607\n",
      "Epoch 29/100\n",
      "902/902 [==============================] - 105s 116ms/step - loss: 0.4572 - acc: 0.7787 - val_loss: 0.5352 - val_acc: 0.7587\n",
      "Epoch 30/100\n",
      "902/902 [==============================] - 105s 116ms/step - loss: 0.4494 - acc: 0.7841 - val_loss: 0.5264 - val_acc: 0.7357\n",
      "Epoch 31/100\n",
      "902/902 [==============================] - 105s 116ms/step - loss: 0.4518 - acc: 0.7846 - val_loss: 0.5192 - val_acc: 0.7569\n",
      "Epoch 32/100\n",
      "902/902 [==============================] - 105s 116ms/step - loss: 0.4499 - acc: 0.7870 - val_loss: 0.5401 - val_acc: 0.7486\n",
      "Epoch 33/100\n",
      "902/902 [==============================] - 105s 116ms/step - loss: 0.4526 - acc: 0.7820 - val_loss: 0.5088 - val_acc: 0.7601loss: 0.4518 - acc: \n",
      "Epoch 34/100\n",
      "902/902 [==============================] - 105s 116ms/step - loss: 0.4514 - acc: 0.7831 - val_loss: 0.5102 - val_acc: 0.7545\n",
      "Epoch 35/100\n",
      "902/902 [==============================] - 105s 116ms/step - loss: 0.4530 - acc: 0.7830 - val_loss: 0.5202 - val_acc: 0.7498\n",
      "Epoch 36/100\n",
      "902/902 [==============================] - 105s 116ms/step - loss: 0.4510 - acc: 0.7864 - val_loss: 0.5086 - val_acc: 0.7542\n",
      "Epoch 37/100\n",
      "902/902 [==============================] - 105s 116ms/step - loss: 0.4505 - acc: 0.7831 - val_loss: 0.5085 - val_acc: 0.7493\n",
      "Epoch 38/100\n",
      "902/902 [==============================] - 105s 116ms/step - loss: 0.4504 - acc: 0.7841 - val_loss: 0.5178 - val_acc: 0.7493\n",
      "Epoch 39/100\n",
      "902/902 [==============================] - 105s 116ms/step - loss: 0.4476 - acc: 0.7864 - val_loss: 0.5355 - val_acc: 0.7381\n",
      "Epoch 40/100\n",
      "902/902 [==============================] - 106s 117ms/step - loss: 0.4506 - acc: 0.7851 - val_loss: 0.5060 - val_acc: 0.7598\n",
      "Epoch 41/100\n",
      "902/902 [==============================] - 106s 117ms/step - loss: 0.4503 - acc: 0.7844 - val_loss: 0.5284 - val_acc: 0.7429\n",
      "Epoch 42/100\n",
      "902/902 [==============================] - 105s 117ms/step - loss: 0.4491 - acc: 0.7867 - val_loss: 0.5663 - val_acc: 0.7571\n",
      "Epoch 43/100\n",
      "902/902 [==============================] - 105s 117ms/step - loss: 0.4470 - acc: 0.7867 - val_loss: 0.5449 - val_acc: 0.7559\n",
      "Epoch 44/100\n",
      "902/902 [==============================] - 105s 117ms/step - loss: 0.4473 - acc: 0.7851 - val_loss: 0.5453 - val_acc: 0.7349s - ETA: 1s - loss: 0.44\n",
      "Epoch 45/100\n",
      "902/902 [==============================] - 119s 132ms/step - loss: 0.4470 - acc: 0.7865 - val_loss: 0.5359 - val_acc: 0.7539\n",
      "Epoch 46/100\n",
      "902/902 [==============================] - 113s 125ms/step - loss: 0.4471 - acc: 0.7847 - val_loss: 0.5999 - val_acc: 0.7428s - loss: 0.4454\n",
      "Epoch 47/100\n",
      "902/902 [==============================] - 112s 124ms/step - loss: 0.4481 - acc: 0.7863 - val_loss: 0.5491 - val_acc: 0.7631\n",
      "Epoch 48/100\n",
      "902/902 [==============================] - 113s 125ms/step - loss: 0.4470 - acc: 0.7880 - val_loss: 0.5284 - val_acc: 0.7537\n",
      "Epoch 49/100\n",
      "902/902 [==============================] - 117s 130ms/step - loss: 0.4456 - acc: 0.7878 - val_loss: 0.5227 - val_acc: 0.7598\n",
      "Epoch 50/100\n",
      "902/902 [==============================] - 113s 125ms/step - loss: 0.4407 - acc: 0.7866 - val_loss: 0.5452 - val_acc: 0.7559\n",
      "Epoch 51/100\n",
      "902/902 [==============================] - 120s 133ms/step - loss: 0.4454 - acc: 0.7867 - val_loss: 0.5115 - val_acc: 0.7563\n",
      "Epoch 52/100\n",
      "902/902 [==============================] - 117s 129ms/step - loss: 0.4435 - acc: 0.7907 - val_loss: 0.5222 - val_acc: 0.7613- lo\n",
      "Epoch 53/100\n",
      "902/902 [==============================] - 110s 122ms/step - loss: 0.4502 - acc: 0.7822 - val_loss: 0.5345 - val_acc: 0.7652\n",
      "Epoch 54/100\n",
      "902/902 [==============================] - 111s 123ms/step - loss: 0.4476 - acc: 0.7845 - val_loss: 0.5070 - val_acc: 0.7651\n",
      "Epoch 55/100\n",
      "902/902 [==============================] - 114s 126ms/step - loss: 0.4455 - acc: 0.7887 - val_loss: 0.5271 - val_acc: 0.7690\n",
      "Epoch 56/100\n",
      "902/902 [==============================] - 114s 127ms/step - loss: 0.4389 - acc: 0.7894 - val_loss: 0.6379 - val_acc: 0.7569\n",
      "Epoch 57/100\n",
      "902/902 [==============================] - 110s 122ms/step - loss: 0.4487 - acc: 0.7861 - val_loss: 0.5202 - val_acc: 0.7694\n",
      "Epoch 58/100\n",
      "902/902 [==============================] - 109s 121ms/step - loss: 0.4368 - acc: 0.7898 - val_loss: 0.5037 - val_acc: 0.7723\n",
      "Epoch 59/100\n",
      "902/902 [==============================] - 111s 123ms/step - loss: 0.4452 - acc: 0.7887 - val_loss: 0.5270 - val_acc: 0.7684\n",
      "Epoch 60/100\n",
      "902/902 [==============================] - 110s 122ms/step - loss: 0.4456 - acc: 0.7879 - val_loss: 0.5256 - val_acc: 0.7491\n",
      "Epoch 61/100\n",
      "902/902 [==============================] - 107s 118ms/step - loss: 0.4441 - acc: 0.7872 - val_loss: 0.5369 - val_acc: 0.7569\n",
      "Epoch 62/100\n",
      "902/902 [==============================] - 110s 122ms/step - loss: 0.4409 - acc: 0.7862 - val_loss: 0.5250 - val_acc: 0.7684\n",
      "Epoch 63/100\n",
      "902/902 [==============================] - 109s 121ms/step - loss: 0.4483 - acc: 0.7853 - val_loss: 0.4961 - val_acc: 0.76174s - loss: 0.4472 - a - ETA: \n",
      "Epoch 64/100\n",
      "902/902 [==============================] - 112s 124ms/step - loss: 0.4401 - acc: 0.7883 - val_loss: 0.5079 - val_acc: 0.7646TA: 5s - loss: 0.439 - E\n",
      "Epoch 65/100\n",
      "902/902 [==============================] - 110s 122ms/step - loss: 0.4423 - acc: 0.7900 - val_loss: 0.5275 - val_acc: 0.7638TA: 2s - l\n",
      "Epoch 66/100\n",
      "902/902 [==============================] - 113s 126ms/step - loss: 0.4441 - acc: 0.7885 - val_loss: 0.5279 - val_acc: 0.7699\n",
      "Epoch 67/100\n",
      "902/902 [==============================] - 110s 121ms/step - loss: 0.4456 - acc: 0.7895 - val_loss: 0.5579 - val_acc: 0.77442s - loss: 0.4459 - acc: 0. - ETA: 2s - loss: 0.4462 - ETA: 0s - loss: 0.4456 - acc:\n",
      "Epoch 68/100\n",
      "902/902 [==============================] - 114s 127ms/step - loss: 0.4402 - acc: 0.7850 - val_loss: 0.5233 - val_acc: 0.7664\n",
      "Epoch 69/100\n",
      "902/902 [==============================] - 110s 122ms/step - loss: 0.4432 - acc: 0.7902 - val_loss: 0.5162 - val_acc: 0.76565 \n",
      "Epoch 70/100\n",
      "902/902 [==============================] - 108s 120ms/step - loss: 0.4496 - acc: 0.7853 - val_loss: 0.5359 - val_acc: 0.7644\n",
      "Epoch 71/100\n",
      "902/902 [==============================] - 108s 120ms/step - loss: 0.4461 - acc: 0.7886 - val_loss: 0.5492 - val_acc: 0.7649\n",
      "Epoch 72/100\n",
      "902/902 [==============================] - 114s 126ms/step - loss: 0.4433 - acc: 0.7891 - val_loss: 0.5722 - val_acc: 0.7631\n",
      "Epoch 73/100\n",
      "902/902 [==============================] - 114s 126ms/step - loss: 0.4443 - acc: 0.7891 - val_loss: 0.5456 - val_acc: 0.7664TA: 4s - loss: 0.4433 - - ETA: 3s - loss: 0.4436 - acc: 0.7 - ETA: 2s - loss: 0.4437 - acc: 0.789 - ETA: 2s - los - ETA: 0s - loss: 0.4442 - acc: 0.78\n",
      "Epoch 74/100\n",
      "902/902 [==============================] - 113s 125ms/step - loss: 0.4485 - acc: 0.7855 - val_loss: 0.5199 - val_acc: 0.7690\n",
      "Epoch 75/100\n",
      "902/902 [==============================] - 112s 125ms/step - loss: 0.4409 - acc: 0.7888 - val_loss: 0.5790 - val_acc: 0.7587\n",
      "Epoch 76/100\n",
      "902/902 [==============================] - 111s 123ms/step - loss: 0.4466 - acc: 0.7894 - val_loss: 0.5202 - val_acc: 0.7671\n",
      "Epoch 77/100\n",
      "902/902 [==============================] - 111s 123ms/step - loss: 0.4435 - acc: 0.7853 - val_loss: 0.5181 - val_acc: 0.7710 - a\n",
      "Epoch 78/100\n",
      "902/902 [==============================] - 112s 124ms/step - loss: 0.4376 - acc: 0.7875 - val_loss: 0.5056 - val_acc: 0.7709\n",
      "Epoch 79/100\n",
      "902/902 [==============================] - 112s 124ms/step - loss: 0.4407 - acc: 0.7874 - val_loss: 0.5281 - val_acc: 0.7674\n",
      "Epoch 80/100\n",
      "902/902 [==============================] - 110s 122ms/step - loss: 0.4380 - acc: 0.7904 - val_loss: 0.5420 - val_acc: 0.7659\n",
      "Epoch 81/100\n",
      "902/902 [==============================] - 111s 123ms/step - loss: 0.4477 - acc: 0.7851 - val_loss: 0.5412 - val_acc: 0.76691 - a - ETA: 3s - loss: 0.4466 - acc - ETA: 2s - loss:\n",
      "Epoch 82/100\n",
      "902/902 [==============================] - 107s 119ms/step - loss: 0.4440 - acc: 0.7882 - val_loss: 0.5447 - val_acc: 0.7581\n",
      "Epoch 83/100\n",
      "902/902 [==============================] - 106s 118ms/step - loss: 0.4460 - acc: 0.7859 - val_loss: 0.5209 - val_acc: 0.7699\n",
      "Epoch 84/100\n",
      "902/902 [==============================] - 107s 118ms/step - loss: 0.4463 - acc: 0.7884 - val_loss: 0.4933 - val_acc: 0.7710\n",
      "Epoch 85/100\n",
      "902/902 [==============================] - 107s 118ms/step - loss: 0.4405 - acc: 0.7892 - val_loss: 0.5078 - val_acc: 0.7663\n",
      "Epoch 86/100\n",
      "902/902 [==============================] - 107s 118ms/step - loss: 0.4429 - acc: 0.7857 - val_loss: 0.5247 - val_acc: 0.7656\n",
      "Epoch 87/100\n",
      "902/902 [==============================] - 107s 119ms/step - loss: 0.4440 - acc: 0.7893 - val_loss: 0.5208 - val_acc: 0.7708\n",
      "Epoch 88/100\n",
      "902/902 [==============================] - 107s 118ms/step - loss: 0.4444 - acc: 0.7854 - val_loss: 0.5539 - val_acc: 0.7685\n",
      "Epoch 89/100\n",
      "902/902 [==============================] - 108s 120ms/step - loss: 0.4439 - acc: 0.7876 - val_loss: 0.5010 - val_acc: 0.7674\n",
      "Epoch 90/100\n",
      "902/902 [==============================] - 107s 119ms/step - loss: 0.4389 - acc: 0.7862 - val_loss: 0.4999 - val_acc: 0.7706\n",
      "Epoch 91/100\n",
      "902/902 [==============================] - 107s 119ms/step - loss: 0.4422 - acc: 0.7865 - val_loss: 0.5082 - val_acc: 0.7673\n",
      "Epoch 92/100\n",
      "902/902 [==============================] - 107s 119ms/step - loss: 0.4414 - acc: 0.7881 - val_loss: 0.5559 - val_acc: 0.7630 0.4405 - acc: \n",
      "Epoch 93/100\n",
      "902/902 [==============================] - 107s 119ms/step - loss: 0.4451 - acc: 0.7843 - val_loss: 0.5300 - val_acc: 0.7673\n",
      "Epoch 94/100\n",
      "902/902 [==============================] - 107s 118ms/step - loss: 0.4463 - acc: 0.7858 - val_loss: 0.5457 - val_acc: 0.7694\n",
      "Epoch 95/100\n",
      "902/902 [==============================] - 107s 118ms/step - loss: 0.4489 - acc: 0.7851 - val_loss: 0.5375 - val_acc: 0.7705\n",
      "Epoch 96/100\n",
      "902/902 [==============================] - 107s 119ms/step - loss: 0.4518 - acc: 0.7803 - val_loss: 0.4983 - val_acc: 0.7687\n",
      "Epoch 97/100\n",
      "902/902 [==============================] - 107s 119ms/step - loss: 0.4442 - acc: 0.7872 - val_loss: 0.5941 - val_acc: 0.73568s - loss:  - ET - ETA:\n",
      "Epoch 98/100\n",
      "902/902 [==============================] - 107s 119ms/step - loss: 0.4462 - acc: 0.7846 - val_loss: 0.5274 - val_acc: 0.7698\n",
      "Epoch 99/100\n",
      "902/902 [==============================] - 107s 118ms/step - loss: 0.4436 - acc: 0.7863 - val_loss: 0.5382 - val_acc: 0.7716\n",
      "Epoch 100/100\n",
      "902/902 [==============================] - 107s 119ms/step - loss: 0.4445 - acc: 0.7848 - val_loss: 0.5416 - val_acc: 0.7393\n"
     ]
    }
   ],
   "source": [
    "modelCNN = Sequential()\n",
    "modelCNN.add(Conv2D(numNeuronsC1, (convKernelSize, convKernelSize), padding='same', input_shape=inputShape,kernel_initializer=initializers.glorot_uniform(seed=42)))\n",
    "modelCNN.add(Activation(\"elu\"))\n",
    "modelCNN.add(MaxPooling2D(pool_size=(poolSize, poolSize)))\n",
    "modelCNN.add(Dropout(0.16))\n",
    "modelCNN.add(Conv2D(numNeuronsC2, (convKernelSize, convKernelSize), padding='same', kernel_initializer=initializers.glorot_uniform(seed=42)))\n",
    "modelCNN.add(Activation(\"elu\"))\n",
    "modelCNN.add(MaxPooling2D(pool_size=(poolSize, poolSize)))\n",
    "modelCNN.add(Dropout(0.21))\n",
    "modelCNN.add(Conv2D(numNeuronsC3, (convKernelSize, convKernelSize), padding='same', kernel_initializer=initializers.glorot_uniform(seed=42)))\n",
    "modelCNN.add(Activation(\"elu\"))\n",
    "modelCNN.add(MaxPooling2D(pool_size=(poolSize, poolSize)))       \n",
    "modelCNN.add(Dropout(0.44))\n",
    "modelCNN.add(Conv2D(numNeuronsC4, (convKernelSize, convKernelSize), padding='same', kernel_initializer=initializers.glorot_uniform(seed=42)))\n",
    "modelCNN.add(Activation(\"elu\"))\n",
    "modelCNN.add(MaxPooling2D(pool_size=(poolSize, poolSize)))       \n",
    "modelCNN.add(Dropout(0.13))\n",
    "modelCNN.add(Conv2D(numNeuronsC5, (convKernelSize, convKernelSize), padding='same', kernel_initializer=initializers.glorot_uniform(seed=42)))\n",
    "modelCNN.add(Activation(\"elu\"))\n",
    "modelCNN.add(MaxPooling2D(pool_size=(poolSize, poolSize)))       \n",
    "modelCNN.add(Dropout(0.40))\n",
    "modelCNN.add(Flatten())\n",
    "modelCNN.add(Dense(numNeuronsD1, kernel_initializer=initializers.glorot_uniform(seed=42)))\n",
    "modelCNN.add(Activation(\"elu\"))\n",
    "modelCNN.add(Dropout(0.07)) \n",
    "modelCNN.add(Dense(2, kernel_initializer=initializers.glorot_uniform(seed=42)))\n",
    "modelCNN.add(Activation('softmax'))\n",
    "            \n",
    "modelCNN.compile(loss='binary_crossentropy', optimizer=\"Adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "earlyStopping  = cb.EarlyStopping(monitor='val_acc', patience=100, verbose=1, mode='max')\n",
    "checkpointSafe = cb.ModelCheckpoint('ergebnisse_versuch5/modell_versuch5_' + experimentNumber, monitor='val_acc', save_best_only=True)   \n",
    "hist = modelCNN.fit_generator(dataLoader(xTrain, yTrain, batchSize), epochs=100, steps_per_epoch=(int(len(xTrain)/batchSize)),\n",
    "              validation_data=dataLoader(xVal, yVal, batchSize), validation_steps=(int(len(xVal)/batchSize)), callbacks=[earlyStopping,checkpointSafe])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.69201155362930977, 0.5165929203539823]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modell1.evaluate_generator( dataLoader(xVal, yVal, 32), steps=int(len(xVal)/32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Versuch 7: Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Bei diesen Test wurden die 3 Faltungsnetze spezialisiert auf jeweils eine Fahrqualität geladen.\n",
    "## Dann wurde mit den Testdaten die Accuracy berechnet\n",
    "modellGood = load_model('ergebnisse_versuch5/modell_versuch5_7_1')\n",
    "modellMean = load_model('ergebnisse_versuch5/modell_versuch5_7_2')\n",
    "modellBad = load_model('ergebnisse_versuch5/modell_versuch5_7_3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "validPreds = []\n",
    "imageList = []\n",
    "modellGoodOutput = 0\n",
    "modellMeanOutput = 0\n",
    "modellBadOutput = 0\n",
    "for path in xTest:\n",
    "    imageList = []   \n",
    "    img = cv2.cvtColor(cv2.imread(path),cv2.COLOR_BGR2RGB)\n",
    "    img = np.array(img)\n",
    "    img = img.astype('float32')\n",
    "    img /= 255\n",
    "    imageList.append(img)\n",
    "    modellGoodOutput = modellGood.predict(np.asarray(imageList))\n",
    "    modellMeanOutput = modellMean.predict(np.asarray(imageList))\n",
    "    modellBadOutput  = modellBad.predict(np.asarray(imageList))\n",
    "    validPreds.append([modellGoodOutput[0][0], modellMeanOutput[0][0], modellBadOutput[0][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.13602863, 0.48937768, 0.66923726],\n",
       " [2.6950711e-06, 0.50833988, 0.90332907],\n",
       " [0.64940959, 0.51275796, 7.5535419e-05],\n",
       " [0.97699183, 0.51438123, 0.00033945063],\n",
       " [0.64940959, 0.51706988, 0.03990639],\n",
       " [0.022441713, 0.51140642, 0.78822809],\n",
       " [0.64940953, 0.51397979, 0.0091521489],\n",
       " [4.1461612e-07, 0.50780535, 0.99386978],\n",
       " [0.69579732, 0.51136762, 0.041706208],\n",
       " [0.011734012, 0.51268035, 0.52508563],\n",
       " [0.6494115, 0.50982964, 0.028280733],\n",
       " [0.075321093, 0.51265097, 0.32577133],\n",
       " [0.00046232363, 0.51061571, 0.99262792],\n",
       " [0.00050078076, 0.51245743, 0.86260152],\n",
       " [0.64944738, 0.51222247, 0.040685695],\n",
       " [0.68508035, 0.51253617, 0.010512487],\n",
       " [0.90998572, 0.46874377, 8.9872929e-06],\n",
       " [8.3114566e-05, 0.49845371, 0.73557013],\n",
       " [0.038628273, 0.51135683, 0.69491905],\n",
       " [0.043561663, 0.51293844, 0.52393669],\n",
       " [0.039705556, 0.50881243, 0.58299786],\n",
       " [0.88829178, 0.51002657, 0.010652349],\n",
       " [0.19041653, 0.51527756, 0.1941251],\n",
       " [0.0014990552, 0.50460988, 0.64352167],\n",
       " [9.5338415e-05, 0.51151758, 0.72118717],\n",
       " [0.75283819, 0.51277822, 0.0070298864],\n",
       " [0.64940947, 0.51019913, 0.21125698],\n",
       " [0.98291826, 0.51122725, 3.7562111e-06],\n",
       " [0.64940953, 0.51028699, 0.0079971747],\n",
       " [0.68129677, 0.51922244, 0.0090005565],\n",
       " [0.87060434, 0.49140805, 7.4496173e-05],\n",
       " [0.30363736, 0.51396161, 0.265266],\n",
       " [0.0035611314, 0.51130211, 0.7139256],\n",
       " [0.68613464, 0.49759075, 0.076156735],\n",
       " [0.0025398349, 0.51715857, 0.52508563],\n",
       " [0.9838838, 0.45534062, 6.4276301e-10],\n",
       " [0.85886121, 0.51728857, 0.0051836455],\n",
       " [0.96393448, 0.49419546, 0.0046961606],\n",
       " [0.23687494, 0.51015377, 0.343784],\n",
       " [0.99229783, 0.51065606, 0.019945629],\n",
       " [0.89384753, 0.50999874, 0.025561234],\n",
       " [0.042141646, 0.50261545, 0.68960047],\n",
       " [0.70911479, 0.50138116, 0.0041039325],\n",
       " [0.4915337, 0.50557327, 0.050661311],\n",
       " [0.25225312, 0.51301575, 0.52508563],\n",
       " [0.74049795, 0.50749213, 0.053722315],\n",
       " [0.74263149, 0.49025801, 0.0001775275],\n",
       " [8.4425892e-08, 0.50592881, 0.98103571],\n",
       " [0.99931777, 0.51720232, 3.2233871e-05],\n",
       " [0.00010796036, 0.51195711, 0.79403365],\n",
       " [0.98821521, 0.51536846, 4.3134942e-06],\n",
       " [0.53021103, 0.5105496, 0.0015364052],\n",
       " [0.29120848, 0.5086953, 0.52508563],\n",
       " [0.99199414, 0.51339453, 0.017938063],\n",
       " [0.64941388, 0.5103938, 0.048585325],\n",
       " [8.4993197e-05, 0.49218902, 0.87343895],\n",
       " [0.051405653, 0.5122627, 0.53804433],\n",
       " [0.98288029, 0.50938129, 0.0019782768],\n",
       " [0.7616784, 0.50573301, 0.052930761],\n",
       " [0.35078356, 0.51405799, 0.52508563],\n",
       " [0.067429952, 0.52081591, 0.0084282747],\n",
       " [0.14212628, 0.50402385, 0.81814927],\n",
       " [0.87123394, 0.50586259, 0.013702128],\n",
       " [0.72504175, 0.50003523, 0.028566225],\n",
       " [0.93106663, 0.50386328, 0.020285249],\n",
       " [0.99525827, 0.48993748, 8.1720164e-06],\n",
       " [8.1155078e-05, 0.51224422, 0.82487684],\n",
       " [0.09275192, 0.51061094, 0.76714277],\n",
       " [0.64942056, 0.50833458, 0.0016646285],\n",
       " [0.85016191, 0.51202035, 0.00018588887],\n",
       " [0.64940953, 0.51073211, 0.034685273],\n",
       " [0.24312063, 0.5108403, 0.52508569],\n",
       " [0.84911996, 0.50840759, 0.056223914],\n",
       " [0.99986422, 0.51488, 5.9468931e-07],\n",
       " [0.97410369, 0.51552993, 5.5130296e-09],\n",
       " [0.0011356995, 0.50961816, 0.32926321],\n",
       " [3.8938761e-12, 0.50300497, 0.99770844],\n",
       " [0.64940953, 0.51708478, 0.030607479],\n",
       " [0.24903136, 0.50620121, 0.081567109],\n",
       " [0.5756287, 0.50749362, 0.1118158],\n",
       " [0.69058162, 0.49121922, 0.015217884],\n",
       " [0.91204196, 0.51362735, 0.02866138],\n",
       " [0.97860378, 0.51576161, 3.8983725e-08],\n",
       " [0.9817909, 0.51768488, 0.00040962602],\n",
       " [0.086498789, 0.51141256, 0.63532096],\n",
       " [1.301826e-05, 0.51101953, 0.71541196],\n",
       " [0.79034412, 0.50755179, 0.0012218766],\n",
       " [0.0095081907, 0.50680906, 0.5251174],\n",
       " [0.51932758, 0.51280576, 0.28340474],\n",
       " [0.52472192, 0.50460321, 0.16027604],\n",
       " [8.6025551e-08, 0.50735891, 0.79368091],\n",
       " [0.75379968, 0.50891042, 0.0033743186],\n",
       " [0.66328007, 0.51100653, 0.053700253],\n",
       " [0.15212078, 0.51229823, 0.60480285],\n",
       " [0.54041988, 0.5106076, 0.31485006],\n",
       " [0.7131567, 0.51275551, 0.074685238],\n",
       " [0.0010545056, 0.51168424, 0.7303623],\n",
       " [0.5803079, 0.50701559, 0.14497377],\n",
       " [0.00057477335, 0.51121241, 0.86363024],\n",
       " [0.0059914459, 0.51087284, 0.6151877],\n",
       " [0.02406776, 0.50160944, 0.52462739],\n",
       " [0.093113057, 0.51391953, 0.10210314],\n",
       " [0.74644476, 0.5003233, 0.048135724],\n",
       " [0.99799782, 0.51890332, 0.00064732879],\n",
       " [0.77091652, 0.50925946, 0.030610096],\n",
       " [0.20892715, 0.51643759, 0.19448259],\n",
       " [0.72969961, 0.48383561, 0.00025034949],\n",
       " [0.67576188, 0.49686921, 0.034802891],\n",
       " [0.00021215747, 0.51042432, 0.92697805],\n",
       " [0.64940953, 0.51412183, 0.0006297364],\n",
       " [0.16955784, 0.51003355, 0.53293628],\n",
       " [0.04230658, 0.44896063, 0.68326873],\n",
       " [2.4815308e-06, 0.49947309, 0.76229274],\n",
       " [0.64940953, 0.51054329, 0.015811861],\n",
       " [0.96042043, 0.51664066, 0.00029560633],\n",
       " [0.99900442, 0.50338358, 0.020681685],\n",
       " [1.0074623e-05, 0.48987475, 0.8349542],\n",
       " [0.70928323, 0.48977834, 0.14314634],\n",
       " [0.19111036, 0.50452209, 0.061507665],\n",
       " [0.062879622, 0.51014394, 0.52546245],\n",
       " [0.49807242, 0.50979453, 0.76151478],\n",
       " [0.99496186, 0.50186294, 0.030404778],\n",
       " [0.71918964, 0.50643778, 0.012990779],\n",
       " [0.80652332, 0.5003497, 0.0054889224],\n",
       " [0.79344374, 0.50090462, 0.0017776197],\n",
       " [0.39021137, 0.51018375, 0.50948054],\n",
       " [0.0025969318, 0.50491995, 0.96200162],\n",
       " [0.3432067, 0.51398212, 0.42663774],\n",
       " [0.91156328, 0.51044941, 0.0006108167],\n",
       " [0.01289413, 0.50839382, 0.9157992],\n",
       " [1.083947e-10, 0.50666666, 0.92393082],\n",
       " [0.67598045, 0.50695544, 0.10237454],\n",
       " [0.8386845, 0.51092148, 0.0038063598],\n",
       " [4.6109224e-05, 0.46391913, 0.93976951],\n",
       " [0.14004101, 0.50802004, 0.58269918],\n",
       " [0.0024123024, 0.51725429, 0.58329338],\n",
       " [0.0020156594, 0.49404109, 0.52540827],\n",
       " [0.74981779, 0.4689284, 0.0078353779],\n",
       " [0.74223936, 0.50242633, 0.99999833],\n",
       " [7.3972774e-06, 0.46616796, 0.95667964],\n",
       " [0.017608851, 0.5015437, 0.72567445],\n",
       " [0.00091326656, 0.50263035, 0.95117325],\n",
       " [0.99999475, 0.5157451, 4.7329951e-14],\n",
       " [0.00018476801, 0.51224297, 0.96272939],\n",
       " [0.00018665935, 0.48648632, 0.7353465],\n",
       " [0.010644552, 0.50992513, 0.66189051],\n",
       " [0.00084581261, 0.45121691, 0.69280952],\n",
       " [0.70081007, 0.49779359, 0.028780108],\n",
       " [0.0044693947, 0.5086627, 0.77076977],\n",
       " [0.0024380961, 0.51229525, 0.62527049],\n",
       " [0.54479402, 0.50654328, 0.00023337181],\n",
       " [0.64940214, 0.51209521, 0.0086437371],\n",
       " [0.13257268, 0.5095998, 0.31268167],\n",
       " [1.2521826e-05, 0.50319767, 0.85498357],\n",
       " [0.00071084272, 0.51118064, 0.98927259],\n",
       " [6.3390995e-05, 0.50571346, 0.6983853],\n",
       " [0.64941007, 0.50479805, 0.21639003],\n",
       " [7.844539e-07, 0.48789486, 0.95887768],\n",
       " [0.72857231, 0.52046657, 0.00023474247],\n",
       " [0.00044875039, 0.50035298, 0.96593237],\n",
       " [0.41271392, 0.51193833, 0.40310687],\n",
       " [0.11489533, 0.50697047, 0.41596648],\n",
       " [0.64940959, 0.50919068, 0.0019403978],\n",
       " [0.66178769, 0.50948131, 0.0038742276],\n",
       " [0.82200903, 0.50700629, 0.0068977508],\n",
       " [0.99905711, 0.51471651, 1.5998667e-06],\n",
       " [0.71648443, 0.50702369, 0.011063534],\n",
       " [0.85180819, 0.49548426, 0.00036814206],\n",
       " [0.88344353, 0.5107916, 0.0073842704],\n",
       " [4.6197033e-06, 0.50106198, 0.95388418],\n",
       " [0.75989836, 0.50797886, 0.088952623],\n",
       " [0.0006762516, 0.46207377, 0.57258874],\n",
       " [0.030623907, 0.5143221, 0.98393321],\n",
       " [2.2532676e-08, 0.51299661, 0.89259768],\n",
       " [0.9619692, 0.50373632, 0.13423419],\n",
       " [0.0014808627, 0.4999983, 0.52509648],\n",
       " [3.7157943e-06, 0.49090436, 0.94590831],\n",
       " [0.90321046, 0.47748315, 0.12075463],\n",
       " [2.1738066e-11, 0.50623542, 0.99347395],\n",
       " [0.7125439, 0.51159483, 0.24399824],\n",
       " [1.6234826e-07, 0.50285357, 0.93684298],\n",
       " [0.97404701, 0.51264882, 0.0061416449],\n",
       " [0.12704326, 0.50857574, 0.52508563],\n",
       " [0.99041969, 0.50675696, 1.4319739e-06],\n",
       " [0.72504562, 0.51033646, 0.002000978],\n",
       " [0.99964929, 0.45834556, 8.7695567e-10],\n",
       " [0.00022107463, 0.45449179, 0.9841724],\n",
       " [0.79086757, 0.5113095, 0.17327748],\n",
       " [5.9566519e-05, 0.49282256, 0.90804935],\n",
       " [5.3685624e-05, 0.49802271, 0.84991521],\n",
       " [0.18207502, 0.50872433, 0.28286639],\n",
       " [0.42268124, 0.52197796, 0.11367867],\n",
       " [0.74934822, 0.50465328, 0.46201071],\n",
       " [0.99890959, 0.48438877, 2.9693026e-06],\n",
       " [0.0010563406, 0.48318329, 0.83756173],\n",
       " [0.00078205223, 0.50879645, 0.64142936],\n",
       " [0.002943364, 0.5069474, 0.95428199],\n",
       " [0.0017974654, 0.51228195, 0.75348157],\n",
       " [0.23064931, 0.51592332, 0.92230821],\n",
       " [0.81261569, 0.5097003, 0.18556377],\n",
       " [0.55054724, 0.51557899, 0.38205343],\n",
       " [0.80638349, 0.50492227, 0.14201707],\n",
       " [0.71358204, 0.51507562, 0.040705312],\n",
       " [0.00053471606, 0.51239747, 0.97880471],\n",
       " [0.62094748, 0.50691694, 0.00093770411],\n",
       " [0.74239957, 0.51058459, 0.0026234714],\n",
       " [0.0094565777, 0.51210254, 0.66043365],\n",
       " [0.00012103358, 0.49697596, 0.80198425],\n",
       " [0.037347317, 0.49612084, 0.60575587],\n",
       " [0.33100131, 0.50557917, 0.52507973],\n",
       " [4.3248663e-05, 0.51328117, 0.97126818],\n",
       " [0.96246022, 0.51120913, 0.014460746],\n",
       " [0.61279827, 0.51285034, 0.0014063938],\n",
       " [0.38456526, 0.50950968, 0.4930217],\n",
       " [0.78722554, 0.51348978, 0.005449851],\n",
       " [0.86155331, 0.50415474, 0.010286612],\n",
       " [0.23166104, 0.50924772, 0.53152668],\n",
       " [0.80392617, 0.50421345, 0.024340197],\n",
       " [0.95361674, 0.51383686, 2.1487481e-14],\n",
       " [0.004457715, 0.51157773, 0.70331317],\n",
       " [0.6494239, 0.50548136, 0.03560419],\n",
       " [0.079631194, 0.51596195, 0.27795833],\n",
       " [0.82805502, 0.50328946, 0.024020933],\n",
       " [0.036202237, 0.51015478, 0.52381867],\n",
       " [0.0014064319, 0.50956076, 0.87009323],\n",
       " [0.99288529, 0.51547152, 5.7687939e-06],\n",
       " [0.20624122, 0.51091456, 0.44265008],\n",
       " [0.90923536, 0.51044631, 0.012358965],\n",
       " [0.0053114244, 0.51022959, 0.66101527],\n",
       " [0.0079922043, 0.51361644, 0.87570739],\n",
       " [6.9926886e-05, 0.48155433, 0.56903034],\n",
       " [2.759426e-05, 0.50989187, 0.52516949],\n",
       " [0.98550272, 0.50462151, 0.0010920126],\n",
       " [0.64005041, 0.50961614, 0.034509003],\n",
       " [0.52849376, 0.50990468, 0.18970871],\n",
       " [6.8364302e-06, 0.50411707, 0.95721889],\n",
       " [0.89109206, 0.5126453, 0.022550829],\n",
       " [0.65410727, 0.50724924, 0.065997705],\n",
       " [4.7298155e-08, 0.48236176, 0.93321979],\n",
       " [2.2899444e-11, 0.51108581, 0.92569518],\n",
       " [0.0018450033, 0.50645757, 0.56823319],\n",
       " [0.0090803979, 0.51276708, 0.079433538],\n",
       " [0.46804085, 0.50884324, 0.16771686],\n",
       " [0.00048470029, 0.51105428, 0.72674811],\n",
       " [5.6110803e-05, 0.50838053, 0.52495372],\n",
       " [0.0037509564, 0.51241606, 0.075652055],\n",
       " [0.21379636, 0.51194304, 0.077603288],\n",
       " [0.02975365, 0.51441473, 0.036818154],\n",
       " [4.4196819e-05, 0.47655037, 0.91744161],\n",
       " [0.37620848, 0.50042373, 0.25072533],\n",
       " [8.6961052e-05, 0.50702453, 0.8222124],\n",
       " [0.75663036, 0.44804865, 0.0078086006],\n",
       " [0.99298054, 0.51601642, 0.00039132117],\n",
       " [0.62657231, 0.51440948, 0.00017188363],\n",
       " [0.0034172661, 0.51068115, 0.72718382],\n",
       " [0.03762291, 0.50677311, 0.080521688],\n",
       " [0.039821014, 0.49750608, 0.91380674],\n",
       " [9.2516086e-07, 0.49575949, 0.79899609],\n",
       " [0.99890161, 0.50845993, 0.0013465539],\n",
       " [0.0090301996, 0.51162463, 0.36253801],\n",
       " [0.94768679, 0.50079817, 0.0087041901],\n",
       " [0.35910389, 0.50817305, 0.58684593],\n",
       " [0.01577851, 0.50391281, 0.52692902],\n",
       " [0.69574934, 0.50755912, 0.028636139],\n",
       " [0.7678293, 0.49828324, 0.014957522],\n",
       " [0.00017093911, 0.49328753, 0.79051375],\n",
       " [0.11212444, 0.51203078, 0.76460713],\n",
       " [0.9914791, 0.50076473, 0.013503846],\n",
       " [0.99330795, 0.5059067, 0.0036274372],\n",
       " [0.39748976, 0.5072673, 0.52508563],\n",
       " [0.044788584, 0.51080245, 0.18263687],\n",
       " [0.65004075, 0.51017338, 0.086631529],\n",
       " [2.8839504e-06, 0.50055808, 0.98246199],\n",
       " [0.47175014, 0.51354384, 0.0073812939],\n",
       " [0.3108359, 0.50419801, 0.57635075],\n",
       " [0.98898357, 0.51033092, 0.012299796],\n",
       " [0.97654086, 0.51221913, 0.026755406],\n",
       " [0.085843071, 0.51685381, 0.77852249],\n",
       " [0.79103422, 0.50236034, 0.22995399],\n",
       " [0.00027529013, 0.50991791, 0.97396517],\n",
       " [0.99805272, 0.51407057, 1.0544086e-12],\n",
       " [0.60642117, 0.45941052, 0.0030935218],\n",
       " [5.0336468e-05, 0.50648272, 0.66536343],\n",
       " [4.4162025e-05, 0.50647956, 0.9924826],\n",
       " [0.34553239, 0.52200973, 0.0088125505],\n",
       " [0.89735073, 0.50395447, 0.030014563],\n",
       " [0.97877538, 0.47906569, 8.0763793e-06],\n",
       " [1.0, 0.47173402, 7.4051686e-15],\n",
       " [0.43276319, 0.50550276, 0.43891388],\n",
       " [0.062237404, 0.51190829, 0.93020457],\n",
       " [0.75041217, 0.49672404, 0.0036619268],\n",
       " [0.029105065, 0.51158202, 0.52508563],\n",
       " [0.025708683, 0.51165622, 0.52508605],\n",
       " [0.00051578035, 0.5115481, 0.67336512],\n",
       " [1.2193512e-06, 0.51606029, 3.8043424e-08],\n",
       " [0.0021169742, 0.50736833, 0.99381173],\n",
       " [0.018257834, 0.50498116, 0.65873826],\n",
       " [0.67670345, 0.49386364, 0.019184643],\n",
       " [0.97951424, 0.51788354, 0.00077418319],\n",
       " [0.31157452, 0.51304078, 0.52504545],\n",
       " [0.90782744, 0.5131709, 0.0017267878],\n",
       " [2.9597691e-06, 0.49519554, 0.81366706],\n",
       " [0.8384704, 0.50737506, 0.022381043],\n",
       " [0.00078024331, 0.50495249, 0.81215984],\n",
       " [0.43957886, 0.50446808, 0.30421534],\n",
       " [2.6473026e-06, 0.50396013, 0.99916089],\n",
       " [0.11558396, 0.51223266, 0.52497351],\n",
       " [0.14928792, 0.50581568, 0.5231756],\n",
       " [0.022138938, 0.51466316, 0.52319324],\n",
       " [0.94680417, 0.51244313, 3.6759737e-09],\n",
       " [0.87189525, 0.50756967, 0.12664542],\n",
       " [0.0021733006, 0.50107104, 0.67102653],\n",
       " [0.00042297452, 0.49877736, 0.52508563],\n",
       " [0.24729295, 0.51873565, 0.66430843],\n",
       " [0.64940965, 0.51190662, 0.068805315],\n",
       " [0.71878982, 0.48672718, 0.47455999],\n",
       " [0.040643275, 0.50867915, 0.077247351],\n",
       " [0.64717054, 0.50495231, 0.50902247],\n",
       " [0.74213946, 0.49070185, 3.9174616e-08],\n",
       " [0.99997675, 0.51305258, 0.00012038555],\n",
       " [0.004184755, 0.50426459, 0.57997376],\n",
       " [0.037649233, 0.50578809, 0.43778747],\n",
       " [0.79139382, 0.51136094, 0.025686953],\n",
       " [0.3146866, 0.50651032, 0.73751557],\n",
       " [0.057897747, 0.50900143, 0.52596247],\n",
       " [0.015859116, 0.4875589, 0.52612573],\n",
       " [0.00090148667, 0.50038153, 0.57685399],\n",
       " [0.64940953, 0.49257937, 0.40957785],\n",
       " [0.9510361, 0.51620579, 4.1371034e-12],\n",
       " [0.66211939, 0.5126003, 0.030503804],\n",
       " [0.95687705, 0.51262277, 0.001320487],\n",
       " [0.76637185, 0.50572187, 0.0049635307],\n",
       " [0.79592478, 0.51049823, 0.022190094],\n",
       " [0.045948252, 0.50531995, 0.90508348],\n",
       " [0.71736449, 0.49878377, 0.0008225129],\n",
       " [0.50574917, 0.45517537, 0.38233608],\n",
       " [0.71784139, 0.49753651, 0.0033362345],\n",
       " [7.6700354e-11, 0.50372279, 0.9960227],\n",
       " [0.78571314, 0.50171447, 0.010059109],\n",
       " [0.9916265, 0.50912821, 0.00081993046],\n",
       " [0.64940953, 0.46948022, 0.00020947443],\n",
       " [0.16358371, 0.50917155, 0.28031594],\n",
       " [0.58338058, 0.51144874, 0.00093577011],\n",
       " [0.0019046256, 0.50582355, 0.45217106],\n",
       " [0.51232851, 0.49707556, 0.18211032],\n",
       " [0.64940953, 0.49228451, 0.044064485],\n",
       " [0.00019446608, 0.51314175, 0.69252712],\n",
       " [0.64940953, 0.51145214, 0.011902936],\n",
       " [0.34859455, 0.49951759, 0.52508563],\n",
       " [3.3640433e-06, 0.51043785, 0.95689124],\n",
       " [0.00010771809, 0.47996947, 0.77943468],\n",
       " [0.67071611, 0.51378888, 0.0259232],\n",
       " [0.022309596, 0.51188141, 0.098257802],\n",
       " [0.35981983, 0.50368196, 0.022579024],\n",
       " [0.18372668, 0.46229789, 3.182117e-08],\n",
       " [0.30467522, 0.52337664, 0.343851],\n",
       " [0.6003837, 0.51116854, 0.49144068],\n",
       " [0.00012641441, 0.4677327, 0.74994862],\n",
       " [0.72025454, 0.51027507, 0.27543214],\n",
       " [0.69028556, 0.49697879, 0.11111043],\n",
       " [0.77598715, 0.50546288, 0.087316483],\n",
       " [0.0043977089, 0.49240085, 0.65079516],\n",
       " [0.9696312, 0.50582176, 0.0042001992],\n",
       " [0.99979454, 0.47185794, 2.9355626e-05],\n",
       " [0.88525844, 0.51099563, 0.029098568],\n",
       " [0.67938519, 0.51160955, 0.0078605795],\n",
       " [0.073955767, 0.50286853, 0.0018838466],\n",
       " [0.33835712, 0.50745481, 0.22829859],\n",
       " [0.00023300697, 0.50873697, 0.97259045],\n",
       " [0.64940953, 0.51129985, 0.075826064],\n",
       " [0.99780887, 0.51025593, 4.9836191e-05],\n",
       " [0.88506788, 0.49289361, 0.0052607371],\n",
       " [0.78436822, 0.49207839, 0.023190388],\n",
       " [0.031510614, 0.51184052, 0.52508563],\n",
       " [0.64940953, 0.51215529, 0.0052018045],\n",
       " [0.77412379, 0.50754642, 0.03945132],\n",
       " [0.42440176, 0.49499166, 0.52508563],\n",
       " [0.00068622344, 0.49928394, 0.86562133],\n",
       " [0.64940953, 0.50759232, 0.10394534],\n",
       " [0.64940953, 0.51284331, 0.13854855],\n",
       " [0.013125014, 0.51022989, 0.24304742],\n",
       " [0.0003115765, 0.50915921, 0.52508563],\n",
       " [0.069680437, 0.51321656, 0.69555378],\n",
       " [0.29037353, 0.46071526, 0.777641],\n",
       " [0.96995729, 0.4084194, 9.6746786e-05],\n",
       " [0.77174664, 0.51028425, 0.00025003031],\n",
       " [0.76420856, 0.43463367, 0.0024641859],\n",
       " [0.53984451, 0.50453043, 0.30666929],\n",
       " [0.00035918114, 0.50970608, 0.72864914],\n",
       " [0.99668425, 0.49938217, 1.1749989e-05],\n",
       " [0.023463849, 0.50911927, 0.85685843],\n",
       " [1.5782718e-06, 0.50090575, 0.99206889],\n",
       " [1.0, 0.51623058, 6.6152474e-08],\n",
       " [0.043874744, 0.51463521, 0.40604478],\n",
       " [6.0926817e-05, 0.51478332, 0.52508563],\n",
       " [0.23930332, 0.52640069, 0.37523246],\n",
       " [0.98287892, 0.50741684, 0.0012781783],\n",
       " [0.21560089, 0.5118044, 0.17988853],\n",
       " [0.64940953, 0.50198621, 0.20729977],\n",
       " [0.65291971, 0.49205607, 7.6831726e-05],\n",
       " [0.00013642773, 0.5087617, 0.9376123],\n",
       " [0.09772148, 0.5133059, 0.027768629],\n",
       " [0.00013541461, 0.50798094, 0.8787787],\n",
       " [1.7356635e-05, 0.51141471, 0.87059641],\n",
       " [1.4936494e-07, 0.50801694, 0.98109919],\n",
       " [0.7590577, 0.50460953, 0.075152472],\n",
       " [0.00016369774, 0.49472624, 0.70202941],\n",
       " [0.67081517, 0.50731581, 0.0070036859],\n",
       " [7.7627847e-05, 0.51190382, 0.67852789],\n",
       " [0.079841271, 0.51055115, 0.73918641],\n",
       " [2.2299326e-05, 0.50888908, 0.83811975],\n",
       " [0.0017887991, 0.5129897, 0.8010177],\n",
       " [9.4224393e-05, 0.50876325, 0.67176181],\n",
       " [0.078001589, 0.50076658, 0.52508563],\n",
       " [0.16321269, 0.50023609, 0.52689099],\n",
       " [0.0012834644, 0.51150304, 0.55474311],\n",
       " [0.99302131, 0.43001834, 0.00051223446],\n",
       " [0.001139492, 0.51056367, 0.82372588],\n",
       " [0.78980875, 0.50536132, 0.043380398],\n",
       " [0.0049180607, 0.50998855, 0.52509528],\n",
       " [0.43294516, 0.50650507, 0.24549493],\n",
       " [0.041929536, 0.50928289, 0.52508563],\n",
       " [0.011857942, 0.50214565, 0.41245824],\n",
       " [0.11383158, 0.51117939, 0.28489882],\n",
       " [0.75239927, 0.51128268, 0.027551521],\n",
       " [0.14511804, 0.43303388, 0.7389341],\n",
       " [0.84761804, 0.50682551, 0.010036134],\n",
       " [0.83282781, 0.50741899, 0.33518922],\n",
       " [0.85568583, 0.51177651, 0.074146867],\n",
       " [0.64748359, 0.51553434, 0.016174188],\n",
       " [4.7581085e-05, 0.5142647, 0.91533905],\n",
       " [0.01596125, 0.51344669, 0.48660493],\n",
       " [0.0031723781, 0.50677913, 0.59067732],\n",
       " [0.76821643, 0.51208586, 0.0051145251],\n",
       " [0.0075646653, 0.52069324, 0.18423747],\n",
       " [0.012743435, 0.51339972, 0.075312503],\n",
       " [0.56637257, 0.50700253, 0.21111386],\n",
       " [0.76734525, 0.48012349, 0.019551922],\n",
       " [8.1638236e-06, 0.4973574, 0.78347963],\n",
       " [8.4748727e-08, 0.51408082, 0.92484605],\n",
       " [0.00024547527, 0.50994003, 0.93407053],\n",
       " [0.019496178, 0.51651126, 0.26313135],\n",
       " [0.60882479, 0.51158005, 0.0046146293],\n",
       " [0.063950576, 0.51047695, 0.52508563],\n",
       " [2.5236102e-05, 0.51375324, 0.99895644],\n",
       " [0.0056915502, 0.51145637, 0.40338737],\n",
       " [0.0029418766, 0.50626886, 0.85333931],\n",
       " [0.013516549, 0.4885346, 0.57388628],\n",
       " [0.00017150921, 0.51027566, 0.96047765],\n",
       " [0.0008133279, 0.49435401, 0.84399378],\n",
       " [0.71732426, 0.43887439, 0.0050051478],\n",
       " [0.99804652, 0.51212925, 6.7108878e-05],\n",
       " [0.64940953, 0.52041554, 0.77331501],\n",
       " [0.076132551, 0.50067151, 0.39374068],\n",
       " [0.61328715, 0.51584482, 0.014026649],\n",
       " [0.64940953, 0.51121265, 0.063837908],\n",
       " [0.99695861, 0.51055825, 0.019690914],\n",
       " [0.89267468, 0.48066062, 4.1994758e-06],\n",
       " [0.43292573, 0.50334769, 0.65673834],\n",
       " [0.81034166, 0.51307756, 0.0062341713],\n",
       " [0.61408973, 0.51000965, 0.0018112778],\n",
       " [0.82850933, 0.5022251, 0.0067687957],\n",
       " [0.49125436, 0.50904799, 0.53424114],\n",
       " [0.7139349, 0.4953199, 0.049601473],\n",
       " [0.090287536, 0.5068745, 0.30930698],\n",
       " [0.025895242, 0.51224357, 0.70977706],\n",
       " [2.2467777e-09, 0.51037282, 0.97995985],\n",
       " [0.70731562, 0.48914993, 0.023081135],\n",
       " [0.83466309, 0.5086236, 0.004989902],\n",
       " [0.0053509097, 0.5060271, 0.76569742],\n",
       " [0.67228049, 0.49091211, 0.99992859],\n",
       " [0.24156831, 0.51021522, 0.21697402],\n",
       " [0.99094498, 0.51149613, 0.0095529091],\n",
       " [0.016819598, 0.5117259, 0.52510762],\n",
       " [0.91547549, 0.5057568, 0.047720898],\n",
       " [0.027270745, 0.51145959, 0.87954164],\n",
       " [0.65745676, 0.51153088, 0.14229929],\n",
       " [2.1360913e-08, 0.50823361, 0.87160581],\n",
       " [0.78273189, 0.49732256, 0.0061872876],\n",
       " [0.017420055, 0.51231676, 0.34845623],\n",
       " [0.7005018, 0.51081443, 0.0054534706],\n",
       " [0.96360761, 0.47967914, 7.8107092e-05],\n",
       " [0.99174494, 0.5111357, 0.0082027009],\n",
       " [0.14046481, 0.50900662, 0.27560049],\n",
       " [2.767468e-06, 0.51190966, 0.70491767],\n",
       " [0.99842191, 0.51079839, 0.0057794545],\n",
       " [0.97624683, 0.51323783, 0.0012426504],\n",
       " [0.042874079, 0.50953293, 0.65685976],\n",
       " [0.0045840773, 0.51261371, 0.44835657],\n",
       " [0.0051423982, 0.51411963, 0.56612283],\n",
       " [0.64940906, 0.50982273, 0.11003897],\n",
       " [0.0018317874, 0.51390845, 0.66221535],\n",
       " [0.68870431, 0.46450803, 0.18035436],\n",
       " [0.0050875014, 0.5095731, 0.4374615],\n",
       " [0.68210334, 0.51037657, 0.014327505],\n",
       " [0.6271106, 0.50816089, 0.33651683],\n",
       " [9.8798601e-08, 0.50936681, 0.98993462],\n",
       " [0.714477, 0.51316911, 0.049270473],\n",
       " [0.94752294, 0.50583708, 0.0070986333],\n",
       " [0.39496627, 0.51070946, 0.52511197],\n",
       " [0.018250817, 0.50357443, 0.86769629],\n",
       " [0.64940959, 0.50823355, 0.039710622],\n",
       " [0.64940953, 0.50411785, 0.0029474136],\n",
       " [0.64940953, 0.51146555, 0.0306815],\n",
       " [0.80144411, 0.51126873, 0.15188463],\n",
       " [0.93174267, 0.51154351, 0.035471793],\n",
       " [0.78924656, 0.5106883, 0.064746313],\n",
       " [0.027619531, 0.50933242, 0.69654292],\n",
       " [0.0095252246, 0.51418865, 0.52508563],\n",
       " [0.85916966, 0.50283945, 0.012711441],\n",
       " [0.96912473, 0.51451308, 2.1961466e-09],\n",
       " [0.64365965, 0.49955302, 0.015704228],\n",
       " [0.76858395, 0.43266043, 1.6730883e-05],\n",
       " [0.0012774512, 0.50855827, 0.64603621],\n",
       " [0.0077709854, 0.50976586, 0.33303028],\n",
       " [0.69135088, 0.45722619, 0.00073575682],\n",
       " [0.012784277, 0.50994974, 0.81081474],\n",
       " [0.41742042, 0.50511461, 0.16487706],\n",
       " [3.5943717e-07, 0.48301315, 0.98849839],\n",
       " [0.64422464, 0.50141674, 0.52274311],\n",
       " [0.10644919, 0.44426268, 0.28063831],\n",
       " [0.57124305, 0.51150668, 0.0046247328],\n",
       " [0.64940953, 0.51212209, 0.2107026],\n",
       " [0.0008892616, 0.51170808, 0.82674909],\n",
       " [0.99997389, 0.51315385, 4.4273801e-07],\n",
       " [0.64940953, 0.5084883, 0.075401708],\n",
       " [1.8186003e-06, 0.49456161, 0.97253031],\n",
       " [0.00026528721, 0.51278228, 0.77990162],\n",
       " [0.99163508, 0.47114357, 9.6417352e-07],\n",
       " [0.99801505, 0.51391602, 0.00060801936],\n",
       " [0.071223304, 0.51608384, 0.52508706],\n",
       " [0.00014126136, 0.5010516, 0.52508563],\n",
       " [7.0948125e-05, 0.4929702, 0.62779403],\n",
       " [0.096642189, 0.51696557, 0.29070899],\n",
       " [0.6204161, 0.50791526, 0.25629255],\n",
       " [0.64924562, 0.50248247, 0.0383921],\n",
       " [0.00011254694, 0.49391738, 0.83053857],\n",
       " [0.0016279838, 0.51039457, 0.82788795],\n",
       " [0.28870243, 0.51022869, 0.40526929],\n",
       " [0.017848888, 0.51239783, 0.52507967],\n",
       " [0.81777877, 0.48306906, 0.097116604],\n",
       " [0.0042040162, 0.51084036, 0.91942465],\n",
       " [0.0098393336, 0.50578225, 0.53140068],\n",
       " [0.64940953, 0.51589084, 0.01162193],\n",
       " [1.0064194e-08, 0.50999868, 0.98089051],\n",
       " [0.64940953, 0.49871004, 0.036209468],\n",
       " [0.77903163, 0.43880951, 5.7181864e-05],\n",
       " [1.4550275e-05, 0.5085184, 0.98714209],\n",
       " [3.1608488e-07, 0.49294955, 0.95670015],\n",
       " [0.65946245, 0.51029211, 0.027076673],\n",
       " [0.060532525, 0.49865007, 0.22802034],\n",
       " [0.29953191, 0.50852722, 0.3268854],\n",
       " [0.47628984, 0.47036523, 0.31800082],\n",
       " [0.00029975607, 0.49623382, 0.51020801],\n",
       " [0.65000176, 0.49231359, 4.95527e-09],\n",
       " [0.6516183, 0.50106472, 0.0084329732],\n",
       " [0.010476017, 0.51496983, 0.57946551],\n",
       " [0.81560051, 0.4768399, 0.001458798],\n",
       " [0.79502881, 0.4968732, 0.012542022],\n",
       " [0.18050022, 0.51160526, 0.27476129],\n",
       " [0.47863534, 0.51473606, 0.092013106],\n",
       " [0.27850187, 0.51113313, 0.21762821],\n",
       " [0.7812317, 0.50113064, 0.00035908067],\n",
       " [0.64940953, 0.51011986, 0.18621199],\n",
       " [0.10320206, 0.512667, 0.58184588],\n",
       " [0.9999969, 0.51260012, 9.1754822e-20],\n",
       " [0.8354615, 0.50859272, 0.0023931784],\n",
       " [0.64941072, 0.5110296, 0.046397913],\n",
       " [9.8012169e-05, 0.51335716, 0.96074867],\n",
       " [0.72150403, 0.5056718, 0.066098109],\n",
       " [0.7194441, 0.50409406, 0.0020323533],\n",
       " [0.0037294091, 0.51137745, 0.75477642],\n",
       " [0.64940953, 0.509624, 0.27828544],\n",
       " [0.64940953, 0.51049495, 0.10185261],\n",
       " [0.71163845, 0.50448674, 0.090520628],\n",
       " [0.64952832, 0.51263052, 0.0075765233],\n",
       " [0.37241134, 0.5064218, 0.43026143],\n",
       " [0.88289744, 0.5123632, 0.0012362342],\n",
       " [0.17548816, 0.51390195, 0.56101966],\n",
       " [0.64940953, 0.5083552, 0.17033675],\n",
       " [0.00013798184, 0.48214516, 0.62234968],\n",
       " [0.00015060257, 0.5155924, 0.53789812],\n",
       " [0.99411607, 0.50556147, 0.010313871],\n",
       " [0.97295582, 0.44767213, 1.7049737e-05],\n",
       " [0.0067234351, 0.51394558, 0.36072564],\n",
       " [0.00017485066, 0.51231784, 0.79081494],\n",
       " [0.00089152122, 0.51004755, 0.76785105],\n",
       " [0.64940953, 0.51318336, 0.011687555],\n",
       " [0.80161238, 0.50520658, 0.021571519],\n",
       " [0.0011765178, 0.5129196, 0.78811103],\n",
       " [0.31907302, 0.5103125, 0.4809483],\n",
       " [0.00023779212, 0.50924528, 0.52508563],\n",
       " [0.0037496078, 0.51062965, 0.53554302],\n",
       " [0.0054271668, 0.49354202, 0.87963718],\n",
       " [0.57526505, 0.51111186, 0.044508461],\n",
       " [4.2422707e-07, 0.46202868, 0.88691604],\n",
       " [0.20136456, 0.509157, 0.44242585],\n",
       " [0.0012279318, 0.50681329, 0.55227959],\n",
       " [0.23651284, 0.51626211, 0.22356778],\n",
       " [0.86664224, 0.49668601, 0.0022791007],\n",
       " [0.66060662, 0.50100666, 0.11271895],\n",
       " [0.078402795, 0.51513219, 0.22903737],\n",
       " [5.1534324e-05, 0.50659043, 0.9502334],\n",
       " [0.88988256, 0.48904282, 0.0064699668],\n",
       " [0.31400728, 0.50911206, 0.33760574],\n",
       " [0.77337438, 0.41426566, 0.00020797031],\n",
       " [0.78922915, 0.51070392, 0.17981462],\n",
       " [0.080290221, 0.50992823, 0.40752831],\n",
       " [0.011559424, 0.51140696, 0.85815787],\n",
       " [0.70584631, 0.50576222, 0.012220098],\n",
       " [0.7474032, 0.49336204, 0.011085833],\n",
       " [0.00046086812, 0.50221395, 0.52647996],\n",
       " [0.64940953, 0.51274371, 0.020954702],\n",
       " [0.64940953, 0.51056486, 0.060065437],\n",
       " [0.70800585, 0.49385312, 0.058231667],\n",
       " [0.0005183692, 0.50204116, 0.52508563],\n",
       " [0.72728717, 0.50904077, 0.011634414],\n",
       " [0.68624175, 0.49007413, 0.055754468],\n",
       " [0.65043348, 0.50852865, 0.031372223],\n",
       " [0.79276443, 0.50010973, 0.031531107],\n",
       " [2.5686154e-07, 0.4927429, 0.99104238],\n",
       " [0.64940959, 0.50705242, 0.03595103],\n",
       " [0.056979064, 0.44951203, 0.94552511],\n",
       " [0.55227482, 0.51158971, 0.074214786],\n",
       " [0.43365625, 0.51875836, 0.00017034131],\n",
       " [0.74148107, 0.51290202, 0.08040572],\n",
       " [0.81692356, 0.48076105, 0.34731442],\n",
       " [0.70050293, 0.50484312, 0.46320432],\n",
       " [0.70870924, 0.5077939, 0.0055411132],\n",
       " [0.023875698, 0.51668179, 0.41063818],\n",
       " [2.4902096e-07, 0.50399768, 0.91893947],\n",
       " [0.0093882428, 0.51481575, 0.56142431],\n",
       " [0.9407143, 0.5076105, 0.037290696],\n",
       " [0.96798885, 0.5133118, 0.057816803],\n",
       " [0.0015928699, 0.51107949, 0.6699152],\n",
       " [0.64942598, 0.51276261, 0.006597905],\n",
       " [3.6921156e-05, 0.50760436, 0.86530948],\n",
       " [0.99005044, 0.51307809, 0.0081393085],\n",
       " [0.71862447, 0.4991945, 0.0033628338],\n",
       " [0.10967328, 0.51569605, 0.31215227],\n",
       " [0.64940953, 0.51204187, 0.15125361],\n",
       " [0.66671401, 0.50647372, 0.019261833],\n",
       " [0.96507907, 0.51083446, 0.0017397213],\n",
       " [0.0657942, 0.50923359, 0.35673797],\n",
       " [0.98403293, 0.51149511, 0.0019095488],\n",
       " [0.9845804, 0.50078106, 0.0077665634],\n",
       " [0.71321356, 0.51322281, 0.0033297658],\n",
       " [0.00014829844, 0.48403507, 0.62965679],\n",
       " [0.64941192, 0.46165913, 0.0027013905],\n",
       " [0.016009079, 0.5094735, 0.52508563],\n",
       " [0.73666102, 0.49987891, 0.1922005],\n",
       " [0.1188679, 0.51437175, 0.43187615],\n",
       " [0.00091989979, 0.49871632, 0.90324289],\n",
       " [0.8901006, 0.51157981, 0.00067156676],\n",
       " [0.4358899, 0.49146906, 0.39674863],\n",
       " [0.99792308, 0.51383346, 4.3361855e-05],\n",
       " [0.009298699, 0.51412082, 0.34090504],\n",
       " [0.70872962, 0.5069012, 0.22384761],\n",
       " [0.64947325, 0.51242411, 4.0101342e-05],\n",
       " [0.99313933, 0.5149051, 0.016426452],\n",
       " [0.64940959, 0.50946146, 0.022286644],\n",
       " [0.13107021, 0.50173944, 0.51834351],\n",
       " [0.99989128, 0.48201224, 2.1585976e-07],\n",
       " [0.67672038, 0.5051536, 0.18073305],\n",
       " [0.5548324, 0.50399214, 0.3342599],\n",
       " [0.37791064, 0.49950376, 0.80320001],\n",
       " [0.83325499, 0.49728629, 0.0094383182],\n",
       " [0.24602829, 0.51665056, 0.52508563],\n",
       " [0.82840347, 0.51082307, 0.0035867144],\n",
       " [0.97952282, 0.51648623, 1.6630843e-09],\n",
       " [0.00020087657, 0.51069957, 0.55240327],\n",
       " [0.013159435, 0.4817791, 0.58111924],\n",
       " [0.75306535, 0.508555, 0.033371203],\n",
       " [0.97381771, 0.50574654, 0.003436263],\n",
       " [5.1217721e-06, 0.50507993, 0.65310729],\n",
       " [0.020257013, 0.5132826, 0.83748865],\n",
       " [0.00011562678, 0.50330997, 0.62867969],\n",
       " [0.7341482, 0.51010013, 0.019521354],\n",
       " [0.025688201, 0.52369529, 0.23933749],\n",
       " [0.0026345968, 0.50882459, 0.62179261],\n",
       " [0.74084204, 0.50572306, 0.0064459033],\n",
       " [6.888948e-05, 0.51295692, 0.99141556],\n",
       " [0.0025237629, 0.50374275, 0.73426598],\n",
       " [0.00019765865, 0.51107836, 0.87445152],\n",
       " [8.5677912e-06, 0.49359167, 0.84497523],\n",
       " [0.79668385, 0.51210648, 0.06171865],\n",
       " [0.91685712, 0.47583643, 0.056581557],\n",
       " [0.51457602, 0.51460981, 0.010084416],\n",
       " [0.83619833, 0.51164997, 0.028138481],\n",
       " [0.65796822, 0.50724542, 0.20929538],\n",
       " [0.99995172, 0.51162767, 1.59255e-05],\n",
       " [0.012972578, 0.51249474, 0.52508563],\n",
       " [0.00011174298, 0.50435895, 0.70478058],\n",
       " [0.81372321, 0.4753547, 0.0039585494],\n",
       " [0.91019011, 0.506194, 0.00070295809],\n",
       " [0.7970289, 0.51004308, 0.004861868],\n",
       " [0.97011447, 0.51177257, 0.0055289925],\n",
       " [0.0010456294, 0.51138741, 0.36187783],\n",
       " [0.48082313, 0.51015943, 0.072345413],\n",
       " [0.77609402, 0.50833291, 0.020306204],\n",
       " [0.99339229, 0.51359171, 0.0028585829],\n",
       " [0.0098313792, 0.51200831, 0.39278296],\n",
       " [0.012821986, 0.4862631, 0.93027788],\n",
       " [0.00066669594, 0.50932032, 0.87298274],\n",
       " [0.0038280622, 0.51027369, 0.99951732],\n",
       " [0.22062482, 0.50234067, 0.33783314],\n",
       " [0.96106702, 0.51628715, 0.0036131951],\n",
       " [0.64940953, 0.49590832, 0.0044665141],\n",
       " [0.71702605, 0.49299648, 0.0060242154],\n",
       " [0.93026841, 0.42769319, 7.7850217e-07],\n",
       " [0.32698604, 0.51032716, 0.20778254],\n",
       " [0.78480119, 0.51089925, 0.012466913],\n",
       " [0.0090681044, 0.508331, 0.24212117],\n",
       " [0.97408462, 0.51514959, 1.1836465e-34],\n",
       " [0.00011238306, 0.50462639, 0.94592535],\n",
       " [0.99961448, 0.5186379, 3.4738491e-06],\n",
       " [3.1370578e-06, 0.49289438, 0.95702058],\n",
       " [8.3918698e-05, 0.50411773, 0.59780371],\n",
       " [0.67660522, 0.50971115, 0.011969419],\n",
       " [0.63299054, 0.52005261, 0.039754156],\n",
       " [0.71821547, 0.49449703, 0.00056666369],\n",
       " [0.98549962, 0.47424394, 3.2276192e-05],\n",
       " [0.0088624088, 0.51253885, 0.2965517],\n",
       " [0.36886811, 0.50098121, 0.051151313],\n",
       " [0.092813104, 0.51093149, 0.45819554],\n",
       " [0.71344352, 0.49975714, 0.13340743],\n",
       " [0.69276261, 0.51094627, 0.28402501],\n",
       " [0.00094136893, 0.5112021, 0.8958531],\n",
       " [4.8261391e-08, 0.50939506, 0.87718201],\n",
       " [0.65111947, 0.49028131, 6.4254419e-07],\n",
       " [0.26873511, 0.51166332, 0.30971667],\n",
       " [0.28398904, 0.51038772, 0.65228021],\n",
       " [0.23817454, 0.50422251, 0.44272697],\n",
       " [0.8100884, 0.51246119, 0.0071156458],\n",
       " [0.65221918, 0.5112952, 0.00037728166],\n",
       " [0.34425467, 0.51460987, 0.51914632],\n",
       " [0.11675793, 0.49888021, 0.5249415],\n",
       " [0.97084707, 0.50570273, 5.4820998e-08],\n",
       " [0.024350639, 0.50593698, 0.21572226],\n",
       " [0.018366544, 0.51199567, 0.6085996],\n",
       " [0.76912588, 0.50898188, 0.0088689653],\n",
       " [2.1068861e-06, 0.48949835, 0.55453181],\n",
       " [1.654303e-05, 0.504839, 0.68274212],\n",
       " [1.2975498e-05, 0.49923676, 0.83127296],\n",
       " [0.69568765, 0.51178843, 0.012701307],\n",
       " [1.2453573e-07, 0.50341368, 0.97392523],\n",
       " [0.94549119, 0.43315011, 1.3772861e-05],\n",
       " [0.39906293, 0.50614345, 0.5387761],\n",
       " [0.64940953, 0.50494707, 0.13661641],\n",
       " [0.43715543, 0.50760472, 0.15782957],\n",
       " [0.36614642, 0.51612437, 0.52508914],\n",
       " [0.65573508, 0.51097971, 0.21898744],\n",
       " [3.6614183e-05, 0.50759459, 0.9741537],\n",
       " [0.99985516, 0.51625389, 6.7015048e-38],\n",
       " [0.90074146, 0.48921856, 0.0053597442],\n",
       " [0.99972123, 0.51161414, 8.7376382e-09],\n",
       " [0.81505787, 0.50872314, 0.015546815],\n",
       " [5.2875398e-06, 0.50636506, 0.97292942],\n",
       " [0.90651393, 0.4976882, 0.0016704218],\n",
       " [2.0201445e-05, 0.51108629, 0.92446423],\n",
       " [0.00020112035, 0.50483727, 0.45063895],\n",
       " [0.99891973, 0.51130933, 7.5344928e-05],\n",
       " [0.010765132, 0.50413775, 0.59066862],\n",
       " [0.00033887761, 0.4994235, 0.57673752],\n",
       " [4.5434334e-08, 0.50472987, 0.97506136],\n",
       " [0.42448911, 0.50737625, 0.36004019],\n",
       " [0.29771116, 0.52149034, 0.51120079],\n",
       " [0.67167324, 0.50808918, 0.056754906],\n",
       " [8.8676308e-05, 0.48272353, 0.52508563],\n",
       " [0.007067177, 0.4957284, 0.96869993],\n",
       " [8.2297129e-06, 0.51097256, 0.99408567],\n",
       " [0.00014163698, 0.4928399, 0.67377889],\n",
       " [0.9926427, 0.51655143, 2.8501845e-05],\n",
       " [0.28071743, 0.51228088, 0.33359262],\n",
       " [0.0030599586, 0.5099293, 0.52508467],\n",
       " [0.37076944, 0.51362097, 0.022209803],\n",
       " [0.00029761749, 0.51222336, 0.89915395],\n",
       " [0.0016144164, 0.51095229, 0.55021363],\n",
       " [0.41229081, 0.51296777, 0.0033144541],\n",
       " [0.64940143, 0.50821328, 0.3160511],\n",
       " [0.042564236, 0.51308459, 0.53531712],\n",
       " [1.2894219e-08, 0.50730133, 0.99057794],\n",
       " [0.9949131, 0.50480819, 0.00012699072],\n",
       " [0.67552626, 0.50233608, 0.23830774],\n",
       " [7.6649805e-05, 0.51176053, 0.90067482],\n",
       " [0.83164352, 0.50540113, 0.10782475],\n",
       " [0.96966332, 0.51107371, 0.014473261],\n",
       " [0.0037549566, 0.50982159, 0.72893208],\n",
       " [0.64939612, 0.51060003, 0.37602392],\n",
       " [0.52793705, 0.50869441, 0.18196151],\n",
       " [0.39017749, 0.52549416, 0.11797886],\n",
       " [0.74259746, 0.50469077, 0.0071395161],\n",
       " [0.043707449, 0.51284838, 0.72946268],\n",
       " [0.91364312, 0.51578164, 0.000134206],\n",
       " [0.008395859, 0.51311344, 0.87289023],\n",
       " [0.70706397, 0.50739956, 0.036809549],\n",
       " [0.60601068, 0.50704867, 0.20877261],\n",
       " [0.80617112, 0.50512969, 0.032344628],\n",
       " [0.99351728, 0.46269828, 0.00064291793],\n",
       " [0.0015060088, 0.51092696, 0.73577297],\n",
       " [0.00018352504, 0.50851953, 0.66210324],\n",
       " [0.00593162, 0.49620953, 0.52508533],\n",
       " [4.8044149e-05, 0.51206237, 0.92562479],\n",
       " [0.014819036, 0.48109734, 0.75298625],\n",
       " [0.9980489, 0.49091557, 2.0240358e-07],\n",
       " [0.79724419, 0.42038891, 5.9054159e-06],\n",
       " [0.90226495, 0.51203352, 0.03175341],\n",
       " [0.026059581, 0.50362277, 0.50139713],\n",
       " [0.097342752, 0.50834244, 0.54361767],\n",
       " [0.74219888, 0.51192969, 0.008834878],\n",
       " [0.99529028, 0.51660371, 2.007782e-12],\n",
       " [0.76071662, 0.50606412, 0.16327059],\n",
       " [0.090557925, 0.47560161, 0.50622058],\n",
       " [0.38383681, 0.50098234, 0.64782679],\n",
       " [0.71725267, 0.5088684, 0.0030148877],\n",
       " [7.9013853e-06, 0.49220315, 0.9490698],\n",
       " [0.72528338, 0.5097118, 0.012315574],\n",
       " [0.813972, 0.50909233, 0.0061049554],\n",
       " [0.30236262, 0.51728404, 0.52507764],\n",
       " [0.40789875, 0.48539832, 0.0019729128],\n",
       " [2.342148e-07, 0.5076111, 0.95106435],\n",
       " [0.65155888, 0.50695282, 0.081947528],\n",
       " [5.2300926e-05, 0.496508, 0.70383477],\n",
       " [0.54038399, 0.50990498, 0.14169709],\n",
       " [0.27041939, 0.44272527, 0.40149644],\n",
       " [0.024169281, 0.50961936, 0.52508569],\n",
       " [0.00075603434, 0.5061506, 0.95630294],\n",
       " [0.75210571, 0.50388342, 0.071988665],\n",
       " [0.0017808955, 0.50638998, 0.52508801],\n",
       " [0.00012887582, 0.51340854, 0.9795419],\n",
       " [0.22135462, 0.51083553, 0.31128082],\n",
       " [0.77077758, 0.51558828, 0.019250276],\n",
       " [0.64942527, 0.50899529, 0.057354424],\n",
       " [0.71048135, 0.46836376, 0.00029691576],\n",
       " [0.82837105, 0.50803101, 0.019177843],\n",
       " [0.86209178, 0.44919658, 0.015721703],\n",
       " [0.00051859306, 0.51194912, 0.97572392],\n",
       " [0.00035425273, 0.49650401, 0.99906415],\n",
       " [0.0067059402, 0.51842201, 0.36152181],\n",
       " [2.2307644e-05, 0.49047539, 0.99477059],\n",
       " [0.17271201, 0.48801225, 0.36735317],\n",
       " [0.64936495, 0.51624203, 0.0030588403],\n",
       " [0.00050871063, 0.49700308, 0.58638817],\n",
       " [0.22447158, 0.49595386, 0.33210781],\n",
       " [2.0778809e-06, 0.50384945, 0.87173826],\n",
       " [0.155058, 0.51135385, 0.52508855],\n",
       " [1.3730422e-05, 0.51195699, 0.88683534],\n",
       " [0.9956845, 0.51498085, 0.00010686585],\n",
       " [0.64940953, 0.51139045, 0.0068596248],\n",
       " [0.00056293787, 0.505225, 0.64234293],\n",
       " [0.64933741, 0.5053336, 0.49206159],\n",
       " [0.0020540447, 0.51248604, 0.84846008],\n",
       " [0.0074774837, 0.51358032, 0.42636517],\n",
       " [3.0142106e-05, 0.47769135, 0.97236407],\n",
       " [0.84843075, 0.50886482, 0.20775944],\n",
       " [0.64940953, 0.5037052, 0.024041632],\n",
       " [0.99195588, 0.4459382, 5.9081199e-06],\n",
       " [0.63417041, 0.50540733, 0.019748477],\n",
       " [0.52761495, 0.50664634, 0.29044211],\n",
       " [0.67856133, 0.50755662, 0.001781299],\n",
       " [0.8847084, 0.50512773, 0.010760917],\n",
       " [0.64940953, 0.51070422, 0.15159406],\n",
       " [0.95063406, 0.51472312, 9.2298884e-05],\n",
       " [0.91863394, 0.51248014, 0.00016031878],\n",
       " [0.014506094, 0.51509863, 0.57733321],\n",
       " [0.0023969111, 0.51093608, 0.97258049],\n",
       " [2.0769855e-06, 0.50810236, 0.80719805],\n",
       " [0.95029616, 0.46669412, 0.00029581436],\n",
       " [0.73887444, 0.49787462, 0.0066356072],\n",
       " [0.012094893, 0.5110997, 0.5251053],\n",
       " [0.64940953, 0.51135278, 0.3224313],\n",
       " [5.4921125e-06, 0.51558334, 0.41748026],\n",
       " [0.07874582, 0.51852208, 0.52508563],\n",
       " [0.26651469, 0.52766305, 0.52507544],\n",
       " [0.0017804726, 0.51068133, 0.77459729],\n",
       " [0.24781173, 0.50871187, 0.17645697],\n",
       " [0.64940959, 0.50470865, 0.078919888],\n",
       " [0.74555922, 0.50990212, 0.012156702],\n",
       " [0.015184699, 0.51044214, 0.52508026],\n",
       " [0.061575353, 0.50956094, 0.53977847],\n",
       " [0.002657176, 0.49961993, 0.056948811],\n",
       " [0.66054422, 0.48915282, 0.034843925],\n",
       " [0.22792442, 0.49933004, 0.50250334],\n",
       " [0.26272202, 0.51127523, 0.15547836],\n",
       " [0.7109459, 0.5018425, 0.048873756],\n",
       " [0.59006059, 0.49464521, 0.081040874],\n",
       " [0.82911438, 0.51253372, 2.5298132e-09],\n",
       " [0.64940977, 0.5120303, 0.20726173],\n",
       " [0.00067638088, 0.51352137, 0.88300431],\n",
       " [0.34516597, 0.50863957, 0.17755735],\n",
       " [0.0016937451, 0.50456512, 0.950387],\n",
       " [0.940238, 0.51271814, 0.0039518732],\n",
       " [0.0018202177, 0.50888199, 0.67813182],\n",
       " [0.69659597, 0.49998149, 0.065726683],\n",
       " [0.77592283, 0.4902674, 0.037281036],\n",
       " [6.4285079e-05, 0.51294011, 0.99154621],\n",
       " [0.00054206379, 0.4953911, 0.30360022],\n",
       " [0.99454284, 0.51414359, 3.2119604e-07],\n",
       " [0.21048635, 0.50705481, 0.28046069],\n",
       " [0.71543521, 0.49968293, 0.0083350819],\n",
       " [2.4292985e-05, 0.49973923, 0.94956023],\n",
       " [0.97060162, 0.52140009, 0.00040408687],\n",
       " [2.2811695e-05, 0.48117766, 0.92994636],\n",
       " [0.0036561049, 0.51047146, 0.52508569],\n",
       " [0.78117514, 0.49471334, 0.032949205],\n",
       " [0.71933079, 0.50339431, 0.0083615575],\n",
       " [0.64942479, 0.51181293, 0.12804741],\n",
       " [6.4868295e-06, 0.50567067, 0.9666121],\n",
       " [0.087945968, 0.50440609, 0.93134886],\n",
       " [0.99911135, 0.51591939, 1.3243302e-07],\n",
       " [0.0050778971, 0.509498, 0.98935306],\n",
       " [0.98549533, 0.51133782, 0.00014060165],\n",
       " [0.027157394, 0.51467699, 0.52040333],\n",
       " [0.98829716, 0.51590806, 2.0295602e-11],\n",
       " [0.935637, 0.51424277, 2.5957823e-07],\n",
       " [0.95112431, 0.478194, 0.001115976],\n",
       " [0.087238342, 0.51055467, 0.16249581],\n",
       " [0.0010117047, 0.4985624, 0.84539211],\n",
       " [1.9103069e-05, 0.49429283, 0.98588723],\n",
       " [0.64940953, 0.51141959, 0.25571075],\n",
       " [0.9957481, 0.5067572, 0.026295023],\n",
       " [0.0035738817, 0.49745724, 0.52508736],\n",
       " [0.65576077, 0.50729179, 0.026144911],\n",
       " [0.64940953, 0.5131855, 0.40268612],\n",
       " [0.82087159, 0.51422638, 0.0023322247],\n",
       " [1.6129979e-05, 0.51035708, 0.7244308],\n",
       " [3.0874747e-07, 0.50966418, 0.98449475],\n",
       " [0.82117653, 0.50700378, 0.066414282],\n",
       " [0.11095685, 0.50799763, 0.24881317],\n",
       " [0.72569984, 0.50082242, 0.07831037],\n",
       " [5.6584631e-05, 0.5094232, 0.84526485],\n",
       " [0.71131623, 0.47039011, 7.3935029e-05],\n",
       " [0.64940953, 0.51069844, 0.13532853],\n",
       " [0.89178467, 0.50754398, 0.071165077],\n",
       " [3.8149938e-05, 0.51253527, 0.94986486],\n",
       " [0.016495971, 0.51345086, 0.50598556],\n",
       " [0.068712458, 0.50932008, 0.65996671],\n",
       " [0.072647534, 0.48177874, 0.96737891],\n",
       " [0.64940953, 0.51067322, 0.23536986],\n",
       " [0.16729279, 0.51353008, 0.7715503],\n",
       " [2.732376e-05, 0.51426196, 0.98054343],\n",
       " [0.845474, 0.50409764, 0.006334051],\n",
       " [0.61242348, 0.49253666, 0.06701272],\n",
       " [0.17433406, 0.51406431, 0.064900182],\n",
       " [0.69035619, 0.51019669, 0.24543753],\n",
       " [0.1267186, 0.50735909, 0.28710169],\n",
       " [0.0020337773, 0.49513865, 0.7015177],\n",
       " [0.89292091, 0.5147174, 2.3523222e-13],\n",
       " [0.28023496, 0.5158962, 0.13750979],\n",
       " [0.65215975, 0.4683755, 0.085487135],\n",
       " [0.64940953, 0.51004547, 0.0053009894],\n",
       " [0.8846181, 0.51870233, 0.004874777],\n",
       " [0.0056257742, 0.51238406, 0.52508569],\n",
       " [0.084343597, 0.51077408, 0.17164697],\n",
       " [0.94701248, 0.49981707, 0.00083400164],\n",
       " [0.95539314, 0.50525093, 5.71075e-06],\n",
       " [0.56879246, 0.51198357, 0.28336614],\n",
       " [0.98465145, 0.48731485, 0.022094721],\n",
       " [0.060241953, 0.50609624, 0.56215394],\n",
       " [0.0014351262, 0.47891253, 0.5250867],\n",
       " [0.68246317, 0.51272935, 0.01032978],\n",
       " [0.24380216, 0.4945375, 0.57023913],\n",
       " [5.7122732e-05, 0.50748026, 0.73070145],\n",
       " [0.67382741, 0.50020045, 0.0064109331],\n",
       " [0.45465842, 0.49182883, 0.15890519],\n",
       " [0.87594593, 0.49356067, 0.011599932],\n",
       " [0.64948046, 0.50079203, 0.0093304245],\n",
       " [0.82118577, 0.49717614, 0.016280331],\n",
       " [3.4292538e-07, 0.51426065, 0.98135513],\n",
       " [0.63355768, 0.51960373, 0.019443199],\n",
       " [3.2525835e-10, 0.50321823, 0.96351087],\n",
       " [0.0015583647, 0.49598062, 0.85722601],\n",
       " [0.0013078052, 0.489268, 0.68249303],\n",
       " [0.004084988, 0.50877243, 0.7742185],\n",
       " [0.089937367, 0.4681772, 0.96854043],\n",
       " [0.00011903697, 0.51255041, 0.55453795],\n",
       " [0.47057277, 0.52193964, 0.47242478],\n",
       " [0.71668291, 0.48580274, 1.2608244e-05],\n",
       " [0.64940953, 0.51346231, 0.7882244],\n",
       " [0.64940953, 0.50703007, 0.073361292],\n",
       " [0.6774407, 0.51269197, 0.044706661],\n",
       " [0.00077771908, 0.46515423, 0.72884291],\n",
       " [0.669644, 0.51583076, 0.035598446],\n",
       " [0.70160073, 0.51086622, 0.0014475404],\n",
       " [0.0005054074, 0.50140226, 0.59095997],\n",
       " [8.1831695e-06, 0.5041281, 0.95611316],\n",
       " [0.00024709266, 0.49687666, 0.53318357],\n",
       " [0.26136303, 0.5176807, 0.077609695],\n",
       " [0.97457844, 0.51273036, 0.0046187658],\n",
       " [1.7921174e-06, 0.50718302, 0.71316481],\n",
       " [0.77496314, 0.50983274, 0.0018739855],\n",
       " [0.00035531391, 0.51155823, 0.9171747],\n",
       " [0.013194332, 0.48905787, 0.84745193],\n",
       " [0.020562712, 0.51043391, 0.90199357],\n",
       " [0.0007457087, 0.51043838, 0.73458284],\n",
       " [0.7033264, 0.51534605, 0.40722117],\n",
       " [0.71302992, 0.51067704, 0.019226532],\n",
       " [0.001343111, 0.50954074, 0.67326933],\n",
       " [0.77273303, 0.51253951, 0.0044521103],\n",
       " [0.48983642, 0.52290297, 0.75444543],\n",
       " ...]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validPreds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 0, ..., 1, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validPredArray = np.argmax(np.vstack(validPreds), axis=1)\n",
    "validPredArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "yTestMax = np.argmax(yTest,axis=1)\n",
    "cnfMatrix = confusion_matrix(yTestMax, validPredArray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4003  406  182]\n",
      " [1662  998 1786]\n",
      " [ 402  553 3530]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(cnfMatrix)\n",
    "fig, ax = plt.subplots(figsize=(6,6)) \n",
    "ax = sns.heatmap(cnfMatrix, fmt=\"d\", cmap=plt.cm.Blues, ax=ax , annot=True)\n",
    "ax.set_xticklabels(classNames)\n",
    "ax.set_yticklabels(classNames)\n",
    "plt.title('Konfusionsmatrix Versuch 5.7')\n",
    "plt.ylabel('Wahre Klasse')\n",
    "plt.xlabel('Vorhergesagte Klasse')\n",
    "plt.savefig('konfmatrixVersuch5_7.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6308977961839964"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "8531/13522"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "102px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "threshold": 4,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
